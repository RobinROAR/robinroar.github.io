title: 简谈贝叶斯决策
date: 2015-07-14 09:44:57
tags: 
- 贝叶斯
- 分类
- 机器学习
categories: 
- 技术
mathjax: true 
---


**贝叶斯决策**, 一种常用的分类方法，复杂度低，准确度好，可以和多种分类方法结合。最高的理论正确率令人神往（上自动化所模式识别课中多次强调）。
写一写学习贝叶斯方法中的几点理解，一个简单脉络而已，方便回顾，与大家分享。

-------------------
##1. 先验概率与后验概率
两个概念：
- `先验概率`： 事情还没有发生，要求这件事情发生的可能性的大小。如P(A).
- `后验概率`： 事情已经发生，要求这件事情发生的原因`是由某个因素引起的`可能性的大小，是后验概率. 

所以我们可以使用贝叶斯公式求后验概率，进而达到分类的目的。


##2. 贝叶斯公式

$$ P(F\_j|E)= \dfrac{P(F\_jE)}{P(E)} =  \dfrac{P(E|F\_j)P(F\_j)}{ \displaystyle{\sum_{i=1}^{n} }P(E|F\_i)P(F\_i) } $$

两个核心：
- **推导**：条件概率公式。分解中间步骤分子，我们得出结果$P(F\_j|E) \cdot P(E)= P(F\_jE) =   P(E|F\_j) \cdot P(F\_j)$.   

- **应用**：用于分类问题。分解中间步骤分母$P(E) = \displaystyle{\sum\_{i=1}^{n} }P(E|F\_i)P(F\_i）$ ，每个$F\_i$代表一个类别。

扯点零碎，条件概率公式对任意两个事件都是存在的，当两事件独立(即$P(EF) = P(E)P(F)$)时，$P(E|F) = P(EF)/P(F) = P(E)$.



##3. 例子
看过很多讲解贝叶斯公式的例子，下面这个很好:
> 一份信件等可能的在存在三个不同的文件夹中的任意一个。假设信件实际在文件夹$i$中$(i = 1,2,3)$而你经过对文件夹$i$的快速翻阅发现信件的概率记为$a\_i$，问题是，假定你查看了文件夹1且没有发现此信，问信件在文件夹1中的概率是多少？
                                                              ——[《应用随机过程：概率模型导论》](http://book.douban.com/subject/2309401/)

**解**： 以$F\_i(i = 1,2,3)$为信在文件中$i$中的事件`（先验概率）`，E是通过对文件夹1搜索而为看到信这个事件，我们求$P(F\_1|E)$`(后验概率)`，通过贝叶斯公式：
$$	P(F\_1|E)=  \dfrac{P(E|F\_1)P(F\_1)}{ \displaystyle{\sum\_{i=1}^{3} }P(E|F\_i)P(F\_i) } = \dfrac{(1-a\_1) \frac{1}{3}}{ (1-a\_1) \frac{1}{3}+\frac{1}{3}+\frac{1}{3}} = \dfrac{1-a\_1}{3-a\_1}$$

把握两个重点：
	- 对问题中事件的假设
	- 对整体事件空间(分母)的划分


##4. 一种简单实现：朴素贝叶斯
贝叶斯思想只是一个框架，可结合许多假设与分布。
朴素思分类就是一个通过先验概率求后验概率的方法，然后将实例点分到后验概率最大的类。就像街上有一个黑人，我们预测黑人来自非洲，为什么呢？因为黑人中非洲人的比率最高。
####朴素贝叶斯的核心假设：
1. 特征之间相互独立 ：这个“朴素”一词的来源。假设体现在向量的特征之间。所以有
$$P(x|y\_i)P(y\_i)=P(a\_1|y\_i)P(a\_2|y\_i)...P(a\_m|y\_i)P(y\_i)=P(y\_i)\prod^m_{j=1}P(a\_j|y\_i)$$
	- $y\_i$为每一个类别，$a\_i$ 为样本X的特征的每一维分量
	
2. 每个特征同等重要。


##5. 分类准则

将输入向量分到后验概率最大的类，这就是`分类准则`。贝叶斯使用的是`最大后验概率准则`。
在别的分类方法中，我们还用过`最小错误率`的分类准则，比如基于0-1损失的最小错误率，这两者是一样的。

设损失函数

$L(Y,f(X))=$

\begin{align\*}
1,Y \not= f(X) \\\
0, Y= f(X) \\\
\end{align\*}


- 期望风险函数为

\begin{equation\*}
\begin{aligned}  
R\_e(f(x)) & = E[L(Y, f(x))]\\\
& =  \iint\_{X,Y}L(Y,f(X))P(X,Y)\mathrm{d}x\mathrm{d}y \\\
&= \iint\_{X,Y}L(Y,f(X))P(Y|X)P(X)\mathrm{d}x\mathrm{d}y\\\
&= \int\_{X} (\sum\_{k=1}^{k}L(Y= c\_k,f(X))P(Y=c\_k|X) )P(X)\mathrm{d}x\\\
&= E\_x(\sum_{k=1}^{k}L(Y= c\_k,f(X))P(Y=c\_k|X) )
\end{aligned}
\end{equation\*}

- 为了使期望最小，对$X=x$逐个极小化：

\begin{equation\*}
\begin{aligned} 
f(x)& = \arg\min \limits\_{y\in\mathcal{Y}}\sum\_{k=1}^{k}L(c\_k,y)P(c\_k|X=x) \\\
&=  \arg\min \limits\_{y\in\mathcal{Y}}\sum\_{k=1}^{k}P(y \not = c\_k|X=x) \\\
&=  \arg\min \limits\_{y\in\mathcal{Y}}\sum\_{k=1}^{k}(1-P(y  = c\_k|X=x) )\\\
&=  \arg\max \limits\_{y\in\mathcal{Y}}P(y = c\_k|X = x)
\end{aligned}
\end{equation\*}

可以看出，这就是`最大后验概率准则`，也是我们使用贝叶斯决策的分类准则。


---

暂时先写这几点，顺带说一句，Hexo上写$\mathrm{\LaTeX}$公式，冲突比较多，还在寻找比较好的解决方法，

Robin 
2015.7.10 夜 
