<!DOCTYPE html><html lang="en"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=5"><title>Robin Website    </title><meta name="author" content="Robin Zheng"><link rel="shortcut icon" href="/img/favicon.png"><link rel="stylesheet" href="/css/index.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5.13.0/css/all.min.css"><meta name="generator" content="Hexo 5.2.0"></head><body><header id="page_header"><div class="header_wrap"><div id="blog_name"><a class="blog_title" id="site-name" href="/">Robin Website    </a></div><button class="menus_icon"><div class="navicon"></div></button><ul class="menus_items"><li class="menus_item"><a class="site-page" href="/projects/"> Projects</a></li><li class="menus_item"><a class="site-page" href="/publications/"> Publications</a></li><li class="menus_item"><a class="site-page" href="/experience/"> Experience</a></li><li class="menus_item"><a class="site-page" href="/cn/"> 中文</a></li></ul></div></header><main id="page_main"><div class="side-card sticky"><div class="card-wrap" itemscope itemtype="http://schema.org/Person"><div class="author-avatar"><img class="avatar-img" src="/img/avatar.jpg" onerror="this.onerror=null;this.src='/img/profile.png'" alt="avatar"></div><div class="author-discrip"><h3>Robin Zheng</h3><p class="author-bio"></p></div><div class="author-links"><button class="btn m-social-links">Links</button><ul class="social-icons"><li><a class="social-icon" href="/" target="_blank"><i class="fab fa-linkedin" aria-hidden="true"></i></a></li><li><a class="social-icon" href="zrb915@live.com" target="_blank"><i class="fas fa-envelope" aria-hidden="true"></i></a></li></ul><ul class="social-links"><li><a class="e-social-link" href="/" target="_blank"><i class="fas fa-graduation-cap" aria-hidden="true"></i><span>Google Scholar</span></a></li></ul></div><a class="cv-links" href="/attaches/CV.pdf" target="_blank"><i class="fas fa-file-pdf" aria-hidden="true"><span>My Detail CV.</span></i></a></div></div><div class="page" itemscope itemtype="http://schema.org/CreativeWork"><h2 class="page-title"></h2><article><h3 id="About-Me"><a href="#About-Me" class="headerlink" title="About Me"></a>About Me</h3><p>I am a AI researcher at <a target="_blank" rel="noopener" href="http://www.moviebook.com.cn/">Moviebook</a>. My research interests include computer vision and deep learning. My recent projects focus on GAN and its applications to vision tasks, multi-modal synthesis for virtual avatars, face and 3DMMs. Before join Moviebook, I obtained my Ph.D. from <a target="_blank" rel="noopener" href="https://www.ucas.ac.cn/">University of Chinese Academy of Sciences</a>, under the supervision of Prof. <a target="_blank" rel="noopener" href="http://sourcedb.cnic.cas.cn/zw/zjrc/200908/t20090817_2404527.html">Baoping Yan</a>. I received my B.E. from <a target="_blank" rel="noopener" href="https://www.sdu.edu.cn/">Shandong University</a>.</p>
<h3 id="News"><a href="#News" class="headerlink" title="News"></a>News</h3><ul>
<li><strong>[2020-08-01]</strong>  Our photoreal virtual anchor solution is applied to CBA Smart Quiz in CCTV Sports, WeChat app “Smart Quiz of CBA” release! <a target="_blank" rel="noopener" href="http://sports.cctv.com/2020/07/31/ARTIKTUwDw9r7gk0Ei47BokI200731.shtml">link!</a></li>
<li><strong>[2020-06-22]</strong>  Our paper “A Neural Lip-Sync Framework for Synthesizing Photorealistic Virtual News Anchors” has been accpeted by ICPR 2020。<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2002.08700">link!</a></li>
<li><strong>[2020-05-22]</strong> Our virtual anchor solution is used to build the interactive AI newspaper reading project for People’s Daily . <a target="_blank" rel="noopener" href="https://baijiahao.baidu.com/s?id=1667384501608883676&wfr=spider&for=pc">link!</a></li>
<li><strong>[2019-11-15]</strong> We release our all-in-one machine for interactive newspaper reading AI anchor at the World 5G Conference in Beijing, in collaboration with Tech Daily.  <a target="_blank" rel="noopener" href="https://t.cj.sina.com.cn/articles/view/2661790332/9ea7b27c01900p5v2?from=tech&sudaref=www.baidu.com&display=0&retcode=0">link!</a></li>
<li><strong>[2019-07-01]</strong> I join the Deep Innovation Lab at Moviebook , in charge of the development of Virtual Anchor.</li>
<li><strong>[2019-01-22]</strong> I attend AAAI 19 in Hawaii and present our paper “Exploiting Time-Series Image-to-Image Translation to Expand the Range of Wildlife Habitat Analysis”. <a target="_blank" rel="noopener" href="https://www.aaai.org/ojs/index.php/AAAI/article/view/3862">link!</a></li>
</ul>
<h3 id="Recent-Work"><a href="#Recent-Work" class="headerlink" title="Recent Work"></a>Recent Work</h3><h4 id="A-Neural-Lip-Sync-Framework-for-Synthesizing-High-Resolution-Photorealistic-and-Pose-Adaptive-Virtual-News-Anchors"><a href="#A-Neural-Lip-Sync-Framework-for-Synthesizing-High-Resolution-Photorealistic-and-Pose-Adaptive-Virtual-News-Anchors" class="headerlink" title="A Neural Lip-Sync Framework for Synthesizing High-Resolution, Photorealistic, and Pose-Adaptive Virtual News Anchors"></a>A Neural Lip-Sync Framework for Synthesizing High-Resolution, Photorealistic, and Pose-Adaptive Virtual News Anchors</h4><p>We develop a neural framework for producing speech-driven virtual news anchors. Our goal is to synthesize high-resolution, photo-realistic, visual nature videos based on audio content and target roles. Our solution can benefit a variety of scenarios including video editing,film industry, game development, and smart agent.</p>
<h4 id="Facial-Motion-Driven-Based-on-Landmarks-Mapping-and-Neural-Rendering"><a href="#Facial-Motion-Driven-Based-on-Landmarks-Mapping-and-Neural-Rendering" class="headerlink" title="Facial Motion Driven Based on Landmarks Mapping and Neural Rendering"></a>Facial Motion Driven Based on Landmarks Mapping and Neural Rendering</h4><p>We learn the correspondence of facial keypoints between individuals  and implement the interconversion. A neural rendering model based on generative adversarial networks is responsible for rendering the synthetic facial map into high-resolution, photorealistic video frames.</p>
<p><img src="../img/home2.gif" alt="home2"></p>
</article></div></main><div class="nav-wrap"><div class="nav"><button class="site-nav"><div class="navicon"></div></button><ul class="nav_items"><li class="nav_item"><a class="nav-page" href="/projects/"> Projects</a></li><li class="nav_item"><a class="nav-page" href="/publications/"> Publications</a></li><li class="nav_item"><a class="nav-page" href="/experience/"> Experience</a></li><li class="nav_item"><a class="nav-page" href="/cn/"> 中文</a></li></ul></div><div class="cd-top"><i class="fa fa-arrow-up" aria-hidden="true"></i></div></div><footer id="page_footer"><div class="footer_wrap"><div class="copyright">&copy;2020 by Robin Zheng</div><div class="theme-info">Powered by <a target="_blank" href="https://hexo.io" rel="nofollow noopener">Hexo</a> & <a target="_blank" href="https://github.com/PhosphorW/hexo-theme-academia" rel="nofollow noopener">Academia Theme</a></div></div></footer><script src="https://cdn.jsdelivr.net/npm/jquery@latest/dist/jquery.min.js"></script><script src="https://cdn.jsdelivr.net/npm/jquery-pjax@latest/jquery.pjax.min.js"></script><script src="/js/main.js"></script></body></html>