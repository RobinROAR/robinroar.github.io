{"meta":{"version":1,"warehouse":"1.0.2"},"models":{"Asset":[{"_id":"source/CNAME","path":"CNAME","modified":1},{"_id":"themes/jacman/source/js/totop.js","path":"js/totop.js","modified":1},{"_id":"themes/jacman/source/js/jquery.imagesloaded.min.js","path":"js/jquery.imagesloaded.min.js","modified":1},{"_id":"themes/jacman/source/js/jquery-2.0.3.min.js","path":"js/jquery-2.0.3.min.js","modified":1},{"_id":"themes/jacman/source/js/gallery.js","path":"js/gallery.js","modified":1},{"_id":"themes/jacman/source/img/scrollup.png","path":"img/scrollup.png","modified":1},{"_id":"themes/jacman/source/img/logo.svg","path":"img/logo.svg","modified":1},{"_id":"themes/jacman/source/img/logo.png","path":"img/logo.png","modified":1},{"_id":"themes/jacman/source/img/jacman.jpg","path":"img/jacman.jpg","modified":1},{"_id":"themes/jacman/source/img/favicon.ico","path":"img/favicon.ico","modified":1},{"_id":"themes/jacman/source/img/cc-zero.svg","path":"img/cc-zero.svg","modified":1},{"_id":"themes/jacman/source/img/cc-by.svg","path":"img/cc-by.svg","modified":1},{"_id":"themes/jacman/source/img/cc-by-sa.svg","path":"img/cc-by-sa.svg","modified":1},{"_id":"themes/jacman/source/img/cc-by-nd.svg","path":"img/cc-by-nd.svg","modified":1},{"_id":"themes/jacman/source/img/cc-by-nc.svg","path":"img/cc-by-nc.svg","modified":1},{"_id":"themes/jacman/source/img/cc-by-nc-sa.svg","path":"img/cc-by-nc-sa.svg","modified":1},{"_id":"themes/jacman/source/img/cc-by-nc-nd.svg","path":"img/cc-by-nc-nd.svg","modified":1},{"_id":"themes/jacman/source/img/banner.jpg","path":"img/banner.jpg","modified":1},{"_id":"themes/jacman/source/img/author.jpg","path":"img/author.jpg","modified":1},{"_id":"themes/jacman/source/font/fontdiao.woff","path":"font/fontdiao.woff","modified":1},{"_id":"themes/jacman/source/font/fontdiao.ttf","path":"font/fontdiao.ttf","modified":1},{"_id":"themes/jacman/source/font/fontdiao.svg","path":"font/fontdiao.svg","modified":1},{"_id":"themes/jacman/source/font/fontdiao.eot","path":"font/fontdiao.eot","modified":1},{"_id":"themes/jacman/source/font/fontawesome-webfont.woff","path":"font/fontawesome-webfont.woff","modified":1},{"_id":"themes/jacman/source/font/fontawesome-webfont.ttf","path":"font/fontawesome-webfont.ttf","modified":1},{"_id":"themes/jacman/source/font/fontawesome-webfont.svg","path":"font/fontawesome-webfont.svg","modified":1},{"_id":"themes/jacman/source/font/fontawesome-webfont.eot","path":"font/fontawesome-webfont.eot","modified":1},{"_id":"themes/jacman/source/font/coveredbyyourgrace-webfont.woff","path":"font/coveredbyyourgrace-webfont.woff","modified":1},{"_id":"themes/jacman/source/font/coveredbyyourgrace-webfont.ttf","path":"font/coveredbyyourgrace-webfont.ttf","modified":1},{"_id":"themes/jacman/source/font/coveredbyyourgrace-webfont.svg","path":"font/coveredbyyourgrace-webfont.svg","modified":1},{"_id":"themes/jacman/source/font/coveredbyyourgrace-webfont.eot","path":"font/coveredbyyourgrace-webfont.eot","modified":1},{"_id":"themes/jacman/source/font/FontAwesome.otf","path":"font/FontAwesome.otf","modified":1},{"_id":"themes/jacman/source/fancybox/jquery.fancybox.pack.js","path":"fancybox/jquery.fancybox.pack.js","modified":1},{"_id":"themes/jacman/source/fancybox/jquery.fancybox.js","path":"fancybox/jquery.fancybox.js","modified":1},{"_id":"themes/jacman/source/fancybox/jquery.fancybox.css","path":"fancybox/jquery.fancybox.css","modified":1},{"_id":"themes/jacman/source/fancybox/helpers/jquery.fancybox-thumbs.js","path":"fancybox/helpers/jquery.fancybox-thumbs.js","modified":1},{"_id":"themes/jacman/source/fancybox/helpers/jquery.fancybox-thumbs.css","path":"fancybox/helpers/jquery.fancybox-thumbs.css","modified":1},{"_id":"themes/jacman/source/fancybox/helpers/jquery.fancybox-media.js","path":"fancybox/helpers/jquery.fancybox-media.js","modified":1},{"_id":"themes/jacman/source/fancybox/helpers/jquery.fancybox-buttons.js","path":"fancybox/helpers/jquery.fancybox-buttons.js","modified":1},{"_id":"themes/jacman/source/fancybox/helpers/jquery.fancybox-buttons.css","path":"fancybox/helpers/jquery.fancybox-buttons.css","modified":1},{"_id":"themes/jacman/source/fancybox/helpers/fancybox_buttons.png","path":"fancybox/helpers/fancybox_buttons.png","modified":1},{"_id":"themes/jacman/source/fancybox/fancybox_sprite@2x.png","path":"fancybox/fancybox_sprite@2x.png","modified":1},{"_id":"themes/jacman/source/fancybox/fancybox_sprite.png","path":"fancybox/fancybox_sprite.png","modified":1},{"_id":"themes/jacman/source/fancybox/fancybox_overlay.png","path":"fancybox/fancybox_overlay.png","modified":1},{"_id":"themes/jacman/source/fancybox/fancybox_loading@2x.gif","path":"fancybox/fancybox_loading@2x.gif","modified":1},{"_id":"themes/jacman/source/fancybox/fancybox_loading.gif","path":"fancybox/fancybox_loading.gif","modified":1},{"_id":"themes/jacman/source/fancybox/blank.gif","path":"fancybox/blank.gif","modified":1},{"_id":"themes/jacman/source/css/style.styl","path":"css/style.styl","modified":1}],"Cache":[{"_id":"source/CNAME","shasum":"687cfa7f59cec950aeff224df9a93e7d1a7cc284","modified":1433903911000},{"_id":"source/_posts/简谈贝叶斯决策.md","shasum":"49f4315c40d1546f41db367f10409a09a0e45e5a","modified":1440406175000},{"_id":"source/_posts/荆棘谷的青山.md","shasum":"702416b9b90dab498299a2c8c1d111acd005c8fe","modified":1437379524000},{"_id":"source/_posts/谈谈Linux的环境变量与启动文件.md","shasum":"9252229eae6683413e639126499656966ff31d1b","modified":1444903186000},{"_id":"source/_posts/逻辑回归与计算中的向量化思想.md","shasum":"fcfc1e201f2370134a62c9b7f558e2f79862a10c","modified":1439780366000},{"_id":"source/about/index.md","shasum":"8213da7b9c8ea91e13a45250217ba9e11210419a","modified":1444921673000},{"_id":"source/categories/index.md","shasum":"9bc9e6dda975955e994487a45aee79a9c1a0b9af","modified":1433923816000},{"_id":"source/resume/index.md","shasum":"c3529e5af684421e579a192b7237d76096249b38","modified":1434007719000},{"_id":"source/tags/index.md","shasum":"62664f8308f76b7cfb579209c57e48a5779dc5ac","modified":1433929855000},{"_id":"themes/jacman/LICENSE","shasum":"931516aa36c53eb7843c83d82662eb50cc3c4367","modified":1433846040000},{"_id":"themes/jacman/README.md","shasum":"38698c732ca2c0fa48de89cfee9859bc09e74fd4","modified":1433846040000},{"_id":"themes/jacman/README_zh.md","shasum":"ee9eeb2b72e5597a3550d59d231f443d990d3115","modified":1433846040000},{"_id":"themes/jacman/_config.yml","shasum":"869bc090c6d9fa24dc1e14d5a7c463129832ed0d","modified":1436867113000},{"_id":"themes/jacman/languages/default.yml","shasum":"ad0de3e82c7fc238cc067ffc37359b1420aef6b3","modified":1433846040000},{"_id":"themes/jacman/languages/zh-CN.yml","shasum":"5e4ac19d7b2bbf0d5b5aa55d33653380abda8b9a","modified":1433846040000},{"_id":"themes/jacman/languages/zh-TW.yml","shasum":"41c112162d79b4d3f97b417c7cd6ca6d70419ef2","modified":1433846040000},{"_id":"themes/jacman/layout/_partial/after_footer.ejs","shasum":"07b52001fb612d67cfc2d60f982a4e838cdf0c17","modified":1433846040000},{"_id":"themes/jacman/layout/_partial/analytics.ejs","shasum":"fd004beb8d4500afd5fb3b3871a95afa2a375f16","modified":1433846040000},{"_id":"themes/jacman/layout/_partial/archive.ejs","shasum":"2c7395e7563fe016521712a645c28a13f952d52a","modified":1433846040000},{"_id":"themes/jacman/layout/_partial/article.ejs","shasum":"261ecacb8456f4cb972632b6a9103860fa63b9a3","modified":1433846040000},{"_id":"themes/jacman/layout/_partial/article_row.ejs","shasum":"4cb855d91ece7f67b2ca0992fffa55472d0b9c93","modified":1433846040000},{"_id":"themes/jacman/layout/_partial/categories.ejs","shasum":"8a52d0344d5bce1925cf586ed73c11192925209b","modified":1433846040000},{"_id":"themes/jacman/layout/_partial/footer.ejs","shasum":"7a7eaa64ac5139c4c418844aa35076271cd4365d","modified":1433846040000},{"_id":"themes/jacman/layout/_partial/head.ejs","shasum":"761941be4922cd3c177c8130296b909bf7db5c09","modified":1433846040000},{"_id":"themes/jacman/layout/_partial/header.ejs","shasum":"18515612344ff048b9372b91b7eef6f3b143801f","modified":1433846040000},{"_id":"themes/jacman/layout/_partial/mathjax.ejs","shasum":"d42994ac696f52ba99c1cbac382cd76d5b04a3e8","modified":1433846040000},{"_id":"themes/jacman/layout/_partial/pagination.ejs","shasum":"6146ac37dfb4f8613090bc52b3fc8cfa911a186a","modified":1433846040000},{"_id":"themes/jacman/layout/_partial/post/article.ejs","shasum":"b09e3acea7076e1f01dfe0c2295e19951ea09437","modified":1433846040000},{"_id":"themes/jacman/layout/_partial/post/catetags.ejs","shasum":"0e37bababc8f4659f5b59a552a946b46d89e4158","modified":1433846040000},{"_id":"themes/jacman/layout/_partial/post/comment.ejs","shasum":"c88bc8f5805173920a5fdd7e9234a850e3d8e151","modified":1433846040000},{"_id":"themes/jacman/layout/_partial/post/footer.ejs","shasum":"b12ec08a5845a3d8c01257614f1dfead879c87d2","modified":1433846040000},{"_id":"themes/jacman/layout/_partial/post/gallery.ejs","shasum":"fafc2501d7e65983b0f5c2b58151ca12e57c0574","modified":1433846040000},{"_id":"themes/jacman/layout/_partial/post/header.ejs","shasum":"36a705942b691abe0d643ea8afa339981b32f6f2","modified":1433846040000},{"_id":"themes/jacman/layout/_partial/post/jiathis.ejs","shasum":"d7f5960039ac74924559ab6ba03c64457b8f0966","modified":1433846040000},{"_id":"themes/jacman/layout/_partial/post/pagination.ejs","shasum":"7de9c07a4c968429a8088c31a28b7f3a993ded1b","modified":1433846040000},{"_id":"themes/jacman/layout/_partial/search.ejs","shasum":"1083824a6c6c3df02767f2f3b727aee78ebb76ec","modified":1433846040000},{"_id":"themes/jacman/layout/_partial/sidebar.ejs","shasum":"c4f527fff0070fbe65919053a16224412317f40d","modified":1433846040000},{"_id":"themes/jacman/layout/_partial/tags.ejs","shasum":"b33b2b5d08f1d53a8de25a95f660f7f1cea7b3cb","modified":1433846040000},{"_id":"themes/jacman/layout/_partial/tinysou_search.ejs","shasum":"06ecddc8a9d40b480fe2e958af1dab857a9d5441","modified":1433846040000},{"_id":"themes/jacman/layout/_partial/totop.ejs","shasum":"bea5bb7cb9350b8af7d97a8d223af63a5b30ab78","modified":1433846040000},{"_id":"themes/jacman/layout/_widget/archive.ejs","shasum":"39ea6b7888406fbd1b4cf236ebd718e881493374","modified":1433846040000},{"_id":"themes/jacman/layout/_widget/category.ejs","shasum":"c1fae96b5053da021bcc04ab2ce5c2c8d30de8a2","modified":1433846040000},{"_id":"themes/jacman/layout/_widget/douban.ejs","shasum":"94ce1fb7a1143f34ac1365924b00cae64e1a111e","modified":1433846040000},{"_id":"themes/jacman/layout/_widget/links.ejs","shasum":"e49868063439c2092cdf9a8ec82cc295b0e42f66","modified":1433846040000},{"_id":"themes/jacman/layout/_widget/rss.ejs","shasum":"0a4b5f2a2e36a1d504fe2e7c6c8372cbb4628aab","modified":1433846040000},{"_id":"themes/jacman/layout/_widget/tag.ejs","shasum":"7e82ad9c916b9ce871b2f65ce8f283c5ba47947b","modified":1433846040000},{"_id":"themes/jacman/layout/_widget/tagcloud.ejs","shasum":"10a1001189d5c28ce6d42494563b9637c302b454","modified":1433846040000},{"_id":"themes/jacman/layout/_widget/weibo.ejs","shasum":"a31c2b223d0feb2a227e203cac9e5d13b7d328a8","modified":1433846040000},{"_id":"themes/jacman/layout/archive.ejs","shasum":"a18842e3d719fe3ca9b977a6995f8facc75c8673","modified":1433846040000},{"_id":"themes/jacman/layout/category.ejs","shasum":"9b740fc33f6f028df60f0bc4312bf3ebd03aa8ea","modified":1433846040000},{"_id":"themes/jacman/layout/index.ejs","shasum":"75cef2172c286994af412e11ab7f4f5a0daaf1f5","modified":1433846040000},{"_id":"themes/jacman/layout/layout.ejs","shasum":"5b4289a4526899809b9c2facea535367ff51ba2b","modified":1433846040000},{"_id":"themes/jacman/layout/page.ejs","shasum":"bd6bbf2ea8e183bd835867ff617dc6366b56748c","modified":1433846040000},{"_id":"themes/jacman/layout/post.ejs","shasum":"3114134775bdde5a83cf14feb019606fa2b2b2be","modified":1433846040000},{"_id":"themes/jacman/layout/tag.ejs","shasum":"45150a2365768b6b67880193c9264ad2bb4814db","modified":1433846040000},{"_id":"themes/jacman/scripts/fancybox.js","shasum":"aa411cd072399df1ddc8e2181a3204678a5177d9","modified":1433846040000},{"_id":"themes/jacman/source/css/_base/code.styl","shasum":"a7fe002222bfc7a125c8fe92b26ba9dc604595a2","modified":1433846040000},{"_id":"themes/jacman/source/css/_base/font.styl","shasum":"c8a0faf43b08e37ad07a5669db76d595da966159","modified":1433846040000},{"_id":"themes/jacman/source/css/_base/public.styl","shasum":"a29e4a4fbc288323b7f3ca2e501a6609e5646e2f","modified":1433846040000},{"_id":"themes/jacman/source/css/_base/variable.styl","shasum":"cb652eb83c28a208743fabab92de896f8b7cbf7b","modified":1433846040000},{"_id":"themes/jacman/source/css/_partial/article.styl","shasum":"c69641b4a34a8c62986b335414413dbde26de25e","modified":1433846040000},{"_id":"themes/jacman/source/css/_partial/aside.styl","shasum":"6b0e46e2e3be200339197696f5aabd0871aa9952","modified":1433846040000},{"_id":"themes/jacman/source/css/_partial/duoshuo.styl","shasum":"e85f1192283f043115c272a9deb3cb6ced793990","modified":1433846040000},{"_id":"themes/jacman/source/css/_partial/footer.styl","shasum":"1911613a19b605a58f801c21b03b5d4c83b90f9c","modified":1433846040000},{"_id":"themes/jacman/source/css/_partial/gallery.styl","shasum":"7246809f4ce3166ec1b259bf475cae1a48e29aad","modified":1433846040000},{"_id":"themes/jacman/source/css/_partial/header.styl","shasum":"5121ceb712be3f2dde98b8b6e589b546e19eab8f","modified":1433846040000},{"_id":"themes/jacman/source/css/_partial/helper.styl","shasum":"0bf862a860c07aff5f440b5e6f040baa83031d2c","modified":1433846040000},{"_id":"themes/jacman/source/css/_partial/index.styl","shasum":"864fba1fcb3830a9055c366a99ce5c951c2e9fe9","modified":1433846040000},{"_id":"themes/jacman/source/css/_partial/totop.styl","shasum":"96363d7c5aaed5f649667fc0752a62620a67e872","modified":1433846040000},{"_id":"themes/jacman/source/css/style.styl","shasum":"89070fcce9a70c82ea5559ae8f6efc60e624c6d3","modified":1433846040000},{"_id":"themes/jacman/source/fancybox/blank.gif","shasum":"2daeaa8b5f19f0bc209d976c02bd6acb51b00b0a","modified":1433846040000},{"_id":"themes/jacman/source/fancybox/fancybox_loading.gif","shasum":"1a755fb2599f3a313cc6cfdb14df043f8c14a99c","modified":1433846040000},{"_id":"themes/jacman/source/fancybox/fancybox_loading@2x.gif","shasum":"273b123496a42ba45c3416adb027cd99745058b0","modified":1433846040000},{"_id":"themes/jacman/source/fancybox/fancybox_overlay.png","shasum":"b3a4ee645ba494f52840ef8412015ba0f465dbe0","modified":1433846040000},{"_id":"themes/jacman/source/fancybox/fancybox_sprite.png","shasum":"17df19f97628e77be09c352bf27425faea248251","modified":1433846040000},{"_id":"themes/jacman/source/fancybox/fancybox_sprite@2x.png","shasum":"30c58913f327e28f466a00f4c1ac8001b560aed8","modified":1433846040000},{"_id":"themes/jacman/source/fancybox/helpers/fancybox_buttons.png","shasum":"e385b139516c6813dcd64b8fc431c364ceafe5f3","modified":1433846040000},{"_id":"themes/jacman/source/fancybox/helpers/jquery.fancybox-buttons.css","shasum":"1a9d8e5c22b371fcc69d4dbbb823d9c39f04c0c8","modified":1433846040000},{"_id":"themes/jacman/source/fancybox/helpers/jquery.fancybox-buttons.js","shasum":"dc3645529a4bf72983a39fa34c1eb9146e082019","modified":1433846040000},{"_id":"themes/jacman/source/fancybox/helpers/jquery.fancybox-media.js","shasum":"294420f9ff20f4e3584d212b0c262a00a96ecdb3","modified":1433846040000},{"_id":"themes/jacman/source/fancybox/helpers/jquery.fancybox-thumbs.css","shasum":"4ac329c16a5277592fc12a37cca3d72ca4ec292f","modified":1433846040000},{"_id":"themes/jacman/source/fancybox/helpers/jquery.fancybox-thumbs.js","shasum":"47da1ae5401c24b5c17cc18e2730780f5c1a7a0c","modified":1433846040000},{"_id":"themes/jacman/source/fancybox/jquery.fancybox.css","shasum":"aaa582fb9eb4b7092dc69fcb2d5b1c20cca58ab6","modified":1433846040000},{"_id":"themes/jacman/source/fancybox/jquery.fancybox.js","shasum":"d08b03a42d5c4ba456ef8ba33116fdbb7a9cabed","modified":1433846040000},{"_id":"themes/jacman/source/fancybox/jquery.fancybox.pack.js","shasum":"9e0d51ca1dbe66f6c0c7aefd552dc8122e694a6e","modified":1433846040000},{"_id":"themes/jacman/source/font/FontAwesome.otf","shasum":"b5b4f9be85f91f10799e87a083da1d050f842734","modified":1433846040000},{"_id":"themes/jacman/source/font/coveredbyyourgrace-webfont.eot","shasum":"a17d0f10534303e40f210c506ebb8703fa23b7de","modified":1433846040000},{"_id":"themes/jacman/source/font/coveredbyyourgrace-webfont.ttf","shasum":"194ccb4acf77a03dc25bcc174edb266143704fec","modified":1433846040000},{"_id":"themes/jacman/source/font/coveredbyyourgrace-webfont.woff","shasum":"c6f8dc1a2f6ce914f120e80a876b8fd77b98888e","modified":1433846040000},{"_id":"themes/jacman/source/font/fontawesome-webfont.eot","shasum":"7619748fe34c64fb157a57f6d4ef3678f63a8f5e","modified":1433846040000},{"_id":"themes/jacman/source/font/fontawesome-webfont.woff","shasum":"04c3bf56d87a0828935bd6b4aee859995f321693","modified":1433846040000},{"_id":"themes/jacman/source/font/fontdiao.eot","shasum":"9544a0d7ba208989302bc4da5a184faeb0e883c9","modified":1433846040000},{"_id":"themes/jacman/source/font/fontdiao.ttf","shasum":"ee9fd7be2493c9bf6d2841044e69a0830d9d3fab","modified":1433846040000},{"_id":"themes/jacman/source/font/fontdiao.woff","shasum":"71f54eb6e98aa28cafeb04aab71c0e5b349ea89f","modified":1433846040000},{"_id":"themes/jacman/source/img/author.jpg","shasum":"9058722ba5f9f68672a90cdf8d609ca2bcaae7e2","modified":1433846706000},{"_id":"themes/jacman/source/img/banner.jpg","shasum":"219156face68a3d4505064ff3f02f87423b47606","modified":1434288372000},{"_id":"themes/jacman/source/img/cc-by-nc-nd.svg","shasum":"c6524ece3f8039a5f612feaf865d21ec8a794564","modified":1433846040000},{"_id":"themes/jacman/source/img/cc-by-nc-sa.svg","shasum":"3031be41e8753c70508aa88e84ed8f4f653f157e","modified":1433846040000},{"_id":"themes/jacman/source/img/cc-by-nc.svg","shasum":"8d39b39d88f8501c0d27f8df9aae47136ebc59b7","modified":1433846040000},{"_id":"themes/jacman/source/img/cc-by-nd.svg","shasum":"c563508ce9ced1e66948024ba1153400ac0e0621","modified":1433846040000},{"_id":"themes/jacman/source/img/cc-by-sa.svg","shasum":"aa4742d733c8af8d38d4c183b8adbdcab045872e","modified":1433846040000},{"_id":"themes/jacman/source/img/cc-by.svg","shasum":"28a0a4fe355a974a5e42f68031652b76798d4f7e","modified":1433846040000},{"_id":"themes/jacman/source/img/cc-zero.svg","shasum":"87669bf8ac268a91d027a0a4802c92a1473e9030","modified":1433846040000},{"_id":"themes/jacman/source/img/favicon.ico","shasum":"ec9660de66ff2a0e682e1a5ee7b7d8ee85c585c4","modified":1434008589000},{"_id":"themes/jacman/source/img/jacman.jpg","shasum":"0ba14a4a5e3be012826fc713c33479912126d34e","modified":1433846040000},{"_id":"themes/jacman/source/img/logo.png","shasum":"2a87d4212af648b58859c97f43f3200390e96f44","modified":1434008868000},{"_id":"themes/jacman/source/img/logo.svg","shasum":"9ae38f7225c38624faeb7b74996efa9de7bf065b","modified":1433846040000},{"_id":"themes/jacman/source/img/scrollup.png","shasum":"2137d4f1739aa8aa3fcb0348c3ddf1e41d62f2e3","modified":1433846040000},{"_id":"themes/jacman/source/js/gallery.js","shasum":"f8a4ba7fb8349cca374a3c69fff9b2bf21f742ed","modified":1433846040000},{"_id":"themes/jacman/source/js/jquery.imagesloaded.min.js","shasum":"4109837b1f6477bacc6b095a863b1b95b1b3693f","modified":1433846040000},{"_id":"themes/jacman/source/js/totop.js","shasum":"cad23c5ea7163d1e5c05a0fd3ef9233469da10cb","modified":1433846040000},{"_id":"themes/jacman/source/font/coveredbyyourgrace-webfont.svg","shasum":"eabdb262d8e246865dfb56031f01ff6e8d2f9d53","modified":1433846040000},{"_id":"themes/jacman/source/font/fontawesome-webfont.ttf","shasum":"7f09c97f333917034ad08fa7295e916c9f72fd3f","modified":1433846040000},{"_id":"themes/jacman/source/font/fontdiao.svg","shasum":"334a94e6a66a8b089be7315d876bec93efe38d2b","modified":1433846040000},{"_id":"themes/jacman/source/js/jquery-2.0.3.min.js","shasum":"a0ae3697b0ab8c0e8bd3186c80db42abd6d97a8d","modified":1433846040000},{"_id":"themes/jacman/source/font/fontawesome-webfont.svg","shasum":"46fcc0194d75a0ddac0a038aee41b23456784814","modified":1433846040000}],"Category":[{"name":"技术","_id":"cifsd5p3f00059wbgymyvwsmu"},{"name":"生活","_id":"cifsd5p3n000o9wbg89qfhbg7"}],"Data":[],"Page":[{"title":"tags","layout":"tags","_content":"","source":"tags/index.md","raw":"title: tags \nlayout: tags\n---\n","date":"2015-10-15T15:05:22.000Z","updated":"2015-06-10T09:50:55.000Z","path":"tags/index.html","comments":1,"_id":"cifsd5p2f00009wbgbh5q1wu8"},{"title":"个人简历","date":"2015-06-10T22:07:27.000Z","_content":"wait.....","source":"resume/index.md","raw":"title: 个人简历\ndate: 2015-06-10 18:07:27\n---\nwait.....","updated":"2015-06-11T07:28:39.000Z","path":"resume/index.html","comments":1,"layout":"page","_id":"cifsd5p3800019wbggrh7nmrk"},{"_content":"layout: categories\ntitle: categories","source":"categories/index.md","raw":"layout: categories\ntitle: categories","date":"2015-10-15T15:05:22.000Z","updated":"2015-06-10T08:10:16.000Z","path":"categories/index.html","title":"","comments":1,"layout":"page","_id":"cifsd5p3900029wbghqdr0nxl"},{"title":"关于","date":"2015-06-10T22:07:27.000Z","_content":"欢迎大家来到我的博客\n\n##About Lion's Pride Inn\n\n狮王之傲旅馆，艾尔文森里中闪金镇上的小店，不如暴风城里的镶金玫瑰旅店那样客流熙攘，也不像斯坦索姆里月桂旅店那样危机四伏，它提供美味的麦酒和面包，是每个初出茅庐的联盟冒险者探索新世界的必经之地．\n\n##About me\n###Skill\n- CS在读\n- Java,  Python, C\n- 机器学习，轨迹数据挖掘，遥感处理\n\n###Favorites:\n- <夜行者>： the art of negotiaton and logic\n###Contact:\n- zrb915@live.com\n##About blog\n\n- 博文会以技术为主，原创为主，也许会偶尔有感而发，写点风马牛不想及的想法，权且一乐。\n- 也会逐步把有道云笔记中的各类总结迁移过来。\n\t- Node.js\n\t- Hexo\n\t- EJS\n\t- GitHub Pages\n\t- 多说\n\t- 七牛\n\t- CNZZ\n\n\n","source":"about/index.md","raw":"title: 关于\ndate: 2015-06-10 18:07:27\n---\n欢迎大家来到我的博客\n\n##About Lion's Pride Inn\n\n狮王之傲旅馆，艾尔文森里中闪金镇上的小店，不如暴风城里的镶金玫瑰旅店那样客流熙攘，也不像斯坦索姆里月桂旅店那样危机四伏，它提供美味的麦酒和面包，是每个初出茅庐的联盟冒险者探索新世界的必经之地．\n\n##About me\n###Skill\n- CS在读\n- Java,  Python, C\n- 机器学习，轨迹数据挖掘，遥感处理\n\n###Favorites:\n- <夜行者>： the art of negotiaton and logic\n###Contact:\n- zrb915@live.com\n##About blog\n\n- 博文会以技术为主，原创为主，也许会偶尔有感而发，写点风马牛不想及的想法，权且一乐。\n- 也会逐步把有道云笔记中的各类总结迁移过来。\n\t- Node.js\n\t- Hexo\n\t- EJS\n\t- GitHub Pages\n\t- 多说\n\t- 七牛\n\t- CNZZ\n\n\n","updated":"2015-10-15T15:07:53.000Z","path":"about/index.html","comments":1,"layout":"page","_id":"cifsd5p3a00039wbgazg8f45r"}],"Post":[{"title":"逻辑回归与计算中的向量化思想","date":"2015-08-13T21:36:33.000Z","mathjax":true,"_content":"\n##引子\n- 这部分总结的有点晚了，不过在熟悉机器学习体系和几个其它模型后，再回过头来看逻辑回归，发现从原理到思想都有比较好的理解。\n- 理论主要参考NG的斯坦福课程和李航老师的《统计学习方法》，代码实现参见《机器学习实战》。\n- 向量化计算的思想在逻辑回归代码中体现的很好，利用矩阵计算上的便利性来解决复杂的循环过程。但由于《实战》书中的原理推导及其吝惜笔墨（与其简洁优美的代码对比鲜明），这里做一下补充。\n- 感谢[洞庭之子](http://blog.csdn.net/dongtingzhizi/article/details/15962797)这篇博客，解决了我的很多疑问，很佩服作者的思路和严谨的推导,但这篇博客中的有些写法不规范，容易引起误解。\n\n\n##逻辑回归\n###思想\n- **根据数据对分类边界线建立回归公式:** 与感知机乃至SVM大同小异，都是寻找一个`超平面`将数据集分为两部分。基于如此，逻辑回归一般只能处理两分类问题，同时两个类别线性可分。对于`多分类问题`，还是老思想，化用二分类（目标类为一类，剩余唯一类），构建多个分类器，寻找概率最大的那个类作为分类结果。\n- **通过分类函数（sigmoid函数）寻找分类超平面**： 具体sigmoid函数相关的内容下面有详细叙述.\n- **判别模型的老思路：**假设特征系数$\\theta$，构造预测函数 ——> 构造损失函数 ——> 求解最优化问题：寻找使损失函数最小時的特征系数$\\theta$ ——> 得到分类器（即超平面）。\n- **优缺点：** 计算简单，训练分类器后计算量小;准确度有限，容易欠拟合，只针对二分类问题。\n\n###分类函数Sigmoid\n- 逻辑回归选择`近似于阶越函数`的Sigmoid函数作为分类函数：\n$$\\sigma(z_i) = \\dfrac{1}{1+e^{-z}}，\n(z_i = \\theta_0x_i^{(0)}+\\theta_1x_i^{(1)}+..+\\theta_nx_i^{(m)} )$$\n- 函数图像：![sigmod](http://7xjz3b.com1.z0.glb.clouddn.com/blog4-1.png)\n- $\\theta$为特征系数向量, 每个特征都有一个特征系数$\\theta_n$。另外，$x_i^{(j)}$为输入向量$x_i$的第j个分量，\n- 作用：1. 逻辑回归的分类函数。 2. 将样本映射到0-1区间，进而巧妙地将数据到分界线的距离转化为概率，然后通过最大斯然估计等方法求解，这一些后面会谈到。\n\n###问题求解\n求解的过程这里简单讲讲，具体内容（比如阶梯求导结果等）不在赘述，可以参阅参考博客。\n判别模型的基本套路：\n**预测函数 ——> 构造损失函数or最大斯然估计求发生概率 ——> 求解最优化问题**：\n- 1、预测函数：\n$$h\\_{\\theta}(x)=\\sigma(z_i) = \\dfrac{1}{1+e^{-z}}，\n(z_i = \\theta_0x_i^{(0)}+\\theta_1x_i^{(1)}+..+\\theta_nx_i^{(m)} )$$\n\nsigmoid的函数值可以表示成分类概率：\n\\begin{equation\\*}\n\\begin{aligned}\nP(y=1|x,\\theta) & = h\\_\\theta(x) \\\\\\\nP(y=0|x,\\theta) & = 1-h\\_\\theta(x)\n\\end{aligned}\n\\end{equation\\*}\n\n- 2、通过最大斯然估计求发生概率，同时可构造损失函数：\n$$\nP(y|x,\\theta)=(h\\_{\\theta}(x))^y(1-h\\_{\\theta}(x))^{1-y}\n$$\n取似然函数：\n{% raw %}\n$$\nL(\\theta)  = \\prod_{i=1}^nP(y_i|x_i,\\theta) = \\prod_{i=1}^n (h_{\\theta}(x_i))^{y_i}(1-h_{\\theta}(x_i))^{1-y_i}\n$$\n{% endraw %}\n\n\n取对数：\n{% raw %}\n\\begin{equation*}\n\\begin{aligned}\nl(\\theta) & = \\log L(\\theta) \\\\\n& =\\sum_{i=1}^n(y_i(\\log h_{\\theta}(x_i)+(1-y_i)\\log{(1-h_{\\theta}(x_i))}\n\\end{aligned}\n\\end{equation*}\n{% endraw %}\n到这里，我们就可以用最优化方法求解$l(\\theta)$，本例用的是梯度上升法。\n在斯坦福ML的课程中，取$J(\\theta)=-\\dfrac{1}{m}l(\\theta)$,构建损失函数，然后利用梯度下降法求解$\\theta$，本质是完全一样的。\n- 3、借助梯度上升法求解最优值：\n {% raw %}\n \\begin{equation*}\n\\begin{aligned}\n\\theta_j & = \\theta_j + \\alpha\\dfrac{\\delta}{\\delta\\theta_j}J(\\theta) \\\\\n& = \\theta_j+\\alpha\\sum_{i=1}^{n}(y_i - h_{\\theta}(x_i))x_i^{(j)}\n\\end{aligned}\n\\end{equation*}\n($特征维数j = 0...m,、\\alpha为学习步长$)\n\\begin{equation*}\n\\begin{aligned}\n\\dfrac{\\delta}{\\delta\\theta_j}J(\\theta) & = \\sum_{i=1}^n\\left[y_i\\dfrac{1}{h_\\theta(x_i)}\\dfrac{\\delta}{\\delta\\theta_j} h_\\theta(x_i)-(1-y_i)\\dfrac{1}{1-h_\\theta(x_i)}\\dfrac{\\delta}{\\delta\\theta_j} h_\\theta(x_i) \\right]   \\\\\\\n& = .....\\\\\n& = \\theta_j+\\alpha\\sum_{i=1}^{n}(y_i - h_{\\theta}(x_i))x_i^{(j)}\n\\end{aligned}\n\\end{equation*}\n{% endraw %}\n这里省去了大部分求偏导的过程，详细的求解步骤，可以参见参考博客。博文虽用的是梯度下降法，但每一步的求偏导数过程是完全一致的。\n\n##计算中的向量化思想\n- 在求解最优化方法時，将数据向量化，用矩阵的方式计算，是一种很好的思想。\n- 借助numpy等包，使得以前针对单个数值编写的方法，对矩阵也有了很好的支持度。\n- 《机器学习实战》的实现代码中，使用梯度上升法求解$\\theta$時，就用了向量化思想，用矩阵乘法代替了循环。但书中直接给出了迭代求$\\theta$的公式，缺少了如上节类似的推导，初读还是有些令人费解。\n###上节已知求$\\theta$每一步的更新过程：\n{% raw %}\n\\begin{equation}\n\\begin{aligned}\n\\theta_j & = \\theta_j + \\alpha\\dfrac{\\delta}{\\delta\\theta_j}J(\\theta) \\\\\\\n& = \\theta_j+\\alpha\\sum_{i=1}^{n}(y_i - h_{\\theta}(x_i))x_i^{(j)}\n\\end{aligned}\n\\end{equation}\n{% endraw %}\n\n- 1、首先我们把特征系数$\\theta$用m×1的列向量表示。(假设有n个输入向量，特征共有m维,$\\theta_0x_0当作常量偏移$)：\n\\begin{equation\\*}\n\\theta = \\begin{bmatrix}\n\\theta_0 \\\\\\\n\\theta_1 \\\\\\\n... \\\\\\\n\\theta_m\n\\end{bmatrix}\n\\end{equation\\*}\n\n输入数据用n×m的矩阵表示：\n\\begin{equation\\*}\nX = \\begin{bmatrix}\nx_1 \\\\\\\nx_2 \\\\\\\n... \\\\\\\nx_n\n\\end{bmatrix} = \n\\begin{bmatrix}\nx_1^{(0)},x_1^{(1)}...x_1^{(m)} \\\\\\\nx_2^{(0)},x_2^{(1)}...x_2^{(m)}  \\\\\\\n... \\\\\\\nx_n^{(0)},x_n^{(1)}...x_n^{(m)} \n\\end{bmatrix}\n\\end{equation\\*}\n所以$z_n = \\theta_0x_n^{(0)}+\\theta_1x_n^{(1)}+..+\\theta_nx_n^{(m)} $可以用$X\\cdot\\theta$得出的n×1列向量来表示：\n\\begin{equation\\*}\nZ = \\begin{bmatrix}\n\\theta_0x_1^{(0)}+\\theta_1x_1^{(1)}+...+\\theta_mx_1^{(m)} \\\\\\\n\\theta_0x_2^{(0)}+\\theta_1x_2^{(1)}+...+\\theta_mx_2^{(m)}   \\\\\\\n... \\\\\\\n\\theta_0x_n^{(0)}+\\theta_1x_n^{(1)}+...+\\theta_mx_n^{(m)}  \n\\end{bmatrix} =\\begin{bmatrix}\nz_1 \\\\\\\nz_2 \\\\\\\n... \\\\\\\nz_n\n\\end{bmatrix}\n\\end{equation\\*}\n用矩阵乘法的列向量分解思想来说，$X\\cdot\\theta$表示$X$中的每一个列向量以系数$\\theta$进行`线性组合`，符合公式的含义。\n- 2、我们再用列向量$Y = (y\\_1,y\\_2..y\\_n)^\\mathrm{T}$表示输入数据的类别，然后我们对$Z$进行sigmoid函数运算，所以更新过程公式中$(y\\_i - h\\_{\\theta}(x\\_i))$可以用n×1的列向量E表示：\n\\begin{equation\\*}\nE = \\begin{bmatrix}\ny_1-\\sigma(z_1) \\\\\\\ny_2-\\sigma(z_2)  \\\\\\\n... \\\\\\\ny_n-\\sigma(z_n)  \n\\end{bmatrix} =\\begin{bmatrix}\ne_1 \\\\\\\ne_2 \\\\\\\n... \\\\\\\ne_n\n\\end{bmatrix}\n\\end{equation\\*}\n(这里的E代表误差error)\n- 3、最后我们求解整个式子$\\theta\\_j  = \\theta\\_j+\\alpha\\sum\\_{i=1}^{n}(y\\_i - h\\_{\\theta}(x\\_i))x\\_i^{(j)}$. 式中的连加同样可以通过矩阵乘法解决。\n将$X$转置：\n{% raw %}\n\\begin{equation*}\nX^\\mathrm{T} = \\begin{bmatrix}\nx_1,\nx_2,\n... \nx_n\n\\end{bmatrix} = \n\\begin{bmatrix}\nx_1^{(0)},x_2^{(0)}...x_n^{(0)} \\\\\nx_1^{(1)},x_2^{(1)}...x_n^{(1)}  \\\\\n...... \\\\\nx_1^{(m)},x_2^{(m)}...x_n^{(m)} \n\\end{bmatrix}\n\\end{equation*}\n{% endraw %}\n\n转置后矩阵是m×n的，仔细观察一下转置后的X，每一个列向量是一条输入数据，而**每一个行向量是所有输入数据在某一个特征维度上的记录**，一共有m个行向量。\n所以，我们同时用每一个行向量$\\cdot$列向量$E$,用矩阵乘法即：\n{% raw %}\n\\begin{equation*}\nX^\\mathrm{T}\\cdot\\mathrm{E} = \n\\begin{bmatrix}\nx_1^{(0)}e_1+x_2^{(0)}e_2+...+x_n^{(0)}e_n \\\\\nx_1^{(1)}e_1+x_2^{(1)}e_2+...+x_n^{(1)}e_n \\\\\n...... \\\\\\\nx_1^{(m)}e_1,x_2^{(m)}e_2+...+x_n^{(m)}e_n \n\\end{bmatrix}     \n\\end{equation*}\n{% endraw %}\n(m*1)\n- 4、已知增长系数$\\alpha$，然后我们就可以用矩阵计算进行一次梯度上升迭代：\n{% raw %}\n\\begin{equation*}\n\\theta_{n+1} = \\theta_n +\\alpha \\cdot X^\\mathrm{T}\\cdot\\mathrm{E} = \n\\begin{bmatrix}\n\\theta_{n+1}^{(0)} \\\\\n\\theta_{n+1}^{(1)} \\\\\n... \\\\\n\\theta_{n+1}^{(m)}\n\\end{bmatrix}     \n\\end{equation*}\n{% endraw %}\n\n5. 设置合适的迭代次数，求得最终的特征系数$\\theta$,我们就得到了训练好的判别函数。\n##小结：\n- 看完我们会发现，逻辑回归也是基于简单的线性分类思想，只不过逻辑回归通过一个契合线性分类的sigmoid函数，将数据到分界限的距离巧妙地投影到了0-1区间，进而可以将距离转化为概率，通过最大斯然估计求解。\n- 向量化的思想其实充斥着机器学习的各个角落。就像MIT线性代数公开课中说的，矩阵并不是生来存在的，而是人们后天发明用来方便计算的产物。将数据向量化，通过矩阵运算求解，是解决问题的必经之路。\n---\n这次公式比较多，处理markdown语法和$\\mathrm{\\LaTeX}$的冲突時花了不少时间，不过也因此找到一个好方法，不用修改node.js的配置就可以处理好语法冲突，只需要在```公式的前后用raw标签注释就可以了```，原理应该是通过标签屏蔽了markdown语法的解释器，前端不是很懂，不过很有效哦～\n\n\n\n\n\n","source":"_posts/逻辑回归与计算中的向量化思想.md","raw":"title: 逻辑回归与计算中的向量化思想\ndate: 2015-08-13 17:36:33\ntags: \n- 逻辑回归\n- 判别模型\n- 分类\n- 机器学习\n- 线性代数\ncategories: \n- 技术\nmathjax: true \n---\n\n##引子\n- 这部分总结的有点晚了，不过在熟悉机器学习体系和几个其它模型后，再回过头来看逻辑回归，发现从原理到思想都有比较好的理解。\n- 理论主要参考NG的斯坦福课程和李航老师的《统计学习方法》，代码实现参见《机器学习实战》。\n- 向量化计算的思想在逻辑回归代码中体现的很好，利用矩阵计算上的便利性来解决复杂的循环过程。但由于《实战》书中的原理推导及其吝惜笔墨（与其简洁优美的代码对比鲜明），这里做一下补充。\n- 感谢[洞庭之子](http://blog.csdn.net/dongtingzhizi/article/details/15962797)这篇博客，解决了我的很多疑问，很佩服作者的思路和严谨的推导,但这篇博客中的有些写法不规范，容易引起误解。\n\n\n##逻辑回归\n###思想\n- **根据数据对分类边界线建立回归公式:** 与感知机乃至SVM大同小异，都是寻找一个`超平面`将数据集分为两部分。基于如此，逻辑回归一般只能处理两分类问题，同时两个类别线性可分。对于`多分类问题`，还是老思想，化用二分类（目标类为一类，剩余唯一类），构建多个分类器，寻找概率最大的那个类作为分类结果。\n- **通过分类函数（sigmoid函数）寻找分类超平面**： 具体sigmoid函数相关的内容下面有详细叙述.\n- **判别模型的老思路：**假设特征系数$\\theta$，构造预测函数 ——> 构造损失函数 ——> 求解最优化问题：寻找使损失函数最小時的特征系数$\\theta$ ——> 得到分类器（即超平面）。\n- **优缺点：** 计算简单，训练分类器后计算量小;准确度有限，容易欠拟合，只针对二分类问题。\n\n###分类函数Sigmoid\n- 逻辑回归选择`近似于阶越函数`的Sigmoid函数作为分类函数：\n$$\\sigma(z_i) = \\dfrac{1}{1+e^{-z}}，\n(z_i = \\theta_0x_i^{(0)}+\\theta_1x_i^{(1)}+..+\\theta_nx_i^{(m)} )$$\n- 函数图像：![sigmod](http://7xjz3b.com1.z0.glb.clouddn.com/blog4-1.png)\n- $\\theta$为特征系数向量, 每个特征都有一个特征系数$\\theta_n$。另外，$x_i^{(j)}$为输入向量$x_i$的第j个分量，\n- 作用：1. 逻辑回归的分类函数。 2. 将样本映射到0-1区间，进而巧妙地将数据到分界线的距离转化为概率，然后通过最大斯然估计等方法求解，这一些后面会谈到。\n\n###问题求解\n求解的过程这里简单讲讲，具体内容（比如阶梯求导结果等）不在赘述，可以参阅参考博客。\n判别模型的基本套路：\n**预测函数 ——> 构造损失函数or最大斯然估计求发生概率 ——> 求解最优化问题**：\n- 1、预测函数：\n$$h\\_{\\theta}(x)=\\sigma(z_i) = \\dfrac{1}{1+e^{-z}}，\n(z_i = \\theta_0x_i^{(0)}+\\theta_1x_i^{(1)}+..+\\theta_nx_i^{(m)} )$$\n\nsigmoid的函数值可以表示成分类概率：\n\\begin{equation\\*}\n\\begin{aligned}\nP(y=1|x,\\theta) & = h\\_\\theta(x) \\\\\\\nP(y=0|x,\\theta) & = 1-h\\_\\theta(x)\n\\end{aligned}\n\\end{equation\\*}\n\n- 2、通过最大斯然估计求发生概率，同时可构造损失函数：\n$$\nP(y|x,\\theta)=(h\\_{\\theta}(x))^y(1-h\\_{\\theta}(x))^{1-y}\n$$\n取似然函数：\n{% raw %}\n$$\nL(\\theta)  = \\prod_{i=1}^nP(y_i|x_i,\\theta) = \\prod_{i=1}^n (h_{\\theta}(x_i))^{y_i}(1-h_{\\theta}(x_i))^{1-y_i}\n$$\n{% endraw %}\n\n\n取对数：\n{% raw %}\n\\begin{equation*}\n\\begin{aligned}\nl(\\theta) & = \\log L(\\theta) \\\\\n& =\\sum_{i=1}^n(y_i(\\log h_{\\theta}(x_i)+(1-y_i)\\log{(1-h_{\\theta}(x_i))}\n\\end{aligned}\n\\end{equation*}\n{% endraw %}\n到这里，我们就可以用最优化方法求解$l(\\theta)$，本例用的是梯度上升法。\n在斯坦福ML的课程中，取$J(\\theta)=-\\dfrac{1}{m}l(\\theta)$,构建损失函数，然后利用梯度下降法求解$\\theta$，本质是完全一样的。\n- 3、借助梯度上升法求解最优值：\n {% raw %}\n \\begin{equation*}\n\\begin{aligned}\n\\theta_j & = \\theta_j + \\alpha\\dfrac{\\delta}{\\delta\\theta_j}J(\\theta) \\\\\n& = \\theta_j+\\alpha\\sum_{i=1}^{n}(y_i - h_{\\theta}(x_i))x_i^{(j)}\n\\end{aligned}\n\\end{equation*}\n($特征维数j = 0...m,、\\alpha为学习步长$)\n\\begin{equation*}\n\\begin{aligned}\n\\dfrac{\\delta}{\\delta\\theta_j}J(\\theta) & = \\sum_{i=1}^n\\left[y_i\\dfrac{1}{h_\\theta(x_i)}\\dfrac{\\delta}{\\delta\\theta_j} h_\\theta(x_i)-(1-y_i)\\dfrac{1}{1-h_\\theta(x_i)}\\dfrac{\\delta}{\\delta\\theta_j} h_\\theta(x_i) \\right]   \\\\\\\n& = .....\\\\\n& = \\theta_j+\\alpha\\sum_{i=1}^{n}(y_i - h_{\\theta}(x_i))x_i^{(j)}\n\\end{aligned}\n\\end{equation*}\n{% endraw %}\n这里省去了大部分求偏导的过程，详细的求解步骤，可以参见参考博客。博文虽用的是梯度下降法，但每一步的求偏导数过程是完全一致的。\n\n##计算中的向量化思想\n- 在求解最优化方法時，将数据向量化，用矩阵的方式计算，是一种很好的思想。\n- 借助numpy等包，使得以前针对单个数值编写的方法，对矩阵也有了很好的支持度。\n- 《机器学习实战》的实现代码中，使用梯度上升法求解$\\theta$時，就用了向量化思想，用矩阵乘法代替了循环。但书中直接给出了迭代求$\\theta$的公式，缺少了如上节类似的推导，初读还是有些令人费解。\n###上节已知求$\\theta$每一步的更新过程：\n{% raw %}\n\\begin{equation}\n\\begin{aligned}\n\\theta_j & = \\theta_j + \\alpha\\dfrac{\\delta}{\\delta\\theta_j}J(\\theta) \\\\\\\n& = \\theta_j+\\alpha\\sum_{i=1}^{n}(y_i - h_{\\theta}(x_i))x_i^{(j)}\n\\end{aligned}\n\\end{equation}\n{% endraw %}\n\n- 1、首先我们把特征系数$\\theta$用m×1的列向量表示。(假设有n个输入向量，特征共有m维,$\\theta_0x_0当作常量偏移$)：\n\\begin{equation\\*}\n\\theta = \\begin{bmatrix}\n\\theta_0 \\\\\\\n\\theta_1 \\\\\\\n... \\\\\\\n\\theta_m\n\\end{bmatrix}\n\\end{equation\\*}\n\n输入数据用n×m的矩阵表示：\n\\begin{equation\\*}\nX = \\begin{bmatrix}\nx_1 \\\\\\\nx_2 \\\\\\\n... \\\\\\\nx_n\n\\end{bmatrix} = \n\\begin{bmatrix}\nx_1^{(0)},x_1^{(1)}...x_1^{(m)} \\\\\\\nx_2^{(0)},x_2^{(1)}...x_2^{(m)}  \\\\\\\n... \\\\\\\nx_n^{(0)},x_n^{(1)}...x_n^{(m)} \n\\end{bmatrix}\n\\end{equation\\*}\n所以$z_n = \\theta_0x_n^{(0)}+\\theta_1x_n^{(1)}+..+\\theta_nx_n^{(m)} $可以用$X\\cdot\\theta$得出的n×1列向量来表示：\n\\begin{equation\\*}\nZ = \\begin{bmatrix}\n\\theta_0x_1^{(0)}+\\theta_1x_1^{(1)}+...+\\theta_mx_1^{(m)} \\\\\\\n\\theta_0x_2^{(0)}+\\theta_1x_2^{(1)}+...+\\theta_mx_2^{(m)}   \\\\\\\n... \\\\\\\n\\theta_0x_n^{(0)}+\\theta_1x_n^{(1)}+...+\\theta_mx_n^{(m)}  \n\\end{bmatrix} =\\begin{bmatrix}\nz_1 \\\\\\\nz_2 \\\\\\\n... \\\\\\\nz_n\n\\end{bmatrix}\n\\end{equation\\*}\n用矩阵乘法的列向量分解思想来说，$X\\cdot\\theta$表示$X$中的每一个列向量以系数$\\theta$进行`线性组合`，符合公式的含义。\n- 2、我们再用列向量$Y = (y\\_1,y\\_2..y\\_n)^\\mathrm{T}$表示输入数据的类别，然后我们对$Z$进行sigmoid函数运算，所以更新过程公式中$(y\\_i - h\\_{\\theta}(x\\_i))$可以用n×1的列向量E表示：\n\\begin{equation\\*}\nE = \\begin{bmatrix}\ny_1-\\sigma(z_1) \\\\\\\ny_2-\\sigma(z_2)  \\\\\\\n... \\\\\\\ny_n-\\sigma(z_n)  \n\\end{bmatrix} =\\begin{bmatrix}\ne_1 \\\\\\\ne_2 \\\\\\\n... \\\\\\\ne_n\n\\end{bmatrix}\n\\end{equation\\*}\n(这里的E代表误差error)\n- 3、最后我们求解整个式子$\\theta\\_j  = \\theta\\_j+\\alpha\\sum\\_{i=1}^{n}(y\\_i - h\\_{\\theta}(x\\_i))x\\_i^{(j)}$. 式中的连加同样可以通过矩阵乘法解决。\n将$X$转置：\n{% raw %}\n\\begin{equation*}\nX^\\mathrm{T} = \\begin{bmatrix}\nx_1,\nx_2,\n... \nx_n\n\\end{bmatrix} = \n\\begin{bmatrix}\nx_1^{(0)},x_2^{(0)}...x_n^{(0)} \\\\\nx_1^{(1)},x_2^{(1)}...x_n^{(1)}  \\\\\n...... \\\\\nx_1^{(m)},x_2^{(m)}...x_n^{(m)} \n\\end{bmatrix}\n\\end{equation*}\n{% endraw %}\n\n转置后矩阵是m×n的，仔细观察一下转置后的X，每一个列向量是一条输入数据，而**每一个行向量是所有输入数据在某一个特征维度上的记录**，一共有m个行向量。\n所以，我们同时用每一个行向量$\\cdot$列向量$E$,用矩阵乘法即：\n{% raw %}\n\\begin{equation*}\nX^\\mathrm{T}\\cdot\\mathrm{E} = \n\\begin{bmatrix}\nx_1^{(0)}e_1+x_2^{(0)}e_2+...+x_n^{(0)}e_n \\\\\nx_1^{(1)}e_1+x_2^{(1)}e_2+...+x_n^{(1)}e_n \\\\\n...... \\\\\\\nx_1^{(m)}e_1,x_2^{(m)}e_2+...+x_n^{(m)}e_n \n\\end{bmatrix}     \n\\end{equation*}\n{% endraw %}\n(m*1)\n- 4、已知增长系数$\\alpha$，然后我们就可以用矩阵计算进行一次梯度上升迭代：\n{% raw %}\n\\begin{equation*}\n\\theta_{n+1} = \\theta_n +\\alpha \\cdot X^\\mathrm{T}\\cdot\\mathrm{E} = \n\\begin{bmatrix}\n\\theta_{n+1}^{(0)} \\\\\n\\theta_{n+1}^{(1)} \\\\\n... \\\\\n\\theta_{n+1}^{(m)}\n\\end{bmatrix}     \n\\end{equation*}\n{% endraw %}\n\n5. 设置合适的迭代次数，求得最终的特征系数$\\theta$,我们就得到了训练好的判别函数。\n##小结：\n- 看完我们会发现，逻辑回归也是基于简单的线性分类思想，只不过逻辑回归通过一个契合线性分类的sigmoid函数，将数据到分界限的距离巧妙地投影到了0-1区间，进而可以将距离转化为概率，通过最大斯然估计求解。\n- 向量化的思想其实充斥着机器学习的各个角落。就像MIT线性代数公开课中说的，矩阵并不是生来存在的，而是人们后天发明用来方便计算的产物。将数据向量化，通过矩阵运算求解，是解决问题的必经之路。\n---\n这次公式比较多，处理markdown语法和$\\mathrm{\\LaTeX}$的冲突時花了不少时间，不过也因此找到一个好方法，不用修改node.js的配置就可以处理好语法冲突，只需要在```公式的前后用raw标签注释就可以了```，原理应该是通过标签屏蔽了markdown语法的解释器，前端不是很懂，不过很有效哦～\n\n\n\n\n\n","slug":"逻辑回归与计算中的向量化思想","published":1,"updated":"2015-08-17T02:59:26.000Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cifsd5p3d00049wbgj82tfhuh"},{"title":"谈谈Linux的环境变量与启动文件","date":"2015-06-22T15:53:59.000Z","_content":"\n\n最近在搭建hadoop 的环境，在配置环境变量的时候遇到一些问题，借这个机会系统总结了Linux的环境变量配置以及作用,以供备忘，主要参考了[《Linux命令行与Shell脚本大全》](http://book.douban.com/subject/11589828/)及部分网上文章。\n\n##1.  环境变量的作用  \n在不同系统中，环境变量总是扮演着相同的角色，安装完新环境，新软件，第一件事总是配置环境变量。\n###环境变量 ： \n- 存储有关shell会话和`工作环境`的信息（如Java，hadoop，python等路径，主要是各个bin文件夹的绝对地址）\n- 变量储存在`内存`中，方便程序以及脚本访问\n\n\n##2.  环境变量的分类：\n\n###局部变量：\n- 特性：`作用于当前shell，`在子shell或者其它shell不可见（反映了局部变量的在当前进程可用的意义）  **见例1**\n- 定义：\n```bash \n        $  varname=abc       #局部变量名尽量使用小写; = 左右不要有空格，否则变量名会被解析为命令\n```\n- 显示： \n```bash\n        $  echo $varname     #echo命令要使用$变量名\n```\n- 删除：       \n```bash\n        $ unset varname   \n```\n\n###全局变量：\n- 特性：作用于当前shell`及所有shell创建的子进程`; 系统登录时已经默认设置了许多全局环境变量。  **见例2**\n- 定义：\n```bash\n        $  varname=abc \n        $  export varname    #先建立局部变量，用export导成全局变量\n```\n- 显示：\n```bash\n        $ printenv                 #显示系统环境变量，系统环境变量一律大写  \n        $ echo $varname      #显示单个变量\n```\n\n- 删除： \n```bash\n        $ unset varname      #只对子进程中变量有效，父进程中的全局变量仍然存在。\n```\n\n##3.  系统默认定义的全局环境变量\nPATH变量是我们经常使用和修改的环境变量，它定义了`提供命令解析的搜索路径`。所以每当我们安装了新的环境，总要在更新PATH变量，这样就可以直接在任何位置使用命令，而不会出现commond not found的问题。 <br />　　其它系统变量的修改与添加也大致一样。\n###PATH变量：  \n- 特性： 命令行输入命令的搜索路径, 如：\n```bash\n    $ start-dfs.sh   #启动HDFS，而不用进入../hadoop2.5.2/sbin/目录下\n```\n- 查看： \n```bash\n    $ echo $PATH\n    /usr/local/bin:/usr/bin:/bin:/usr/local/games:/usr/games    #不同值之间由:分割\n```\n- 添加值，修改：\n```bash\n    $ PATH=$PATH:/home/user/test  #添加目录/home/user/test, 只在当前shell有效\n    $ export PATH=.:/HOME/eobin:$PATH     #单点符在PATH变量中代表当前路径，变量名可以放在末尾\n```\n通常我们把对于PATH变量的改动`写在系统或用户的启动文件中`，比如说~/.bashrc中，在登陆系统或者打开新shell时自动加载。\n\n##4. 启动文件与系统环境变量的加载：\n\n###/etc/profile:    \n- 作用：系统默认的`主启动文件`，每个用户登录时都会加载这个文件。\n- 变量持续时间：声明的变量会在每个新shell中存在，除非在子进程中被修改\n\n###$HOME/.bash_profile:  \n- 作用：用户专属的启动文件， 定义用户专属的环境变量，该文件会`先检查并加载HOME目录中的.bashrc文件`（如果存在）\n\n\n\n###$HOME/.bashrc:   \n- 作用： `交互式shell的启动文件`，用于定制自己的命名别名和私有脚本\n- 这里有一点疑问，书中提到\n>如果bash是作为交互式shell启动的，它不去访问/etc/profile, 而会去用户的HOME目录检查.bashrc是否存在  - p116 《Linux命令行与Shell脚本大全》\n\n但/etc/profile是随系统登录时就启动的，任何`交互式shell`都是其子进程，所以我认为不用访问自然会加载其中变量，而此处特地指出，不明其所以然，也许没有理解交互式shell的意义。当然结果并没有什么不同，都会以.bashrc中的设置为准。\n\n\n##5. 附例：\n\n- **例1**   局部变量只在当前进程中有效\n```bash\nrobin@esp:~$ varname=abc       定义变量varname\nrobin@esp:~$ echo $varname     显示，有值\nabc\nrobin@esp:~$ bash              新建子进程\nrobin@esp:~$ echo $varname     显示，无值\n\nrobin@esp:~$ exit              退出子进程\nexit\nrobin@esp:~$ echo $varname     显示，有值\nabc\n```\n\n- **例2**   全局变量的作用范围\n```bash\nrobin@esp:~$ echo $varname           显示变量，无值\n\nrobin@esp:~$ bash                    新建子进程2\nrobin@esp:~$ export varname=bash2    定义全局变量\nrobin@esp:~$ echo $varname           显示，有值\nbash2\nrobin@esp:~$ bash                    新建子进程3\nrobin@esp:~$ echo $varname           显示，有值\nbash2\nrobin@esp:~$ exit                    退出到bash2\nexit\nrobin@esp:~$ exit                    退出到原始进程\nexit\nrobin@esp:~$ echo $varname           显示，无值\n```\n\n###\n##END\n\nRobin \n2015.6.22 夜\n","source":"_posts/谈谈Linux的环境变量与启动文件.md","raw":"title: 谈谈Linux的环境变量与启动文件\ndate: 2015-06-22 11:53:59\ntags: \n- linux\n- 环境变量\ncategories: \n- 技术\n---\n\n\n最近在搭建hadoop 的环境，在配置环境变量的时候遇到一些问题，借这个机会系统总结了Linux的环境变量配置以及作用,以供备忘，主要参考了[《Linux命令行与Shell脚本大全》](http://book.douban.com/subject/11589828/)及部分网上文章。\n\n##1.  环境变量的作用  \n在不同系统中，环境变量总是扮演着相同的角色，安装完新环境，新软件，第一件事总是配置环境变量。\n###环境变量 ： \n- 存储有关shell会话和`工作环境`的信息（如Java，hadoop，python等路径，主要是各个bin文件夹的绝对地址）\n- 变量储存在`内存`中，方便程序以及脚本访问\n\n\n##2.  环境变量的分类：\n\n###局部变量：\n- 特性：`作用于当前shell，`在子shell或者其它shell不可见（反映了局部变量的在当前进程可用的意义）  **见例1**\n- 定义：\n```bash \n        $  varname=abc       #局部变量名尽量使用小写; = 左右不要有空格，否则变量名会被解析为命令\n```\n- 显示： \n```bash\n        $  echo $varname     #echo命令要使用$变量名\n```\n- 删除：       \n```bash\n        $ unset varname   \n```\n\n###全局变量：\n- 特性：作用于当前shell`及所有shell创建的子进程`; 系统登录时已经默认设置了许多全局环境变量。  **见例2**\n- 定义：\n```bash\n        $  varname=abc \n        $  export varname    #先建立局部变量，用export导成全局变量\n```\n- 显示：\n```bash\n        $ printenv                 #显示系统环境变量，系统环境变量一律大写  \n        $ echo $varname      #显示单个变量\n```\n\n- 删除： \n```bash\n        $ unset varname      #只对子进程中变量有效，父进程中的全局变量仍然存在。\n```\n\n##3.  系统默认定义的全局环境变量\nPATH变量是我们经常使用和修改的环境变量，它定义了`提供命令解析的搜索路径`。所以每当我们安装了新的环境，总要在更新PATH变量，这样就可以直接在任何位置使用命令，而不会出现commond not found的问题。 <br />　　其它系统变量的修改与添加也大致一样。\n###PATH变量：  \n- 特性： 命令行输入命令的搜索路径, 如：\n```bash\n    $ start-dfs.sh   #启动HDFS，而不用进入../hadoop2.5.2/sbin/目录下\n```\n- 查看： \n```bash\n    $ echo $PATH\n    /usr/local/bin:/usr/bin:/bin:/usr/local/games:/usr/games    #不同值之间由:分割\n```\n- 添加值，修改：\n```bash\n    $ PATH=$PATH:/home/user/test  #添加目录/home/user/test, 只在当前shell有效\n    $ export PATH=.:/HOME/eobin:$PATH     #单点符在PATH变量中代表当前路径，变量名可以放在末尾\n```\n通常我们把对于PATH变量的改动`写在系统或用户的启动文件中`，比如说~/.bashrc中，在登陆系统或者打开新shell时自动加载。\n\n##4. 启动文件与系统环境变量的加载：\n\n###/etc/profile:    \n- 作用：系统默认的`主启动文件`，每个用户登录时都会加载这个文件。\n- 变量持续时间：声明的变量会在每个新shell中存在，除非在子进程中被修改\n\n###$HOME/.bash_profile:  \n- 作用：用户专属的启动文件， 定义用户专属的环境变量，该文件会`先检查并加载HOME目录中的.bashrc文件`（如果存在）\n\n\n\n###$HOME/.bashrc:   \n- 作用： `交互式shell的启动文件`，用于定制自己的命名别名和私有脚本\n- 这里有一点疑问，书中提到\n>如果bash是作为交互式shell启动的，它不去访问/etc/profile, 而会去用户的HOME目录检查.bashrc是否存在  - p116 《Linux命令行与Shell脚本大全》\n\n但/etc/profile是随系统登录时就启动的，任何`交互式shell`都是其子进程，所以我认为不用访问自然会加载其中变量，而此处特地指出，不明其所以然，也许没有理解交互式shell的意义。当然结果并没有什么不同，都会以.bashrc中的设置为准。\n\n\n##5. 附例：\n\n- **例1**   局部变量只在当前进程中有效\n```bash\nrobin@esp:~$ varname=abc       定义变量varname\nrobin@esp:~$ echo $varname     显示，有值\nabc\nrobin@esp:~$ bash              新建子进程\nrobin@esp:~$ echo $varname     显示，无值\n\nrobin@esp:~$ exit              退出子进程\nexit\nrobin@esp:~$ echo $varname     显示，有值\nabc\n```\n\n- **例2**   全局变量的作用范围\n```bash\nrobin@esp:~$ echo $varname           显示变量，无值\n\nrobin@esp:~$ bash                    新建子进程2\nrobin@esp:~$ export varname=bash2    定义全局变量\nrobin@esp:~$ echo $varname           显示，有值\nbash2\nrobin@esp:~$ bash                    新建子进程3\nrobin@esp:~$ echo $varname           显示，有值\nbash2\nrobin@esp:~$ exit                    退出到bash2\nexit\nrobin@esp:~$ exit                    退出到原始进程\nexit\nrobin@esp:~$ echo $varname           显示，无值\n```\n\n###\n##END\n\nRobin \n2015.6.22 夜\n","slug":"谈谈Linux的环境变量与启动文件","published":1,"updated":"2015-10-15T09:59:46.000Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cifsd5p3j000h9wbg3hutczmu"},{"title":"荆棘谷的青山","date":"2015-06-10T22:07:27.000Z","_content":"\n##正文\n\n- 在GitPages安家了，第一篇博文，又是新的一行＂Hello world＂，一次新的开始．\n\n- 建博客的念头由来已久，但又久未实行．总在挑什么语言，选什么框架，却忘了博客应该做什么．仔细想想，抛开技术因素，内容才是博客价值所在．\n\n- 博文会以技术为主，原创为主，偶尔也会写点风马牛不想及的东西，权且一乐。有时间也会逐步把有道云笔记的一直以来积累的东西，挑一挑转过来。希望能做到少而精，有些意义．就像荆棘谷里的那个知名任务，一片片零散的残章断卷后，讲述了一个美妙的故事．\n\n- 一直以来也拜读了许多大咖的博客，细致度与原创性都令人佩服，不求超越，只求也像他们一样有一颗沉静的心，能仔细的把好东西沉淀下来。\n\n- 读博路上人渐稀。正值毕业季，可惜这次我是一个孤独的旁观者，只好贪婪的呼吸点弥漫在空气之中的喜悦．前路漫漫，希望能从这篇博文开始，真正的做点事情.\n\n\n  \n 　　 \n　　\n---\n\n### 最后附一段Eminem的歌词，烂熟于心，很喜欢：\n\n> There's batteries in my Walkman nothing isthe matter with me\n> 即使生活充满悲剧，只要我Walkman还有电，困难算什么东西\n> Shit look on the bright side at least Iain't walking\n> 想想开心的事，至少我还有辆单车，是一个老司机\n> I bike ride through the neighborhood of myapartment\n> 我嗨皮的驶过邻居家门口\n> Complex on a ten speed which I've acquiredparts that I\n> 骑着这辆时速十码的组装车\n> Found in the garbage, a frame and put tireson it\n> 从垃圾堆里找的部件，然后把轮胎装到架子上\n> Headphones on straight ahead and kids tryto start shit\n> 耳机在耳边轰鸣，夹杂着熊孩子们的嘲笑\n> But if this all there is for me life offers\n> 如果这是生活赐予我的全部\n> Why bother even tryna put up a fight, it'snonsense\n> 那我为何还要与别人争斗？这毫无意义\n> But I think a lightbulb just lit up in my conscience\n> 突然有些东西，它照亮了我的脑海\n> What about those rhymes I've been jottin'\n> 是那些我随手记下的词句\n> They are kinda givin’ me confidence\n> 它一直给予我自信和希望\n> Instead of tryna escape through my comics,\n> 与其挣扎在这喜剧般的人生\n> Why don't I just blast a little somethinglike Onyx\n> 为何不像Onyx组合一样唱点什么？\n\n\n\nRobin\n2015.6.10   夜","source":"_posts/荆棘谷的青山.md","raw":"title: 荆棘谷的青山\ndate: 2015-06-10 18:07:27\ntags: 纪念\ncategories: 生活\n---\n\n##正文\n\n- 在GitPages安家了，第一篇博文，又是新的一行＂Hello world＂，一次新的开始．\n\n- 建博客的念头由来已久，但又久未实行．总在挑什么语言，选什么框架，却忘了博客应该做什么．仔细想想，抛开技术因素，内容才是博客价值所在．\n\n- 博文会以技术为主，原创为主，偶尔也会写点风马牛不想及的东西，权且一乐。有时间也会逐步把有道云笔记的一直以来积累的东西，挑一挑转过来。希望能做到少而精，有些意义．就像荆棘谷里的那个知名任务，一片片零散的残章断卷后，讲述了一个美妙的故事．\n\n- 一直以来也拜读了许多大咖的博客，细致度与原创性都令人佩服，不求超越，只求也像他们一样有一颗沉静的心，能仔细的把好东西沉淀下来。\n\n- 读博路上人渐稀。正值毕业季，可惜这次我是一个孤独的旁观者，只好贪婪的呼吸点弥漫在空气之中的喜悦．前路漫漫，希望能从这篇博文开始，真正的做点事情.\n\n\n  \n 　　 \n　　\n---\n\n### 最后附一段Eminem的歌词，烂熟于心，很喜欢：\n\n> There's batteries in my Walkman nothing isthe matter with me\n> 即使生活充满悲剧，只要我Walkman还有电，困难算什么东西\n> Shit look on the bright side at least Iain't walking\n> 想想开心的事，至少我还有辆单车，是一个老司机\n> I bike ride through the neighborhood of myapartment\n> 我嗨皮的驶过邻居家门口\n> Complex on a ten speed which I've acquiredparts that I\n> 骑着这辆时速十码的组装车\n> Found in the garbage, a frame and put tireson it\n> 从垃圾堆里找的部件，然后把轮胎装到架子上\n> Headphones on straight ahead and kids tryto start shit\n> 耳机在耳边轰鸣，夹杂着熊孩子们的嘲笑\n> But if this all there is for me life offers\n> 如果这是生活赐予我的全部\n> Why bother even tryna put up a fight, it'snonsense\n> 那我为何还要与别人争斗？这毫无意义\n> But I think a lightbulb just lit up in my conscience\n> 突然有些东西，它照亮了我的脑海\n> What about those rhymes I've been jottin'\n> 是那些我随手记下的词句\n> They are kinda givin’ me confidence\n> 它一直给予我自信和希望\n> Instead of tryna escape through my comics,\n> 与其挣扎在这喜剧般的人生\n> Why don't I just blast a little somethinglike Onyx\n> 为何不像Onyx组合一样唱点什么？\n\n\n\nRobin\n2015.6.10   夜","slug":"荆棘谷的青山","published":1,"updated":"2015-07-20T08:05:24.000Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cifsd5p3m000n9wbgkjchzmea"},{"title":"简谈贝叶斯决策","date":"2015-07-14T13:44:57.000Z","mathjax":true,"_content":"\n\n**贝叶斯决策**, 一种常用的分类方法，复杂度低，准确度好，可以和多种分类方法结合。最高的理论正确率令人神往（上自动化所模式识别课中多次强调）。\n写一写学习贝叶斯方法中的几点理解，一个简单脉络而已，方便回顾，与大家分享。\n\n-------------------\n##1. 先验概率与后验概率\n两个概念：\n- `先验概率`： 事情还没有发生，要求这件事情发生的可能性的大小。如P(A).\n- `后验概率`： 事情已经发生，要求这件事情发生的原因`是由某个因素引起的`可能性的大小，是后验概率. \n\n所以我们可以使用贝叶斯公式求后验概率，进而达到分类的目的。\n\n\n##2. 贝叶斯公式\n\n$$ P(F\\_j|E)= \\dfrac{P(F\\_jE)}{P(E)} =  \\dfrac{P(E|F\\_j)P(F\\_j)}{ \\displaystyle{\\sum_{i=1}^{n} }P(E|F\\_i)P(F\\_i) } $$\n\n两个核心：\n- **推导**：条件概率公式。分解中间步骤分子，我们得出结果$P(F\\_j|E) \\cdot P(E)= P(F\\_jE) =   P(E|F\\_j) \\cdot P(F\\_j)$.   \n\n- **应用**：用于分类问题。分解中间步骤分母$P(E) = \\displaystyle{\\sum\\_{i=1}^{n} }P(E|F\\_i)P(F\\_i）$ ，每个$F\\_i$代表一个类别。\n\n扯点零碎，条件概率公式对任意两个事件都是存在的，当两事件独立(即$P(EF) = P(E)P(F)$)时，$P(E|F) = P(EF)/P(F) = P(E)$.\n\n\n\n##3. 例子\n看过很多讲解贝叶斯公式的例子，下面这个很好:\n> 一份信件等可能的在存在三个不同的文件夹中的任意一个。假设信件实际在文件夹$i$中$(i = 1,2,3)$而你经过对文件夹$i$的快速翻阅发现信件的概率记为$a\\_i$，问题是，假定你查看了文件夹1且没有发现此信，问信件在文件夹1中的概率是多少？\n                                                              ——[《应用随机过程：概率模型导论》](http://book.douban.com/subject/2309401/)\n\n**解**： 以$F\\_i(i = 1,2,3)$为信在文件中$i$中的事件`（先验概率）`，E是通过对文件夹1搜索而为看到信这个事件，我们求$P(F\\_1|E)$`(后验概率)`，通过贝叶斯公式：\n$$\tP(F\\_1|E)=  \\dfrac{P(E|F\\_1)P(F\\_1)}{ \\displaystyle{\\sum\\_{i=1}^{3} }P(E|F\\_i)P(F\\_i) } = \\dfrac{(1-a\\_1) \\frac{1}{3}}{ (1-a\\_1) \\frac{1}{3}+\\frac{1}{3}+\\frac{1}{3}} = \\dfrac{1-a\\_1}{3-a\\_1}$$\n\n把握两个重点：\n\t- 对问题中事件的假设\n\t- 对整体事件空间(分母)的划分\n\n\n##4. 一种简单实现：朴素贝叶斯\n贝叶斯思想只是一个框架，可结合许多假设与分布。\n朴素思分类就是一个通过先验概率求后验概率的方法，然后将实例点分到后验概率最大的类。就像街上有一个黑人，我们预测黑人来自非洲，为什么呢？因为黑人中非洲人的比率最高。\n####朴素贝叶斯的核心假设：\n1. 特征之间相互独立 ：这个“朴素”一词的来源。假设体现在向量的特征之间。所以有\n$$P(x|y\\_i)P(y\\_i)=P(a\\_1|y\\_i)P(a\\_2|y\\_i)...P(a\\_m|y\\_i)P(y\\_i)=P(y\\_i)\\prod^m_{j=1}P(a\\_j|y\\_i)$$\n\t- $y\\_i$为每一个类别，$a\\_i$ 为样本X的特征的每一维分量\n\t\n2. 每个特征同等重要。\n\n\n##5. 分类准则\n\n将输入向量分到后验概率最大的类，这就是`分类准则`。贝叶斯使用的是`最大后验概率准则`。\n在别的分类方法中，我们还用过`最小错误率`的分类准则，比如基于0-1损失的最小错误率，这两者是一样的。\n\n设损失函数\n\n$L(Y,f(X))=$\n\n\\begin{align\\*}\n1,Y \\not= f(X) \\\\\\\n0, Y= f(X) \\\\\\\n\\end{align\\*}\n\n\n- 期望风险函数为\n\n\\begin{equation\\*}\n\\begin{aligned}  \nR\\_e(f(x)) & = E[L(Y, f(x))]\\\\\\\n& =  \\iint\\_{X,Y}L(Y,f(X))P(X,Y)\\mathrm{d}x\\mathrm{d}y \\\\\\\n&= \\iint\\_{X,Y}L(Y,f(X))P(Y|X)P(X)\\mathrm{d}x\\mathrm{d}y\\\\\\\n&= \\int\\_{X} (\\sum\\_{k=1}^{k}L(Y= c\\_k,f(X))P(Y=c\\_k|X) )P(X)\\mathrm{d}x\\\\\\\n&= E\\_x(\\sum_{k=1}^{k}L(Y= c\\_k,f(X))P(Y=c\\_k|X) )\n\\end{aligned}\n\\end{equation\\*}\n\n- 为了使期望最小，对$X=x$逐个极小化：\n\n\\begin{equation\\*}\n\\begin{aligned} \nf(x)& = \\arg\\min \\limits\\_{y\\in\\mathcal{Y}}\\sum\\_{k=1}^{k}L(c\\_k,y)P(c\\_k|X=x) \\\\\\\n&=  \\arg\\min \\limits\\_{y\\in\\mathcal{Y}}\\sum\\_{k=1}^{k}P(y \\not = c\\_k|X=x) \\\\\\\n&=  \\arg\\min \\limits\\_{y\\in\\mathcal{Y}}\\sum\\_{k=1}^{k}(1-P(y  = c\\_k|X=x) )\\\\\\\n&=  \\arg\\max \\limits\\_{y\\in\\mathcal{Y}}P(y = c\\_k|X = x)\n\\end{aligned}\n\\end{equation\\*}\n\n可以看出，这就是`最大后验概率准则`，也是我们使用贝叶斯决策的分类准则。\n\n\n---\n\n暂时先写这几点，顺带说一句，Hexo上写$\\mathrm{\\LaTeX}$公式，冲突比较多，还在寻找比较好的解决方法，\n\nRobin \n2015.7.10 夜 \n","source":"_posts/简谈贝叶斯决策.md","raw":"title: 简谈贝叶斯决策\ndate: 2015-07-14 09:44:57\ntags: \n- 贝叶斯\n- 分类\n- 机器学习\ncategories: \n- 技术\nmathjax: true \n---\n\n\n**贝叶斯决策**, 一种常用的分类方法，复杂度低，准确度好，可以和多种分类方法结合。最高的理论正确率令人神往（上自动化所模式识别课中多次强调）。\n写一写学习贝叶斯方法中的几点理解，一个简单脉络而已，方便回顾，与大家分享。\n\n-------------------\n##1. 先验概率与后验概率\n两个概念：\n- `先验概率`： 事情还没有发生，要求这件事情发生的可能性的大小。如P(A).\n- `后验概率`： 事情已经发生，要求这件事情发生的原因`是由某个因素引起的`可能性的大小，是后验概率. \n\n所以我们可以使用贝叶斯公式求后验概率，进而达到分类的目的。\n\n\n##2. 贝叶斯公式\n\n$$ P(F\\_j|E)= \\dfrac{P(F\\_jE)}{P(E)} =  \\dfrac{P(E|F\\_j)P(F\\_j)}{ \\displaystyle{\\sum_{i=1}^{n} }P(E|F\\_i)P(F\\_i) } $$\n\n两个核心：\n- **推导**：条件概率公式。分解中间步骤分子，我们得出结果$P(F\\_j|E) \\cdot P(E)= P(F\\_jE) =   P(E|F\\_j) \\cdot P(F\\_j)$.   \n\n- **应用**：用于分类问题。分解中间步骤分母$P(E) = \\displaystyle{\\sum\\_{i=1}^{n} }P(E|F\\_i)P(F\\_i）$ ，每个$F\\_i$代表一个类别。\n\n扯点零碎，条件概率公式对任意两个事件都是存在的，当两事件独立(即$P(EF) = P(E)P(F)$)时，$P(E|F) = P(EF)/P(F) = P(E)$.\n\n\n\n##3. 例子\n看过很多讲解贝叶斯公式的例子，下面这个很好:\n> 一份信件等可能的在存在三个不同的文件夹中的任意一个。假设信件实际在文件夹$i$中$(i = 1,2,3)$而你经过对文件夹$i$的快速翻阅发现信件的概率记为$a\\_i$，问题是，假定你查看了文件夹1且没有发现此信，问信件在文件夹1中的概率是多少？\n                                                              ——[《应用随机过程：概率模型导论》](http://book.douban.com/subject/2309401/)\n\n**解**： 以$F\\_i(i = 1,2,3)$为信在文件中$i$中的事件`（先验概率）`，E是通过对文件夹1搜索而为看到信这个事件，我们求$P(F\\_1|E)$`(后验概率)`，通过贝叶斯公式：\n$$\tP(F\\_1|E)=  \\dfrac{P(E|F\\_1)P(F\\_1)}{ \\displaystyle{\\sum\\_{i=1}^{3} }P(E|F\\_i)P(F\\_i) } = \\dfrac{(1-a\\_1) \\frac{1}{3}}{ (1-a\\_1) \\frac{1}{3}+\\frac{1}{3}+\\frac{1}{3}} = \\dfrac{1-a\\_1}{3-a\\_1}$$\n\n把握两个重点：\n\t- 对问题中事件的假设\n\t- 对整体事件空间(分母)的划分\n\n\n##4. 一种简单实现：朴素贝叶斯\n贝叶斯思想只是一个框架，可结合许多假设与分布。\n朴素思分类就是一个通过先验概率求后验概率的方法，然后将实例点分到后验概率最大的类。就像街上有一个黑人，我们预测黑人来自非洲，为什么呢？因为黑人中非洲人的比率最高。\n####朴素贝叶斯的核心假设：\n1. 特征之间相互独立 ：这个“朴素”一词的来源。假设体现在向量的特征之间。所以有\n$$P(x|y\\_i)P(y\\_i)=P(a\\_1|y\\_i)P(a\\_2|y\\_i)...P(a\\_m|y\\_i)P(y\\_i)=P(y\\_i)\\prod^m_{j=1}P(a\\_j|y\\_i)$$\n\t- $y\\_i$为每一个类别，$a\\_i$ 为样本X的特征的每一维分量\n\t\n2. 每个特征同等重要。\n\n\n##5. 分类准则\n\n将输入向量分到后验概率最大的类，这就是`分类准则`。贝叶斯使用的是`最大后验概率准则`。\n在别的分类方法中，我们还用过`最小错误率`的分类准则，比如基于0-1损失的最小错误率，这两者是一样的。\n\n设损失函数\n\n$L(Y,f(X))=$\n\n\\begin{align\\*}\n1,Y \\not= f(X) \\\\\\\n0, Y= f(X) \\\\\\\n\\end{align\\*}\n\n\n- 期望风险函数为\n\n\\begin{equation\\*}\n\\begin{aligned}  \nR\\_e(f(x)) & = E[L(Y, f(x))]\\\\\\\n& =  \\iint\\_{X,Y}L(Y,f(X))P(X,Y)\\mathrm{d}x\\mathrm{d}y \\\\\\\n&= \\iint\\_{X,Y}L(Y,f(X))P(Y|X)P(X)\\mathrm{d}x\\mathrm{d}y\\\\\\\n&= \\int\\_{X} (\\sum\\_{k=1}^{k}L(Y= c\\_k,f(X))P(Y=c\\_k|X) )P(X)\\mathrm{d}x\\\\\\\n&= E\\_x(\\sum_{k=1}^{k}L(Y= c\\_k,f(X))P(Y=c\\_k|X) )\n\\end{aligned}\n\\end{equation\\*}\n\n- 为了使期望最小，对$X=x$逐个极小化：\n\n\\begin{equation\\*}\n\\begin{aligned} \nf(x)& = \\arg\\min \\limits\\_{y\\in\\mathcal{Y}}\\sum\\_{k=1}^{k}L(c\\_k,y)P(c\\_k|X=x) \\\\\\\n&=  \\arg\\min \\limits\\_{y\\in\\mathcal{Y}}\\sum\\_{k=1}^{k}P(y \\not = c\\_k|X=x) \\\\\\\n&=  \\arg\\min \\limits\\_{y\\in\\mathcal{Y}}\\sum\\_{k=1}^{k}(1-P(y  = c\\_k|X=x) )\\\\\\\n&=  \\arg\\max \\limits\\_{y\\in\\mathcal{Y}}P(y = c\\_k|X = x)\n\\end{aligned}\n\\end{equation\\*}\n\n可以看出，这就是`最大后验概率准则`，也是我们使用贝叶斯决策的分类准则。\n\n\n---\n\n暂时先写这几点，顺带说一句，Hexo上写$\\mathrm{\\LaTeX}$公式，冲突比较多，还在寻找比较好的解决方法，\n\nRobin \n2015.7.10 夜 \n","slug":"简谈贝叶斯决策","published":1,"updated":"2015-08-24T08:49:35.000Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cifsd5p3p000s9wbgrcljpyyh"}],"PostAsset":[],"PostCategory":[{"post_id":"cifsd5p3d00049wbgj82tfhuh","category_id":"cifsd5p3f00059wbgymyvwsmu","_id":"cifsd5p3h00089wbggytbyrxb"},{"post_id":"cifsd5p3j000h9wbg3hutczmu","category_id":"cifsd5p3f00059wbgymyvwsmu","_id":"cifsd5p3k000i9wbgzgzxpc5n"},{"post_id":"cifsd5p3m000n9wbgkjchzmea","category_id":"cifsd5p3n000o9wbg89qfhbg7","_id":"cifsd5p3n000r9wbgg0bmb5xv"},{"post_id":"cifsd5p3p000s9wbgrcljpyyh","category_id":"cifsd5p3f00059wbgymyvwsmu","_id":"cifsd5p3q000t9wbgbgvctf8s"}],"PostTag":[{"post_id":"cifsd5p3d00049wbgj82tfhuh","tag_id":"cifsd5p3g00069wbgietdu6jl","_id":"cifsd5p3i000c9wbg0xrst0qa"},{"post_id":"cifsd5p3d00049wbgj82tfhuh","tag_id":"cifsd5p3h00079wbglorj2lhd","_id":"cifsd5p3i000d9wbg17eh741z"},{"post_id":"cifsd5p3d00049wbgj82tfhuh","tag_id":"cifsd5p3h00099wbgbprdg7ao","_id":"cifsd5p3i000e9wbgdttpcxdv"},{"post_id":"cifsd5p3d00049wbgj82tfhuh","tag_id":"cifsd5p3i000a9wbggz1k7hrh","_id":"cifsd5p3i000f9wbgf2ujsqcw"},{"post_id":"cifsd5p3d00049wbgj82tfhuh","tag_id":"cifsd5p3i000b9wbgvc9hz4eb","_id":"cifsd5p3i000g9wbg3xa78f8e"},{"post_id":"cifsd5p3j000h9wbg3hutczmu","tag_id":"cifsd5p3k000j9wbggdzmyip1","_id":"cifsd5p3l000l9wbgusfq3x7v"},{"post_id":"cifsd5p3j000h9wbg3hutczmu","tag_id":"cifsd5p3l000k9wbgtxh4guu9","_id":"cifsd5p3l000m9wbgrfjga2ke"},{"post_id":"cifsd5p3m000n9wbgkjchzmea","tag_id":"cifsd5p3n000p9wbg0j9gveau","_id":"cifsd5p3n000q9wbgxytriqmb"},{"post_id":"cifsd5p3p000s9wbgrcljpyyh","tag_id":"cifsd5p3q000u9wbgp9vjd8nb","_id":"cifsd5p3q000v9wbglmffyx2u"},{"post_id":"cifsd5p3p000s9wbgrcljpyyh","tag_id":"cifsd5p3h00099wbgbprdg7ao","_id":"cifsd5p3q000w9wbgcdb0363s"},{"post_id":"cifsd5p3p000s9wbgrcljpyyh","tag_id":"cifsd5p3i000a9wbggz1k7hrh","_id":"cifsd5p3q000x9wbgcny0ukrx"}],"Tag":[{"name":"逻辑回归","_id":"cifsd5p3g00069wbgietdu6jl"},{"name":"判别模型","_id":"cifsd5p3h00079wbglorj2lhd"},{"name":"分类","_id":"cifsd5p3h00099wbgbprdg7ao"},{"name":"机器学习","_id":"cifsd5p3i000a9wbggz1k7hrh"},{"name":"线性代数","_id":"cifsd5p3i000b9wbgvc9hz4eb"},{"name":"linux","_id":"cifsd5p3k000j9wbggdzmyip1"},{"name":"环境变量","_id":"cifsd5p3l000k9wbgtxh4guu9"},{"name":"纪念","_id":"cifsd5p3n000p9wbg0j9gveau"},{"name":"贝叶斯","_id":"cifsd5p3q000u9wbgp9vjd8nb"}]}}