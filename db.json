{"meta":{"version":1,"warehouse":"1.0.2"},"models":{"Asset":[{"_id":"source/images/scrollup.png","path":"images/scrollup.png","modified":0},{"_id":"source/images/logo.svg","path":"images/logo.svg","modified":0},{"_id":"source/images/logo.png","path":"images/logo.png","modified":0},{"_id":"source/images/jacman.jpg","path":"images/jacman.jpg","modified":0},{"_id":"source/images/favicon.ico","path":"images/favicon.ico","modified":0},{"_id":"source/images/cc-zero.svg","path":"images/cc-zero.svg","modified":0},{"_id":"source/images/cc-by.svg","path":"images/cc-by.svg","modified":0},{"_id":"source/images/cc-by-sa.svg","path":"images/cc-by-sa.svg","modified":0},{"_id":"source/images/cc-by-nd.svg","path":"images/cc-by-nd.svg","modified":0},{"_id":"source/images/cc-by-nc.svg","path":"images/cc-by-nc.svg","modified":0},{"_id":"source/images/cc-by-nc-sa.svg","path":"images/cc-by-nc-sa.svg","modified":0},{"_id":"source/images/cc-by-nc-nd.svg","path":"images/cc-by-nc-nd.svg","modified":0},{"_id":"source/images/banner.jpg","path":"images/banner.jpg","modified":0},{"_id":"source/images/avatar.jpg","path":"images/avatar.jpg","modified":0},{"_id":"source/images/author.jpg","path":"images/author.jpg","modified":0},{"_id":"themes/next/source/vendors/velocity/velocity.ui.min.js","path":"vendors/velocity/velocity.ui.min.js","modified":0},{"_id":"themes/next/source/vendors/velocity/velocity.ui.js","path":"vendors/velocity/velocity.ui.js","modified":0},{"_id":"themes/next/source/vendors/velocity/velocity.min.js","path":"vendors/velocity/velocity.min.js","modified":0},{"_id":"themes/next/source/vendors/velocity/velocity.js","path":"vendors/velocity/velocity.js","modified":0},{"_id":"themes/next/source/vendors/velocity/bower.json","path":"vendors/velocity/bower.json","modified":0},{"_id":"themes/next/source/vendors/jquery_lazyload/jquery.scrollstop.js","path":"vendors/jquery_lazyload/jquery.scrollstop.js","modified":0},{"_id":"themes/next/source/vendors/jquery_lazyload/jquery.lazyload.js","path":"vendors/jquery_lazyload/jquery.lazyload.js","modified":0},{"_id":"themes/next/source/vendors/jquery_lazyload/bower.json","path":"vendors/jquery_lazyload/bower.json","modified":0},{"_id":"themes/next/source/vendors/jquery_lazyload/README.md","path":"vendors/jquery_lazyload/README.md","modified":0},{"_id":"themes/next/source/vendors/jquery_lazyload/CONTRIBUTING.md","path":"vendors/jquery_lazyload/CONTRIBUTING.md","modified":0},{"_id":"themes/next/source/vendors/jquery/index.js","path":"vendors/jquery/index.js","modified":0},{"_id":"themes/next/source/vendors/font-awesome/fonts/fontawesome-webfont.woff2","path":"vendors/font-awesome/fonts/fontawesome-webfont.woff2","modified":0},{"_id":"themes/next/source/vendors/font-awesome/fonts/fontawesome-webfont.woff","path":"vendors/font-awesome/fonts/fontawesome-webfont.woff","modified":0},{"_id":"themes/next/source/vendors/font-awesome/fonts/fontawesome-webfont.ttf","path":"vendors/font-awesome/fonts/fontawesome-webfont.ttf","modified":0},{"_id":"themes/next/source/vendors/font-awesome/fonts/fontawesome-webfont.svg","path":"vendors/font-awesome/fonts/fontawesome-webfont.svg","modified":0},{"_id":"themes/next/source/vendors/font-awesome/fonts/fontawesome-webfont.eot","path":"vendors/font-awesome/fonts/fontawesome-webfont.eot","modified":0},{"_id":"themes/next/source/vendors/font-awesome/fonts/FontAwesome.otf","path":"vendors/font-awesome/fonts/FontAwesome.otf","modified":0},{"_id":"themes/next/source/vendors/font-awesome/css/font-awesome.min.css","path":"vendors/font-awesome/css/font-awesome.min.css","modified":0},{"_id":"themes/next/source/vendors/font-awesome/css/font-awesome.css.map","path":"vendors/font-awesome/css/font-awesome.css.map","modified":0},{"_id":"themes/next/source/vendors/font-awesome/css/font-awesome.css","path":"vendors/font-awesome/css/font-awesome.css","modified":0},{"_id":"themes/next/source/vendors/font-awesome/bower.json","path":"vendors/font-awesome/bower.json","modified":0},{"_id":"themes/next/source/vendors/font-awesome/HELP-US-OUT.txt","path":"vendors/font-awesome/HELP-US-OUT.txt","modified":0},{"_id":"themes/next/source/vendors/fastclick/lib/fastclick.min.js","path":"vendors/fastclick/lib/fastclick.min.js","modified":0},{"_id":"themes/next/source/vendors/fastclick/lib/fastclick.js","path":"vendors/fastclick/lib/fastclick.js","modified":0},{"_id":"themes/next/source/vendors/fastclick/bower.json","path":"vendors/fastclick/bower.json","modified":0},{"_id":"themes/next/source/vendors/fastclick/README.md","path":"vendors/fastclick/README.md","modified":0},{"_id":"themes/next/source/vendors/fastclick/LICENSE","path":"vendors/fastclick/LICENSE","modified":0},{"_id":"themes/next/source/vendors/fancybox/source/jquery.fancybox.pack.js","path":"vendors/fancybox/source/jquery.fancybox.pack.js","modified":0},{"_id":"themes/next/source/vendors/fancybox/source/jquery.fancybox.js","path":"vendors/fancybox/source/jquery.fancybox.js","modified":0},{"_id":"themes/next/source/vendors/fancybox/source/jquery.fancybox.css","path":"vendors/fancybox/source/jquery.fancybox.css","modified":0},{"_id":"themes/next/source/vendors/fancybox/source/helpers/jquery.fancybox-thumbs.js","path":"vendors/fancybox/source/helpers/jquery.fancybox-thumbs.js","modified":0},{"_id":"themes/next/source/vendors/fancybox/source/helpers/jquery.fancybox-thumbs.css","path":"vendors/fancybox/source/helpers/jquery.fancybox-thumbs.css","modified":0},{"_id":"themes/next/source/vendors/fancybox/source/helpers/jquery.fancybox-media.js","path":"vendors/fancybox/source/helpers/jquery.fancybox-media.js","modified":0},{"_id":"themes/next/source/vendors/fancybox/source/helpers/jquery.fancybox-buttons.js","path":"vendors/fancybox/source/helpers/jquery.fancybox-buttons.js","modified":0},{"_id":"themes/next/source/vendors/fancybox/source/helpers/jquery.fancybox-buttons.css","path":"vendors/fancybox/source/helpers/jquery.fancybox-buttons.css","modified":0},{"_id":"themes/next/source/vendors/fancybox/source/helpers/fancybox_buttons.png","path":"vendors/fancybox/source/helpers/fancybox_buttons.png","modified":0},{"_id":"themes/next/source/vendors/fancybox/source/fancybox_sprite@2x.png","path":"vendors/fancybox/source/fancybox_sprite@2x.png","modified":0},{"_id":"themes/next/source/vendors/fancybox/source/fancybox_sprite.png","path":"vendors/fancybox/source/fancybox_sprite.png","modified":0},{"_id":"themes/next/source/vendors/fancybox/source/fancybox_overlay.png","path":"vendors/fancybox/source/fancybox_overlay.png","modified":0},{"_id":"themes/next/source/vendors/fancybox/source/fancybox_loading@2x.gif","path":"vendors/fancybox/source/fancybox_loading@2x.gif","modified":0},{"_id":"themes/next/source/vendors/fancybox/source/fancybox_loading.gif","path":"vendors/fancybox/source/fancybox_loading.gif","modified":0},{"_id":"themes/next/source/vendors/fancybox/source/blank.gif","path":"vendors/fancybox/source/blank.gif","modified":0},{"_id":"themes/next/source/js/ua-parser.min.js","path":"js/ua-parser.min.js","modified":0},{"_id":"themes/next/source/js/motion.js","path":"js/motion.js","modified":0},{"_id":"themes/next/source/js/hook-duoshuo.js","path":"js/hook-duoshuo.js","modified":0},{"_id":"themes/next/source/js/helpers.js","path":"js/helpers.js","modified":0},{"_id":"themes/next/source/js/fancy-box.js","path":"js/fancy-box.js","modified":0},{"_id":"themes/next/source/js/bootstrap.scrollspy.js","path":"js/bootstrap.scrollspy.js","modified":0},{"_id":"themes/next/source/js/bootstrap.js","path":"js/bootstrap.js","modified":0},{"_id":"themes/next/source/images/searchicon.png","path":"images/searchicon.png","modified":0},{"_id":"themes/next/source/images/quote-r.svg","path":"images/quote-r.svg","modified":0},{"_id":"themes/next/source/images/quote-l.svg","path":"images/quote-l.svg","modified":0},{"_id":"themes/next/source/images/placeholder.gif","path":"images/placeholder.gif","modified":0},{"_id":"themes/next/source/images/loading.gif","path":"images/loading.gif","modified":0},{"_id":"themes/next/source/images/cc-zero.svg","path":"images/cc-zero.svg","modified":0},{"_id":"themes/next/source/images/cc-by.svg","path":"images/cc-by.svg","modified":0},{"_id":"themes/next/source/images/cc-by-sa.svg","path":"images/cc-by-sa.svg","modified":0},{"_id":"themes/next/source/images/cc-by-nd.svg","path":"images/cc-by-nd.svg","modified":0},{"_id":"themes/next/source/images/cc-by-nc.svg","path":"images/cc-by-nc.svg","modified":0},{"_id":"themes/next/source/images/cc-by-nc-sa.svg","path":"images/cc-by-nc-sa.svg","modified":0},{"_id":"themes/next/source/images/cc-by-nc-nd.svg","path":"images/cc-by-nc-nd.svg","modified":0},{"_id":"themes/next/source/css/main.styl","path":"css/main.styl","modified":0}],"Cache":[{"_id":"source/_posts/scikt-learn示例解析 色彩量化Color Quantization using K-Means.md","shasum":"fb2cc5f203b0b44338029a32e03335e5b3bf7d25","modified":1447905055000},{"_id":"source/_posts/卷积的理解.md","shasum":"891e7022c20c8a0b3deea75c5c870998a1012b2a","modified":1447933061000},{"_id":"source/_posts/理解Linux的环境变量、启动文件与命令目录.md","shasum":"8a9bc4d7420ae9a33276c5c9084cf1a6d77d79f5","modified":1447900859000},{"_id":"source/_posts/简谈贝叶斯决策.md","shasum":"ef672deda4986bfb464119274dceda6123ae5cb8","modified":1447068172000},{"_id":"source/_posts/荆棘谷的青山.md","shasum":"5d039f8f79418fe23c706aa3f72021d37ba87348","modified":1445847315000},{"_id":"source/_posts/迁移Hexo博客：GitHub至GitCafe.md","shasum":"38ae64d8c2e95e0b46a096e6271ef31fba311e04","modified":1447292733000},{"_id":"source/_posts/逻辑回归与计算中的向量化思想.md","shasum":"fcfc1e201f2370134a62c9b7f558e2f79862a10c","modified":1439780366000},{"_id":"source/about/index.md","shasum":"892da060b3a04a2fb1618ed752650f444f941157","modified":1445081954000},{"_id":"source/baidu_verify_bhY7p6c45b.html","shasum":"a777ee7595f9fc5e733f2c14a251e98013fca57e","modified":1445485964000},{"_id":"source/categories/index.md","shasum":"9bc9e6dda975955e994487a45aee79a9c1a0b9af","modified":1433923816000},{"_id":"source/google89c38fd0efaaa284.html","shasum":"758f4c21cfa370fcd984b9d5645ccc0881e2c575","modified":1445484811000},{"_id":"source/images/author.jpg","shasum":"9058722ba5f9f68672a90cdf8d609ca2bcaae7e2","modified":1433846706000},{"_id":"source/images/avatar.jpg","shasum":"9058722ba5f9f68672a90cdf8d609ca2bcaae7e2","modified":1433846706000},{"_id":"source/images/banner.jpg","shasum":"219156face68a3d4505064ff3f02f87423b47606","modified":1434288372000},{"_id":"source/images/cc-by-nc-nd.svg","shasum":"c6524ece3f8039a5f612feaf865d21ec8a794564","modified":1433846040000},{"_id":"source/images/cc-by-nc-sa.svg","shasum":"3031be41e8753c70508aa88e84ed8f4f653f157e","modified":1433846040000},{"_id":"source/images/cc-by-nc.svg","shasum":"8d39b39d88f8501c0d27f8df9aae47136ebc59b7","modified":1433846040000},{"_id":"source/images/cc-by-nd.svg","shasum":"c563508ce9ced1e66948024ba1153400ac0e0621","modified":1433846040000},{"_id":"source/images/cc-by-sa.svg","shasum":"aa4742d733c8af8d38d4c183b8adbdcab045872e","modified":1433846040000},{"_id":"source/images/cc-by.svg","shasum":"28a0a4fe355a974a5e42f68031652b76798d4f7e","modified":1433846040000},{"_id":"source/images/cc-zero.svg","shasum":"87669bf8ac268a91d027a0a4802c92a1473e9030","modified":1433846040000},{"_id":"source/images/favicon.ico","shasum":"ec9660de66ff2a0e682e1a5ee7b7d8ee85c585c4","modified":1434008589000},{"_id":"source/images/jacman.jpg","shasum":"0ba14a4a5e3be012826fc713c33479912126d34e","modified":1433846040000},{"_id":"source/images/logo.png","shasum":"2a87d4212af648b58859c97f43f3200390e96f44","modified":1434008868000},{"_id":"source/images/logo.svg","shasum":"9ae38f7225c38624faeb7b74996efa9de7bf065b","modified":1433846040000},{"_id":"source/images/scrollup.png","shasum":"2137d4f1739aa8aa3fcb0348c3ddf1e41d62f2e3","modified":1433846040000},{"_id":"source/resume/index.md","shasum":"c3529e5af684421e579a192b7237d76096249b38","modified":1434007719000},{"_id":"source/tags/index.md","shasum":"481a9fb5d52f83983f5f3e5856fcc007b480fd26","modified":1447903055000},{"_id":"themes/next/source/css/_common/_page/home.styl","shasum":"da39a3ee5e6b4b0d3255bfef95601890afd80709","modified":1447897401000},{"_id":"themes/next/source/css/_mixins/Mist.styl","shasum":"da39a3ee5e6b4b0d3255bfef95601890afd80709","modified":1447897401000},{"_id":"themes/next/source/css/_mixins/custom.styl","shasum":"da39a3ee5e6b4b0d3255bfef95601890afd80709","modified":1447897401000},{"_id":"themes/next/source/css/_mixins/default.styl","shasum":"da39a3ee5e6b4b0d3255bfef95601890afd80709","modified":1447897401000},{"_id":"themes/next/source/css/_variables/default.styl","shasum":"da39a3ee5e6b4b0d3255bfef95601890afd80709","modified":1447897401000},{"_id":"themes/next/README.en.md","shasum":"565ba52b3825b85a9f05b41183caca7f18b741d4","modified":1447897401000},{"_id":"themes/next/README.md","shasum":"0b709591995001cd860384d6c189e51d91690714","modified":1447897401000},{"_id":"themes/next/_config.yml","shasum":"c305a1b10f8b2f6f9b8c52f7c9b49df8736adda7","modified":1447904653000},{"_id":"themes/next/_config.yml~","shasum":"106974f256e53c6c29de61878fbee14fdbd67270","modified":1447904487000},{"_id":"themes/next/bower.json","shasum":"4a53cab758c7d69be2ce773b2afff7dd962b7cb0","modified":1447897401000},{"_id":"themes/next/languages/de.yml","shasum":"7a8de0e5665c52a1bf168c1e7dd222c8a74fb0ab","modified":1447897401000},{"_id":"themes/next/languages/default.yml","shasum":"f57623e47f533c8d53d859628fa6a368a5298a00","modified":1447897401000},{"_id":"themes/next/languages/en.yml","shasum":"f57623e47f533c8d53d859628fa6a368a5298a00","modified":1447897401000},{"_id":"themes/next/languages/fr-FR.yml","shasum":"2cec663601ac8d178e97aee91d967fa99a95ad4e","modified":1447897401000},{"_id":"themes/next/languages/pt.yml","shasum":"8e38fdf3a5232b428d2e4a641666dbabab87c3d1","modified":1447897401000},{"_id":"themes/next/languages/ru.yml","shasum":"1d1b158f9cff1b38978086043f299b3fc590e007","modified":1447897401000},{"_id":"themes/next/languages/zh-Hans.yml","shasum":"37413bccbc6bf9d86ecbebf3215e5480cafa878b","modified":1447933775000},{"_id":"themes/next/languages/zh-hk.yml","shasum":"248b88c825fde8e35839f3954d38df4e72a0537c","modified":1447897401000},{"_id":"themes/next/languages/zh-tw.yml","shasum":"42ba1d0c6b6026ba1e613ad11efb75432a8132ac","modified":1447897401000},{"_id":"themes/next/layout/_layout.swig","shasum":"4efe52f310b797f4bdedacbd979caf285d2bc731","modified":1447897401000},{"_id":"themes/next/layout/_macro/post-collapse.swig","shasum":"9032ae9056cb19b4c2d069d66ead7abf828f9922","modified":1447897401000},{"_id":"themes/next/layout/_macro/post.swig","shasum":"f32eaace2afb0450cf230f96ee8c87f20d6a9072","modified":1447897401000},{"_id":"themes/next/layout/_macro/sidebar.swig","shasum":"88e179ff660874135b17d0febc91d04749053a73","modified":1447897401000},{"_id":"themes/next/layout/_partials/comments.swig","shasum":"a612a4eca51ffc87b53a5470b451071a7ad6a031","modified":1447897401000},{"_id":"themes/next/layout/_partials/duoshuo-hot-articles.swig","shasum":"5d4638c46aef65bf32a01681495b62416ccc98db","modified":1447897401000},{"_id":"themes/next/layout/_partials/footer.swig","shasum":"970a669aae1889d37b2028d174d9d45e88dc0e4f","modified":1447897401000},{"_id":"themes/next/layout/_partials/head.swig","shasum":"fcef099c268bd4964e65cfe3109e3fe99d0b925c","modified":1447897401000},{"_id":"themes/next/layout/_partials/header.swig","shasum":"ac252de15ed6d3389efd85ccaba63e87abe6705d","modified":1447897401000},{"_id":"themes/next/layout/_partials/old-browsers.swig","shasum":"dbbfea810bf3a2ed9c83b9a6683037175aacfc67","modified":1447897401000},{"_id":"themes/next/layout/_partials/pagination.swig","shasum":"9e8e21d194ef44d271b1cca0bc1448c14d7edf4f","modified":1447897401000},{"_id":"themes/next/layout/_partials/search/swiftype.swig","shasum":"00c2b49f6289198b0b2b4e157e4ee783277f32a7","modified":1447897401000},{"_id":"themes/next/layout/_partials/search/tinysou.swig","shasum":"eefe2388ff3d424694045eda21346989b123977c","modified":1447897401000},{"_id":"themes/next/layout/_partials/search.swig","shasum":"64f14da26792a17bc27836c4e9d83190175f36e6","modified":1447897401000},{"_id":"themes/next/layout/_partials/share/duoshuo_share.swig","shasum":"89c5a5240ecb223acfe1d12377df5562a943fd5d","modified":1447897401000},{"_id":"themes/next/layout/_partials/share/jiathis.swig","shasum":"63315fcf210799f894208c9f512737096df84962","modified":1447897401000},{"_id":"themes/next/layout/_scripts/analytics/baidu-analytics.swig","shasum":"7c43d66da93cde65b473a7d6db2a86f9a42647d6","modified":1447897401000},{"_id":"themes/next/layout/_scripts/analytics/facebook-sdk.swig","shasum":"334176d838ee528e58468d8bc74ff3a6d3f25b2b","modified":1447897401000},{"_id":"themes/next/layout/_scripts/analytics/google-analytics.swig","shasum":"30a23fa7e816496fdec0e932aa42e2d13098a9c2","modified":1447897401000},{"_id":"themes/next/layout/_scripts/analytics.swig","shasum":"33ca06b9bd9a15a19432d5396b85bd319f017319","modified":1447897401000},{"_id":"themes/next/layout/_scripts/baidushare.swig","shasum":"d726361945437cf6e48067b3dd041b7e36e98d85","modified":1447897401000},{"_id":"themes/next/layout/_scripts/bootstrap.scrollspy.swig","shasum":"85295f126836b95f0837d03e58228bb3cf8c4490","modified":1447897401000},{"_id":"themes/next/layout/_scripts/comments/disqus.swig","shasum":"3491d3cebabc8a28857200db28a1be65aad6adc2","modified":1447897401000},{"_id":"themes/next/layout/_scripts/comments/duoshuo.swig","shasum":"44e3d567fd49c2a093f4a0a8af9f00542c935a58","modified":1447897401000},{"_id":"themes/next/layout/_scripts/fancy-box.swig","shasum":"41b4ff1446060c88c33bf666a32277dcf12129f0","modified":1447897401000},{"_id":"themes/next/layout/_scripts/helpers.swig","shasum":"4d2cbfca0aaf546a2b5813288073e824c1498fdf","modified":1447897401000},{"_id":"themes/next/layout/_scripts/mathjax.swig","shasum":"df03220eb8526e17dc9c9f17780c2d6699367181","modified":1447897401000},{"_id":"themes/next/layout/_scripts/motion.swig","shasum":"0d9761e3b1bb9e666ccc71bad59f035deb5a88c6","modified":1447897401000},{"_id":"themes/next/layout/_scripts/pages/post-details.swig","shasum":"7a34b02808f144ee4a11032ae3a149eb634a7e82","modified":1447897401000},{"_id":"themes/next/layout/_scripts/tinysou.swig","shasum":"cb3a5d36dbe1630bab84e03a52733a46df7c219b","modified":1447897401000},{"_id":"themes/next/layout/archive.swig","shasum":"0c3ce594759f347ea90a4ce592a7a18e2ae4cc5c","modified":1447897401000},{"_id":"themes/next/layout/category.swig","shasum":"d6b3e1dc5e0b8deade9a084c463126e70188ee9b","modified":1447897401000},{"_id":"themes/next/layout/index.swig","shasum":"38b1ad401b748965369296b86327d23082a1fe93","modified":1447933993000},{"_id":"themes/next/layout/page.swig","shasum":"8019d02232a6dd1a665b6a4d2daef8e5dd2f0049","modified":1447897401000},{"_id":"themes/next/layout/post.swig","shasum":"a84457e8ced46e63bc7a8a9e0541a6ba53122a92","modified":1447897401000},{"_id":"themes/next/layout/tag.swig","shasum":"aab44af54fcbc66fea4ad12b2767ffca3eadd451","modified":1447897401000},{"_id":"themes/next/scripts/filters/sticky.js","shasum":"6b1ea0c09105352813357d0fff4e1d3f4c821fa3","modified":1447897401000},{"_id":"themes/next/scripts/merge-configs.js","shasum":"0c56be2e85c694247cfa327ea6d627b99ca265e8","modified":1447897401000},{"_id":"themes/next/scripts/tags/center-quote.js","shasum":"535fc542781021c4326dec24d8495cbb1387634a","modified":1447897401000},{"_id":"themes/next/scripts/tags/full-image.js","shasum":"3acce36db0feb11a982c6c799aa6b6b47df2827c","modified":1447897401000},{"_id":"themes/next/scripts/tags/group-pictures.js","shasum":"49252824cd53184dc9b97b2f2d87ff28e1b3ef27","modified":1447897401000},{"_id":"themes/next/source/css/_common/_component/back-to-top.styl","shasum":"88cd66910260006aa8e9e795df4948d4b67bfa11","modified":1447897401000},{"_id":"themes/next/source/css/_common/_component/blockquote-center.styl","shasum":"c2abe4d87148e23e15d49ee225bc650de60baf46","modified":1447897401000},{"_id":"themes/next/source/css/_common/_component/buttons.styl","shasum":"81063e0979f04a0f9af37f321d7321dda9abf593","modified":1447897401000},{"_id":"themes/next/source/css/_common/_component/comments.styl","shasum":"471f1627891aca5c0e1973e09fbcb01e1510d193","modified":1447897401000},{"_id":"themes/next/source/css/_common/_component/duoshuo.styl","shasum":"6ad988254fe34a03e8ddfa8d4941e196bd07d238","modified":1447897401000},{"_id":"themes/next/source/css/_common/_component/gallery.styl","shasum":"387ce23bba52b22a586b2dfb4ec618fe1ffd3926","modified":1447897401000},{"_id":"themes/next/source/css/_common/_component/group-pictures.styl","shasum":"1ee40743000173495728855f734081eb2b6167cc","modified":1447897401000},{"_id":"themes/next/source/css/_common/_component/jiathis.styl","shasum":"327b5f63d55ec26f7663185c1a778440588d9803","modified":1447897401000},{"_id":"themes/next/source/css/_common/_component/pagination.styl","shasum":"711c8830886619d4f4a0598b0cde5499dce50c62","modified":1447897401000},{"_id":"themes/next/source/css/_common/_component/posts-collapse.styl","shasum":"6750b61236eb359028da8f2c4765f7c89b03dc9a","modified":1447897401000},{"_id":"themes/next/source/css/_common/_component/posts-expand.styl","shasum":"e438aeb63315012ad1e7b4a5b7f302ce6249869f","modified":1447897401000},{"_id":"themes/next/source/css/_common/_component/posts-type.styl","shasum":"40b593134bf96d1d6095b3439d47820659d7f10b","modified":1447897401000},{"_id":"themes/next/source/css/_common/_component/posts.styl","shasum":"b05ac51dd266d27f12e39e59a94383fe6474b7b3","modified":1447897401000},{"_id":"themes/next/source/css/_common/_component/tag-cloud.styl","shasum":"dd8a3b22fc2f222ac6e6c05bd8a773fb039169c0","modified":1447897401000},{"_id":"themes/next/source/css/_common/_core/base.styl","shasum":"388aa7c69c97728c64941db01e0f29a88837120c","modified":1447897401000},{"_id":"themes/next/source/css/_common/_core/helpers.styl","shasum":"d339d114e52a9abbc797ec236a8a770c29e288a6","modified":1447897401000},{"_id":"themes/next/source/css/_common/_core/normalize.styl","shasum":"ece571f38180febaf02ace8187ead8318a300ea7","modified":1447897401000},{"_id":"themes/next/source/css/_common/_core/scaffolding.styl","shasum":"1f8acb3331300eec696a09e7859e11f191e16d7f","modified":1447897401000},{"_id":"themes/next/source/css/_common/_core/tables.styl","shasum":"5f766cf26f966dbf9dcfe681f40ab9032e3e8a08","modified":1447897401000},{"_id":"themes/next/source/css/_common/_fonts/icon-default.styl","shasum":"8b809aef383bebaeb3f282b47675f3a364ce3569","modified":1447897401000},{"_id":"themes/next/source/css/_common/_fonts/icon-feather.styl","shasum":"80413afacfa656322100ce1900fed1ebcd8f8f44","modified":1447897401000},{"_id":"themes/next/source/css/_common/_fonts/icon-fifty-shades.styl","shasum":"249f75bafa26b99d272352c0646e7497ea680b39","modified":1447897401000},{"_id":"themes/next/source/css/_common/_fonts/icon-font.styl","shasum":"ec3f86739bede393cafcd3e31052c01115ae20d6","modified":1447897401000},{"_id":"themes/next/source/css/_common/_fonts/icon-linecons.styl","shasum":"9cdbedb3627ac941cfb063b152abe5a75c3c699a","modified":1447897401000},{"_id":"themes/next/source/css/_common/_page/archive.styl","shasum":"dff879f55ca65fa79c07e9098719e53eeea7ac88","modified":1447897401000},{"_id":"themes/next/source/css/_common/_page/categories.styl","shasum":"4f696a2eaeee2f214adcf273eab25c62a398077a","modified":1447897401000},{"_id":"themes/next/source/css/_common/_page/post-detail.styl","shasum":"9bf4362a4d0ae151ada84b219d39fbe5bb8c790e","modified":1447897401000},{"_id":"themes/next/source/css/_common/_section/body.styl","shasum":"ca1a4766cbe25baac757c6b47a4858d221afdc40","modified":1447897401000},{"_id":"themes/next/source/css/_common/_section/footer.styl","shasum":"8994ffcce84deac0471532f270f97c44fea54dc0","modified":1447897401000},{"_id":"themes/next/source/css/_common/_section/header.styl","shasum":"e351f1ebc0d3c63e3443bcba3a34677b06c37455","modified":1447897401000},{"_id":"themes/next/source/css/_common/_section/layout.styl","shasum":"03ae7b808dde9065412968aa69916162e790455d","modified":1447897401000},{"_id":"themes/next/source/css/_common/_section/media.styl","shasum":"fa9809d2ecc753cf32f70803c1d0821c405211f4","modified":1447897401000},{"_id":"themes/next/source/css/_common/_section/sidebar.styl","shasum":"bc106c3e759cd752c2b4c53ac27bc5ef5e3b18ea","modified":1447897401000},{"_id":"themes/next/source/css/_common/_vendor/highlight/highlight.styl","shasum":"6242be4307a3b3dafc14e556f51c8875c41a1ddd","modified":1447897401000},{"_id":"themes/next/source/css/_common/_vendor/highlight/theme.styl","shasum":"ae19721ceee5ba460e131cb2427dae3c1ff39d6f","modified":1447897401000},{"_id":"themes/next/source/css/_custom/custom.styl","shasum":"328d9a9696cc2ccf59c67d3c26000d569f46344c","modified":1447897401000},{"_id":"themes/next/source/css/_mixins/base.styl","shasum":"4e49707c99c8bbcfa0a607dfdaff0fbb7dffd2a3","modified":1447897401000},{"_id":"themes/next/source/css/_schemes/Mist/_base.styl","shasum":"d50c2a9ae363d26ed2e9bc226a9dc7abeb9ace1b","modified":1447897401000},{"_id":"themes/next/source/css/_schemes/Mist/_header.styl","shasum":"1631a430655eadb485574d1a9bedd49460988b11","modified":1447897401000},{"_id":"themes/next/source/css/_schemes/Mist/_logo.styl","shasum":"38e5df90c8689a71c978fd83ba74af3d4e4e5386","modified":1447897401000},{"_id":"themes/next/source/css/_schemes/Mist/_menu.styl","shasum":"5a8036fc61207ca0fe38c9782ed2f686fbf764be","modified":1447897401000},{"_id":"themes/next/source/css/_schemes/Mist/_posts-expanded.styl","shasum":"4a1c113d752cf97c8762d32da474542668d948ee","modified":1447897401000},{"_id":"themes/next/source/css/_schemes/Mist/index.styl","shasum":"fc7d96b897290dbd93bc8c515a2058fc4c374ea7","modified":1447897401000},{"_id":"themes/next/source/css/_schemes/default/_logo.styl","shasum":"8829bc556ca38bfec4add4f15a2f028092ac6d46","modified":1447897401000},{"_id":"themes/next/source/css/_schemes/default/_menu.styl","shasum":"4bba29cece65ffc5122f4e052063dea4439fe4ae","modified":1447897401000},{"_id":"themes/next/source/css/_schemes/default/_search.styl","shasum":"c524bccdc554349106d1c8be9c3f275d4c0d4281","modified":1447897401000},{"_id":"themes/next/source/css/_schemes/default/index.styl","shasum":"159464cb8a7e01e32db9ec70dec391ec70a72f9c","modified":1447897401000},{"_id":"themes/next/source/css/_variables/Mist.styl","shasum":"9f8791860cc1ca724d2dfe609e8cd6abc44d6926","modified":1447897401000},{"_id":"themes/next/source/css/_variables/base.styl","shasum":"f532d6b0f961a8f3867c06e132233286f110180d","modified":1447897401000},{"_id":"themes/next/source/css/_variables/custom.styl","shasum":"e72591fde1545ba31eafa97cc0c87503819d6e0f","modified":1447904673000},{"_id":"themes/next/source/css/_variables/custom.styl~","shasum":"05cf679d097b4ef949f790ebbb62fba060e6056b","modified":1447904181000},{"_id":"themes/next/source/css/main.styl","shasum":"56dacee56c5eaa4b2676d196452314fb50f758aa","modified":1447897401000},{"_id":"themes/next/source/images/cc-by-nc-nd.svg","shasum":"c6524ece3f8039a5f612feaf865d21ec8a794564","modified":1447897401000},{"_id":"themes/next/source/images/cc-by-nc-sa.svg","shasum":"3031be41e8753c70508aa88e84ed8f4f653f157e","modified":1447897401000},{"_id":"themes/next/source/images/cc-by-nc.svg","shasum":"8d39b39d88f8501c0d27f8df9aae47136ebc59b7","modified":1447897401000},{"_id":"themes/next/source/images/cc-by-nd.svg","shasum":"c563508ce9ced1e66948024ba1153400ac0e0621","modified":1447897401000},{"_id":"themes/next/source/images/cc-by-sa.svg","shasum":"aa4742d733c8af8d38d4c183b8adbdcab045872e","modified":1447897401000},{"_id":"themes/next/source/images/cc-by.svg","shasum":"28a0a4fe355a974a5e42f68031652b76798d4f7e","modified":1447897401000},{"_id":"themes/next/source/images/cc-zero.svg","shasum":"87669bf8ac268a91d027a0a4802c92a1473e9030","modified":1447897401000},{"_id":"themes/next/source/images/loading.gif","shasum":"5fbd472222feb8a22cf5b8aa5dc5b8e13af88e2b","modified":1447897401000},{"_id":"themes/next/source/images/placeholder.gif","shasum":"5fbd472222feb8a22cf5b8aa5dc5b8e13af88e2b","modified":1447897401000},{"_id":"themes/next/source/images/quote-l.svg","shasum":"94e870b4c8c48da61d09522196d4dd40e277a98f","modified":1447897401000},{"_id":"themes/next/source/images/quote-r.svg","shasum":"e60ae504f9d99b712c793c3740c6b100d057d4ec","modified":1447897401000},{"_id":"themes/next/source/images/searchicon.png","shasum":"67727a6a969be0b2659b908518fa6706eed307b8","modified":1447897401000},{"_id":"themes/next/source/js/bootstrap.js","shasum":"f9b637b6d064f728d7dc2b6b5058a006a4454299","modified":1447897401000},{"_id":"themes/next/source/js/bootstrap.scrollspy.js","shasum":"ae7bdce88b515aade4eea8bf7407eec458bcd625","modified":1447897401000},{"_id":"themes/next/source/js/fancy-box.js","shasum":"b5fa638ed371b5f658b0826ec4afee25d9986ef2","modified":1447897401000},{"_id":"themes/next/source/js/helpers.js","shasum":"c15216ef897334362789ba37464298948b2eef95","modified":1447897401000},{"_id":"themes/next/source/js/hook-duoshuo.js","shasum":"ccb32e0a1acf798337c9697e1aab5484b52f9df4","modified":1447897401000},{"_id":"themes/next/source/js/motion.js","shasum":"b4132517fe499538ad725094593fb7ead8c04bf7","modified":1447897401000},{"_id":"themes/next/source/js/ua-parser.min.js","shasum":"1148fa2bcb8b2e40c31e5f597bf794a57369a2e6","modified":1447897401000},{"_id":"themes/next/source/vendors/fancybox/source/blank.gif","shasum":"2daeaa8b5f19f0bc209d976c02bd6acb51b00b0a","modified":1447897401000},{"_id":"themes/next/source/vendors/fancybox/source/fancybox_loading.gif","shasum":"1a755fb2599f3a313cc6cfdb14df043f8c14a99c","modified":1447897401000},{"_id":"themes/next/source/vendors/fancybox/source/fancybox_loading@2x.gif","shasum":"273b123496a42ba45c3416adb027cd99745058b0","modified":1447897401000},{"_id":"themes/next/source/vendors/fancybox/source/fancybox_overlay.png","shasum":"b3a4ee645ba494f52840ef8412015ba0f465dbe0","modified":1447897401000},{"_id":"themes/next/source/vendors/fancybox/source/fancybox_sprite.png","shasum":"17df19f97628e77be09c352bf27425faea248251","modified":1447897401000},{"_id":"themes/next/source/vendors/fancybox/source/fancybox_sprite@2x.png","shasum":"30c58913f327e28f466a00f4c1ac8001b560aed8","modified":1447897401000},{"_id":"themes/next/source/vendors/fancybox/source/helpers/fancybox_buttons.png","shasum":"e385b139516c6813dcd64b8fc431c364ceafe5f3","modified":1447897401000},{"_id":"themes/next/source/vendors/fancybox/source/helpers/jquery.fancybox-buttons.css","shasum":"1a9d8e5c22b371fcc69d4dbbb823d9c39f04c0c8","modified":1447897401000},{"_id":"themes/next/source/vendors/fancybox/source/helpers/jquery.fancybox-buttons.js","shasum":"91e41741c2e93f732c82aaacec4cfc6e3f3ec876","modified":1447897401000},{"_id":"themes/next/source/vendors/fancybox/source/helpers/jquery.fancybox-media.js","shasum":"3bdf69ed2469e4fb57f5a95f17300eef891ff90d","modified":1447897401000},{"_id":"themes/next/source/vendors/fancybox/source/helpers/jquery.fancybox-thumbs.css","shasum":"4ac329c16a5277592fc12a37cca3d72ca4ec292f","modified":1447897401000},{"_id":"themes/next/source/vendors/fancybox/source/helpers/jquery.fancybox-thumbs.js","shasum":"53e194f4a72e649c04fb586dd57762b8c022800b","modified":1447897401000},{"_id":"themes/next/source/vendors/fancybox/source/jquery.fancybox.css","shasum":"5f163444617b6cf267342f06ac166a237bb62df9","modified":1447897401000},{"_id":"themes/next/source/vendors/fancybox/source/jquery.fancybox.js","shasum":"1cf3d47b5ccb7cb6e9019c64f2a88d03a64853e4","modified":1447897401000},{"_id":"themes/next/source/vendors/fancybox/source/jquery.fancybox.pack.js","shasum":"53360764b429c212f424399384417ccc233bb3be","modified":1447897401000},{"_id":"themes/next/source/vendors/fastclick/LICENSE","shasum":"dcd5b6b43095d9e90353a28b09cb269de8d4838e","modified":1447897401000},{"_id":"themes/next/source/vendors/fastclick/README.md","shasum":"1decd8e1adad2cd6db0ab50cf56de6035156f4ea","modified":1447897401000},{"_id":"themes/next/source/vendors/fastclick/bower.json","shasum":"13379463c7463b4b96d13556b46faa4cc38d81e6","modified":1447897401000},{"_id":"themes/next/source/vendors/fastclick/lib/fastclick.js","shasum":"06cef196733a710e77ad7e386ced6963f092dc55","modified":1447897401000},{"_id":"themes/next/source/vendors/fastclick/lib/fastclick.min.js","shasum":"2cae0f5a6c5d6f3cb993015e6863f9483fc4de18","modified":1447897401000},{"_id":"themes/next/source/vendors/font-awesome/HELP-US-OUT.txt","shasum":"69a4c537d167b68a0ccf1c6febd138aeffca60d6","modified":1447897401000},{"_id":"themes/next/source/vendors/font-awesome/bower.json","shasum":"279a8a718ab6c930a67c41237f0aac166c1b9440","modified":1447897401000},{"_id":"themes/next/source/vendors/font-awesome/css/font-awesome.css","shasum":"3b87c2560832748cd06f9bfd2fd6ea8edbdae8c7","modified":1447897401000},{"_id":"themes/next/source/vendors/font-awesome/css/font-awesome.css.map","shasum":"0189d278706509412bac4745f96c83984e1d59f4","modified":1447897401000},{"_id":"themes/next/source/vendors/font-awesome/css/font-awesome.min.css","shasum":"05ea25bc9b3ac48993e1fee322d3bc94b49a6e22","modified":1447897401000},{"_id":"themes/next/source/vendors/font-awesome/fonts/fontawesome-webfont.woff2","shasum":"574ea2698c03ae9477db2ea3baf460ee32f1a7ea","modified":1447897401000},{"_id":"themes/next/source/vendors/jquery_lazyload/CONTRIBUTING.md","shasum":"4891864c24c28efecd81a6a8d3f261145190f901","modified":1447897401000},{"_id":"themes/next/source/vendors/jquery_lazyload/README.md","shasum":"895d50fa29759af7835256522e9dd7dac597765c","modified":1447897401000},{"_id":"themes/next/source/vendors/jquery_lazyload/bower.json","shasum":"65bc85d12197e71c40a55c0cd7f6823995a05222","modified":1447897401000},{"_id":"themes/next/source/vendors/jquery_lazyload/jquery.lazyload.js","shasum":"481fd478650e12b67c201a0ea41e92743f8b45a3","modified":1447897401000},{"_id":"themes/next/source/vendors/jquery_lazyload/jquery.scrollstop.js","shasum":"0e9a81785a011c98be5ea821a8ed7d411818cfd1","modified":1447897401000},{"_id":"themes/next/source/vendors/velocity/bower.json","shasum":"2ec99573e84c7117368beccb9e94b6bf35d2db03","modified":1447897401000},{"_id":"themes/next/source/vendors/velocity/velocity.min.js","shasum":"2f1afadc12e4cf59ef3b405308d21baa97e739c6","modified":1447897401000},{"_id":"themes/next/source/vendors/velocity/velocity.ui.js","shasum":"6a1d101eab3de87527bb54fcc8c7b36b79d8f0df","modified":1447897401000},{"_id":"themes/next/source/vendors/velocity/velocity.ui.min.js","shasum":"ed5e534cd680a25d8d14429af824f38a2c7d9908","modified":1447897401000},{"_id":"themes/next/test/helpers.js","shasum":"a1f5de25154c3724ffc24a91ddc576cdbd60864f","modified":1447897401000},{"_id":"themes/next/test/intern.js","shasum":"11fa8a4f5c3b4119a179ae0a2584c8187f907a73","modified":1447897401000},{"_id":"themes/next/source/vendors/font-awesome/fonts/FontAwesome.otf","shasum":"0112e96f327d413938d37c1693806f468ffdbace","modified":1447897401000},{"_id":"themes/next/source/vendors/font-awesome/fonts/fontawesome-webfont.eot","shasum":"b3c2f08e73320135b69c23a3908b87a12053a2f6","modified":1447897401000},{"_id":"themes/next/source/vendors/font-awesome/fonts/fontawesome-webfont.woff","shasum":"507970402e328b2baeb05bde73bf9ded4e2c3a2d","modified":1447897401000},{"_id":"themes/next/source/vendors/jquery/index.js","shasum":"41b4bfbaa96be6d1440db6e78004ade1c134e276","modified":1447897401000},{"_id":"themes/next/source/vendors/font-awesome/fonts/fontawesome-webfont.ttf","shasum":"27cf1f2ec59aece6938c7bb2feb0e287ea778ff9","modified":1447897401000},{"_id":"themes/next/source/vendors/velocity/velocity.js","shasum":"e63dc7cea055ca60a95d286f32349d88b10c5a4d","modified":1447897401000},{"_id":"themes/next/source/vendors/font-awesome/fonts/fontawesome-webfont.svg","shasum":"2b3c8ba7008cc014d8fb37abc6f9f49aeda83824","modified":1447897401000},{"_id":"public/images/scrollup.png","modified":1447904718979,"shasum":"2137d4f1739aa8aa3fcb0348c3ddf1e41d62f2e3"},{"_id":"public/images/logo.svg","modified":1447904718994,"shasum":"9ae38f7225c38624faeb7b74996efa9de7bf065b"},{"_id":"public/images/logo.png","modified":1447904718995,"shasum":"2a87d4212af648b58859c97f43f3200390e96f44"},{"_id":"public/images/jacman.jpg","modified":1447904718997,"shasum":"0ba14a4a5e3be012826fc713c33479912126d34e"},{"_id":"public/images/favicon.ico","modified":1447904718998,"shasum":"ec9660de66ff2a0e682e1a5ee7b7d8ee85c585c4"},{"_id":"public/images/cc-zero.svg","modified":1447904719000,"shasum":"87669bf8ac268a91d027a0a4802c92a1473e9030"},{"_id":"public/images/cc-by.svg","modified":1447904719002,"shasum":"28a0a4fe355a974a5e42f68031652b76798d4f7e"},{"_id":"public/images/cc-by-sa.svg","modified":1447904719003,"shasum":"aa4742d733c8af8d38d4c183b8adbdcab045872e"},{"_id":"public/images/cc-by-nd.svg","modified":1447904719005,"shasum":"c563508ce9ced1e66948024ba1153400ac0e0621"},{"_id":"public/images/cc-by-nc.svg","modified":1447904719007,"shasum":"8d39b39d88f8501c0d27f8df9aae47136ebc59b7"},{"_id":"public/images/cc-by-nc-sa.svg","modified":1447904719008,"shasum":"3031be41e8753c70508aa88e84ed8f4f653f157e"},{"_id":"public/images/cc-by-nc-nd.svg","modified":1447904719009,"shasum":"c6524ece3f8039a5f612feaf865d21ec8a794564"},{"_id":"public/images/banner.jpg","modified":1447904719013,"shasum":"219156face68a3d4505064ff3f02f87423b47606"},{"_id":"public/images/avatar.jpg","modified":1447904719015,"shasum":"9058722ba5f9f68672a90cdf8d609ca2bcaae7e2"},{"_id":"public/images/author.jpg","modified":1447904719020,"shasum":"9058722ba5f9f68672a90cdf8d609ca2bcaae7e2"},{"_id":"public/vendors/velocity/velocity.ui.min.js","modified":1447904719022,"shasum":"ed5e534cd680a25d8d14429af824f38a2c7d9908"},{"_id":"public/vendors/velocity/velocity.ui.js","modified":1447904719023,"shasum":"6a1d101eab3de87527bb54fcc8c7b36b79d8f0df"},{"_id":"public/vendors/velocity/velocity.min.js","modified":1447904719025,"shasum":"2f1afadc12e4cf59ef3b405308d21baa97e739c6"},{"_id":"public/vendors/velocity/velocity.js","modified":1447904719027,"shasum":"9f08181baea0cc0e906703b7e5df9111b9ef3373"},{"_id":"public/vendors/velocity/bower.json","modified":1447904719029,"shasum":"0ef14e7ccdfba5db6eb3f8fc6aa3b47282c36409"},{"_id":"public/vendors/jquery_lazyload/jquery.scrollstop.js","modified":1447904719030,"shasum":"0e9a81785a011c98be5ea821a8ed7d411818cfd1"},{"_id":"public/vendors/jquery_lazyload/jquery.lazyload.js","modified":1447904719030,"shasum":"481fd478650e12b67c201a0ea41e92743f8b45a3"},{"_id":"public/vendors/jquery_lazyload/bower.json","modified":1447904719031,"shasum":"ae3c3b61e6e7f9e1d7e3585ad854380ecc04cf53"},{"_id":"public/vendors/jquery_lazyload/README.html","modified":1447904719059,"shasum":"1eeb7414f97f8ac1f91c19eed31ebba591df787d"},{"_id":"public/vendors/jquery_lazyload/CONTRIBUTING.html","modified":1447904719108,"shasum":"0bd87f225f3d850b299f68efc2ffafce870c0333"},{"_id":"public/vendors/jquery/index.js","modified":1447904719126,"shasum":"41b4bfbaa96be6d1440db6e78004ade1c134e276"},{"_id":"public/vendors/font-awesome/fonts/fontawesome-webfont.woff2","modified":1447904719128,"shasum":"574ea2698c03ae9477db2ea3baf460ee32f1a7ea"},{"_id":"public/vendors/font-awesome/fonts/fontawesome-webfont.woff","modified":1447904719129,"shasum":"507970402e328b2baeb05bde73bf9ded4e2c3a2d"},{"_id":"public/vendors/font-awesome/fonts/fontawesome-webfont.ttf","modified":1447904719131,"shasum":"27cf1f2ec59aece6938c7bb2feb0e287ea778ff9"},{"_id":"public/vendors/font-awesome/fonts/fontawesome-webfont.svg","modified":1447904719133,"shasum":"2b3c8ba7008cc014d8fb37abc6f9f49aeda83824"},{"_id":"public/vendors/font-awesome/fonts/fontawesome-webfont.eot","modified":1447904719135,"shasum":"b3c2f08e73320135b69c23a3908b87a12053a2f6"},{"_id":"public/vendors/font-awesome/fonts/FontAwesome.otf","modified":1447904719138,"shasum":"0112e96f327d413938d37c1693806f468ffdbace"},{"_id":"public/vendors/font-awesome/css/font-awesome.min.css","modified":1447904719139,"shasum":"05ea25bc9b3ac48993e1fee322d3bc94b49a6e22"},{"_id":"public/vendors/font-awesome/css/font-awesome.css.map","modified":1447904719143,"shasum":"0189d278706509412bac4745f96c83984e1d59f4"},{"_id":"public/vendors/font-awesome/css/font-awesome.css","modified":1447904719144,"shasum":"3b87c2560832748cd06f9bfd2fd6ea8edbdae8c7"},{"_id":"public/vendors/font-awesome/bower.json","modified":1447904719145,"shasum":"64394a2a9aa00f8e321d8daa5e51a420f0e96dad"},{"_id":"public/vendors/font-awesome/HELP-US-OUT.txt","modified":1447904719146,"shasum":"69a4c537d167b68a0ccf1c6febd138aeffca60d6"},{"_id":"public/vendors/fastclick/lib/fastclick.min.js","modified":1447904719149,"shasum":"2cae0f5a6c5d6f3cb993015e6863f9483fc4de18"},{"_id":"public/vendors/fastclick/lib/fastclick.js","modified":1447904719150,"shasum":"06cef196733a710e77ad7e386ced6963f092dc55"},{"_id":"public/vendors/fastclick/bower.json","modified":1447904719151,"shasum":"4dcecf83afddba148464d5339c93f6d0aa9f42e9"},{"_id":"public/vendors/fastclick/README.html","modified":1447904719156,"shasum":"4a6074903daa9004301ef30a6fb96556ba3eab60"},{"_id":"public/vendors/fastclick/LICENSE","modified":1447904719168,"shasum":"dcd5b6b43095d9e90353a28b09cb269de8d4838e"},{"_id":"public/vendors/fancybox/source/jquery.fancybox.pack.js","modified":1447904719169,"shasum":"53360764b429c212f424399384417ccc233bb3be"},{"_id":"public/vendors/fancybox/source/jquery.fancybox.js","modified":1447904719170,"shasum":"1cf3d47b5ccb7cb6e9019c64f2a88d03a64853e4"},{"_id":"public/vendors/fancybox/source/jquery.fancybox.css","modified":1447904719171,"shasum":"5f163444617b6cf267342f06ac166a237bb62df9"},{"_id":"public/vendors/fancybox/source/helpers/jquery.fancybox-thumbs.js","modified":1447904719171,"shasum":"53e194f4a72e649c04fb586dd57762b8c022800b"},{"_id":"public/vendors/fancybox/source/helpers/jquery.fancybox-thumbs.css","modified":1447904719172,"shasum":"4ac329c16a5277592fc12a37cca3d72ca4ec292f"},{"_id":"public/vendors/fancybox/source/helpers/jquery.fancybox-media.js","modified":1447904719173,"shasum":"3bdf69ed2469e4fb57f5a95f17300eef891ff90d"},{"_id":"public/vendors/fancybox/source/helpers/jquery.fancybox-buttons.js","modified":1447904719175,"shasum":"91e41741c2e93f732c82aaacec4cfc6e3f3ec876"},{"_id":"public/vendors/fancybox/source/helpers/jquery.fancybox-buttons.css","modified":1447904719176,"shasum":"1a9d8e5c22b371fcc69d4dbbb823d9c39f04c0c8"},{"_id":"public/vendors/fancybox/source/helpers/fancybox_buttons.png","modified":1447904719177,"shasum":"e385b139516c6813dcd64b8fc431c364ceafe5f3"},{"_id":"public/vendors/fancybox/source/fancybox_sprite@2x.png","modified":1447904719178,"shasum":"30c58913f327e28f466a00f4c1ac8001b560aed8"},{"_id":"public/vendors/fancybox/source/fancybox_sprite.png","modified":1447904719179,"shasum":"17df19f97628e77be09c352bf27425faea248251"},{"_id":"public/vendors/fancybox/source/fancybox_overlay.png","modified":1447904719180,"shasum":"b3a4ee645ba494f52840ef8412015ba0f465dbe0"},{"_id":"public/vendors/fancybox/source/fancybox_loading@2x.gif","modified":1447904719182,"shasum":"273b123496a42ba45c3416adb027cd99745058b0"},{"_id":"public/vendors/fancybox/source/fancybox_loading.gif","modified":1447904719184,"shasum":"1a755fb2599f3a313cc6cfdb14df043f8c14a99c"},{"_id":"public/vendors/fancybox/source/blank.gif","modified":1447904719185,"shasum":"2daeaa8b5f19f0bc209d976c02bd6acb51b00b0a"},{"_id":"public/js/ua-parser.min.js","modified":1447904719188,"shasum":"1148fa2bcb8b2e40c31e5f597bf794a57369a2e6"},{"_id":"public/js/motion.js","modified":1447904719189,"shasum":"b4132517fe499538ad725094593fb7ead8c04bf7"},{"_id":"public/js/hook-duoshuo.js","modified":1447904719193,"shasum":"eedaf52377991728f1e3e94f2bc4bf23ec41ecea"},{"_id":"public/js/helpers.js","modified":1447904719194,"shasum":"c15216ef897334362789ba37464298948b2eef95"},{"_id":"public/js/fancy-box.js","modified":1447904719195,"shasum":"b5fa638ed371b5f658b0826ec4afee25d9986ef2"},{"_id":"public/js/bootstrap.scrollspy.js","modified":1447904719195,"shasum":"ae7bdce88b515aade4eea8bf7407eec458bcd625"},{"_id":"public/js/bootstrap.js","modified":1447904719196,"shasum":"f9b637b6d064f728d7dc2b6b5058a006a4454299"},{"_id":"public/images/searchicon.png","modified":1447904719197,"shasum":"67727a6a969be0b2659b908518fa6706eed307b8"},{"_id":"public/images/quote-r.svg","modified":1447904719198,"shasum":"e60ae504f9d99b712c793c3740c6b100d057d4ec"},{"_id":"public/images/quote-l.svg","modified":1447904719201,"shasum":"94e870b4c8c48da61d09522196d4dd40e277a98f"},{"_id":"public/images/placeholder.gif","modified":1447904719202,"shasum":"5fbd472222feb8a22cf5b8aa5dc5b8e13af88e2b"},{"_id":"public/images/loading.gif","modified":1447904719203,"shasum":"5fbd472222feb8a22cf5b8aa5dc5b8e13af88e2b"},{"_id":"public/css/main.css","modified":1447904719436,"shasum":"bfe308b461b9adb1e92a3d0907cf29d246c649db"},{"_id":"public/tags/index.html","modified":1447933780794,"shasum":"c7598112155ab50bd9a65e512e0f3c8017d6a875"},{"_id":"public/resume/index.html","modified":1447933780831,"shasum":"1dd2ced8c8d94ed277ac724406b981f07eef692c"},{"_id":"public/google89c38fd0efaaa284.html","modified":1447904719613,"shasum":"a6f0fd42cb59bbd85e622438a19ff9c883c6f531"},{"_id":"public/categories/index.html","modified":1447933780867,"shasum":"f9c6c451d61ed720ac81b73c012fefdd12352d24"},{"_id":"public/baidu_verify_bhY7p6c45b.html","modified":1447904719644,"shasum":"195b3418e89bad67a36221c97c18b6c98f51b43b"},{"_id":"public/about/index.html","modified":1447933780897,"shasum":"2b6806b56764ce78e7b63b383e3d5bd24530f45e"},{"_id":"public/2015/11/17/卷积的理解/index.html","modified":1447933781046,"shasum":"4f1f2ffa8ffdb6ea358ca7844b17662b66ceee24"},{"_id":"public/2015/11/10/迁移Hexo博客：GitHub至GitCafe/index.html","modified":1447933781091,"shasum":"62a194acd0e748f163902199ea57c9b7e2240275"},{"_id":"public/2015/10/21/Scikt-learn-example/index.html","modified":1447905003339,"shasum":"175a8d27721626cfd5efb3ce8b6bb588e4b96373"},{"_id":"public/2015/10/21/scikt-learn示例解析 色彩量化Color Quantization using K-Means/index.html","modified":1447933781146,"shasum":"ba944dca65fc2698e5e2fd12c8e21fd9dbe97682"},{"_id":"public/2015/08/13/逻辑回归与计算中的向量化思想/index.html","modified":1447933781195,"shasum":"09c94b68548fa7d65a3de3ddc2b35c8f0c6e9f47"},{"_id":"public/2015/07/14/简谈贝叶斯决策/index.html","modified":1447933781242,"shasum":"985b13a3d96f39e1fe891f74c374e1074061dac3"},{"_id":"public/2015/06/22/理解Linux的环境变量、启动文件与命令目录/index.html","modified":1447933781306,"shasum":"8191bf6180f69da9c59d739c6574905f794e1fc9"},{"_id":"public/2015/06/10/荆棘谷的青山/index.html","modified":1447933781370,"shasum":"c0385b0441788f3ced90f080153a09cf227938ec"},{"_id":"public/archives/index.html","modified":1447933781447,"shasum":"13f7683932403e808ec25f1dad3ab275403dbdf1"},{"_id":"public/archives/2015/index.html","modified":1447933781521,"shasum":"3225de1ec150591f98470bf5c143ff93985a9af0"},{"_id":"public/archives/2015/06/index.html","modified":1447933781559,"shasum":"15b315dc975b13d1bfc39164fcb5a949e935396e"},{"_id":"public/archives/2015/07/index.html","modified":1447933781598,"shasum":"adf5297a8a1cfac6809c5c865354da12e3dfcdc1"},{"_id":"public/archives/2015/08/index.html","modified":1447933781629,"shasum":"d25dd887732ddaa93ad53d86d2e04b04077caa42"},{"_id":"public/archives/2015/10/index.html","modified":1447933781670,"shasum":"59e353803f49ffcc60f1efea21b242799d0db471"},{"_id":"public/archives/2015/11/index.html","modified":1447933781705,"shasum":"1dc2e5872838a1ce3b84059cfcd122bd08476ff7"},{"_id":"public/categories/技术/index.html","modified":1447933781754,"shasum":"42faa4c5b7a609a8bdbe743d1f4ce095bf670227"},{"_id":"public/categories/生活/index.html","modified":1447933781784,"shasum":"67919859279d6c6cb393058ce2f65e3063307ed5"},{"_id":"public/categories/基础/index.html","modified":1447933781821,"shasum":"38a05ca6cb15d688346bf3ea28619f3f44ef52aa"},{"_id":"public/atom.xml","modified":1447933073460,"shasum":"60466030d2f9d0443924180c19882bde00a2fb94"},{"_id":"public/index.html","modified":1447933781822,"shasum":"38b1ad401b748965369296b86327d23082a1fe93"},{"_id":"public/tags/sklearn/index.html","modified":1447933781852,"shasum":"c24fd66d8ac54c12c9de01f8bfa3715befd3f996"},{"_id":"public/tags/机器学习/index.html","modified":1447933781891,"shasum":"27ba105fc1640f993e6b97877f17e95604cf0db3"},{"_id":"public/tags/kmeans/index.html","modified":1447933781920,"shasum":"5a70b1036186455892972b62b57c5a602edeadb4"},{"_id":"public/tags/逻辑回归/index.html","modified":1447933781964,"shasum":"3c77eb471de2b4252b4e01f0589fe8073c566cc3"},{"_id":"public/tags/判别模型/index.html","modified":1447933782001,"shasum":"ad0f216a97d99138160ac77cfb2b5fc74c730f72"},{"_id":"public/tags/分类/index.html","modified":1447933782048,"shasum":"bf8fed1125922cf6208c925d8e06eef1d497616b"},{"_id":"public/tags/线性代数/index.html","modified":1447933782091,"shasum":"e097e02442db9eb461eed2a60ae0ea0a863e8e13"},{"_id":"public/tags/hexo/index.html","modified":1447933782131,"shasum":"d78ecea763c2510d2010ba75ba4b4c3dd6062042"},{"_id":"public/tags/Gitpages/index.html","modified":1447933782181,"shasum":"5d2197b775c1214d02a07c658482622369b4c8ce"},{"_id":"public/tags/GitCafe/index.html","modified":1447933782229,"shasum":"9e5a4c5f4573b77d12aaee3bab30645b6fa8fcea"},{"_id":"public/tags/博客迁移/index.html","modified":1447933782258,"shasum":"1f20d2185e1d000243b261e65e39807ff7b9716e"},{"_id":"public/tags/纪念/index.html","modified":1447933782293,"shasum":"94cbf0470f81e0e038675c34d56b7c88300648f4"},{"_id":"public/tags/贝叶斯/index.html","modified":1447933782323,"shasum":"87f68aaec779876dfa4eb539e3c8757222d40fd6"},{"_id":"public/tags/linux/index.html","modified":1447933782352,"shasum":"e9077d5429228a412b4b236c49521d5aea1c69a0"},{"_id":"public/tags/环境变量/index.html","modified":1447933782388,"shasum":"36583f4cd425534f7d300aa8c50082daf0794c4b"},{"_id":"public/tags/卷积/index.html","modified":1447933782418,"shasum":"b44ac62d892f6330e8e0366b580d2d34eaefe906"},{"_id":"public/tags/图像处理/index.html","modified":1447933782454,"shasum":"e63c2751a47ad03912e2c27c5adbbd40960935d3"},{"_id":"public/sitemap.xml","modified":1447933074054,"shasum":"4ad8ec3dc6e8b014b2beb7729507ac59d8936462"},{"_id":"themes/next/languages/zh-Hans.yml~","shasum":"37413bccbc6bf9d86ecbebf3215e5480cafa878b","modified":1447933772000}],"Category":[{"name":"技术","_id":"cih5p32ta0001asbgolg84eqq"},{"name":"生活","_id":"cih5p32wi0011asbgtl15nheq"},{"name":"基础","_id":"cih5p32wp001iasbgsmynufkp"}],"Data":[],"Page":[{"title":"Tagcloud","date":"2015-09-16T04:39:04.000Z","type":"tags","comments":0,"_content":"","source":"tags/index.md","raw":"title: Tagcloud\ndate: 2015-09-16 12:39:04\ntype: \"tags\"\ncomments: false\n---","updated":"2015-11-19T03:17:35.000Z","path":"tags/index.html","layout":"page","_id":"cih5p32v80009asbgg12ik7wy"},{"title":"个人简历","date":"2015-06-10T10:07:27.000Z","_content":"wait.....","source":"resume/index.md","raw":"title: 个人简历\ndate: 2015-06-10 18:07:27\n---\nwait.....","updated":"2015-06-11T07:28:39.000Z","path":"resume/index.html","comments":1,"layout":"page","_id":"cih5p32vx000aasbgakof7xci"},{"layout":"false","_content":"google-site-verification: google89c38fd0efaaa284.html","source":"google89c38fd0efaaa284.html","raw":"layout: false\n---\ngoogle-site-verification: google89c38fd0efaaa284.html","date":"2015-10-22T03:33:31.000Z","updated":"2015-10-22T03:33:31.000Z","path":"google89c38fd0efaaa284.html","title":"","comments":1,"_id":"cih5p32w9000basbg5tesywja"},{"_content":"layout: categories\ntitle: categories","source":"categories/index.md","raw":"layout: categories\ntitle: categories","date":"2015-10-15T15:05:22.000Z","updated":"2015-06-10T08:10:16.000Z","path":"categories/index.html","title":"","comments":1,"layout":"page","_id":"cih5p32wa000casbg7do5plou"},{"layout":"false","_content":"bhY7p6c45b","source":"baidu_verify_bhY7p6c45b.html","raw":"layout: false\n---\nbhY7p6c45b","date":"2015-10-22T03:52:44.000Z","updated":"2015-10-22T03:52:44.000Z","path":"baidu_verify_bhY7p6c45b.html","title":"","comments":1,"_id":"cih5p32wb000dasbg2mscp0kj"},{"title":"关于","date":"2015-06-10T10:07:27.000Z","_content":"欢迎大家来到我的博客\n\n##About Lion's Pride Inn\n\n狮王之傲旅馆，艾尔文森里中闪金镇上的小店，不如暴风城里的镶金玫瑰旅店那样客流熙攘，也不像斯坦索姆里月桂旅店那样危机四伏，它提供美味的麦酒和面包，是每个初出茅庐的联盟冒险者探索新世界的必经之地．\n\n##About me\n###Skill\n- CS在读\n- Java,  Python, C\n- 机器学习，轨迹数据挖掘，遥感处理\n###Favorites:\n- <夜行者>： the art of negotiaton and logic\n\n###Contact:\n- zrb915@live.com\n##About blog\n\n- 博文会以技术为主，原创为主，也许会偶尔有感而发，写点风马牛不想及的想法，权且一乐。\n- 也会逐步把有道云笔记中的各类总结迁移过来。\n\t- Node.js\n\t- Hexo\n\t- EJS\n\t- GitHub Pages\n\t- 多说\n\t- 七牛\n\t- CNZZ\n\n\n","source":"about/index.md","raw":"title: 关于\ndate: 2015-06-10 18:07:27\n---\n欢迎大家来到我的博客\n\n##About Lion's Pride Inn\n\n狮王之傲旅馆，艾尔文森里中闪金镇上的小店，不如暴风城里的镶金玫瑰旅店那样客流熙攘，也不像斯坦索姆里月桂旅店那样危机四伏，它提供美味的麦酒和面包，是每个初出茅庐的联盟冒险者探索新世界的必经之地．\n\n##About me\n###Skill\n- CS在读\n- Java,  Python, C\n- 机器学习，轨迹数据挖掘，遥感处理\n###Favorites:\n- <夜行者>： the art of negotiaton and logic\n\n###Contact:\n- zrb915@live.com\n##About blog\n\n- 博文会以技术为主，原创为主，也许会偶尔有感而发，写点风马牛不想及的想法，权且一乐。\n- 也会逐步把有道云笔记中的各类总结迁移过来。\n\t- Node.js\n\t- Hexo\n\t- EJS\n\t- GitHub Pages\n\t- 多说\n\t- 七牛\n\t- CNZZ\n\n\n","updated":"2015-10-17T11:39:14.000Z","path":"about/index.html","comments":1,"layout":"page","_id":"cih5p32wb000easbg42j3if9b"}],"Post":[{"title":"逻辑回归与计算中的向量化思想","date":"2015-08-13T09:36:33.000Z","mathjax":true,"_content":"\n##引子\n- 这部分总结的有点晚了，不过在熟悉机器学习体系和几个其它模型后，再回过头来看逻辑回归，发现从原理到思想都有比较好的理解。\n- 理论主要参考NG的斯坦福课程和李航老师的《统计学习方法》，代码实现参见《机器学习实战》。\n- 向量化计算的思想在逻辑回归代码中体现的很好，利用矩阵计算上的便利性来解决复杂的循环过程。但由于《实战》书中的原理推导及其吝惜笔墨（与其简洁优美的代码对比鲜明），这里做一下补充。\n- 感谢[洞庭之子](http://blog.csdn.net/dongtingzhizi/article/details/15962797)这篇博客，解决了我的很多疑问，很佩服作者的思路和严谨的推导,但这篇博客中的有些写法不规范，容易引起误解。\n\n\n##逻辑回归\n###思想\n- **根据数据对分类边界线建立回归公式:** 与感知机乃至SVM大同小异，都是寻找一个`超平面`将数据集分为两部分。基于如此，逻辑回归一般只能处理两分类问题，同时两个类别线性可分。对于`多分类问题`，还是老思想，化用二分类（目标类为一类，剩余唯一类），构建多个分类器，寻找概率最大的那个类作为分类结果。\n- **通过分类函数（sigmoid函数）寻找分类超平面**： 具体sigmoid函数相关的内容下面有详细叙述.\n- **判别模型的老思路：**假设特征系数$\\theta$，构造预测函数 ——> 构造损失函数 ——> 求解最优化问题：寻找使损失函数最小時的特征系数$\\theta$ ——> 得到分类器（即超平面）。\n- **优缺点：** 计算简单，训练分类器后计算量小;准确度有限，容易欠拟合，只针对二分类问题。\n\n###分类函数Sigmoid\n- 逻辑回归选择`近似于阶越函数`的Sigmoid函数作为分类函数：\n$$\\sigma(z_i) = \\dfrac{1}{1+e^{-z}}，\n(z_i = \\theta_0x_i^{(0)}+\\theta_1x_i^{(1)}+..+\\theta_nx_i^{(m)} )$$\n- 函数图像：![sigmod](http://7xjz3b.com1.z0.glb.clouddn.com/blog4-1.png)\n- $\\theta$为特征系数向量, 每个特征都有一个特征系数$\\theta_n$。另外，$x_i^{(j)}$为输入向量$x_i$的第j个分量，\n- 作用：1. 逻辑回归的分类函数。 2. 将样本映射到0-1区间，进而巧妙地将数据到分界线的距离转化为概率，然后通过最大斯然估计等方法求解，这一些后面会谈到。\n\n###问题求解\n求解的过程这里简单讲讲，具体内容（比如阶梯求导结果等）不在赘述，可以参阅参考博客。\n判别模型的基本套路：\n**预测函数 ——> 构造损失函数or最大斯然估计求发生概率 ——> 求解最优化问题**：\n- 1、预测函数：\n$$h\\_{\\theta}(x)=\\sigma(z_i) = \\dfrac{1}{1+e^{-z}}，\n(z_i = \\theta_0x_i^{(0)}+\\theta_1x_i^{(1)}+..+\\theta_nx_i^{(m)} )$$\n\nsigmoid的函数值可以表示成分类概率：\n\\begin{equation\\*}\n\\begin{aligned}\nP(y=1|x,\\theta) & = h\\_\\theta(x) \\\\\\\nP(y=0|x,\\theta) & = 1-h\\_\\theta(x)\n\\end{aligned}\n\\end{equation\\*}\n\n- 2、通过最大斯然估计求发生概率，同时可构造损失函数：\n$$\nP(y|x,\\theta)=(h\\_{\\theta}(x))^y(1-h\\_{\\theta}(x))^{1-y}\n$$\n取似然函数：\n{% raw %}\n$$\nL(\\theta)  = \\prod_{i=1}^nP(y_i|x_i,\\theta) = \\prod_{i=1}^n (h_{\\theta}(x_i))^{y_i}(1-h_{\\theta}(x_i))^{1-y_i}\n$$\n{% endraw %}\n\n\n取对数：\n{% raw %}\n\\begin{equation*}\n\\begin{aligned}\nl(\\theta) & = \\log L(\\theta) \\\\\n& =\\sum_{i=1}^n(y_i(\\log h_{\\theta}(x_i)+(1-y_i)\\log{(1-h_{\\theta}(x_i))}\n\\end{aligned}\n\\end{equation*}\n{% endraw %}\n到这里，我们就可以用最优化方法求解$l(\\theta)$，本例用的是梯度上升法。\n在斯坦福ML的课程中，取$J(\\theta)=-\\dfrac{1}{m}l(\\theta)$,构建损失函数，然后利用梯度下降法求解$\\theta$，本质是完全一样的。\n- 3、借助梯度上升法求解最优值：\n {% raw %}\n \\begin{equation*}\n\\begin{aligned}\n\\theta_j & = \\theta_j + \\alpha\\dfrac{\\delta}{\\delta\\theta_j}J(\\theta) \\\\\n& = \\theta_j+\\alpha\\sum_{i=1}^{n}(y_i - h_{\\theta}(x_i))x_i^{(j)}\n\\end{aligned}\n\\end{equation*}\n($特征维数j = 0...m,、\\alpha为学习步长$)\n\\begin{equation*}\n\\begin{aligned}\n\\dfrac{\\delta}{\\delta\\theta_j}J(\\theta) & = \\sum_{i=1}^n\\left[y_i\\dfrac{1}{h_\\theta(x_i)}\\dfrac{\\delta}{\\delta\\theta_j} h_\\theta(x_i)-(1-y_i)\\dfrac{1}{1-h_\\theta(x_i)}\\dfrac{\\delta}{\\delta\\theta_j} h_\\theta(x_i) \\right]   \\\\\\\n& = .....\\\\\n& = \\theta_j+\\alpha\\sum_{i=1}^{n}(y_i - h_{\\theta}(x_i))x_i^{(j)}\n\\end{aligned}\n\\end{equation*}\n{% endraw %}\n这里省去了大部分求偏导的过程，详细的求解步骤，可以参见参考博客。博文虽用的是梯度下降法，但每一步的求偏导数过程是完全一致的。\n\n##计算中的向量化思想\n- 在求解最优化方法時，将数据向量化，用矩阵的方式计算，是一种很好的思想。\n- 借助numpy等包，使得以前针对单个数值编写的方法，对矩阵也有了很好的支持度。\n- 《机器学习实战》的实现代码中，使用梯度上升法求解$\\theta$時，就用了向量化思想，用矩阵乘法代替了循环。但书中直接给出了迭代求$\\theta$的公式，缺少了如上节类似的推导，初读还是有些令人费解。\n###上节已知求$\\theta$每一步的更新过程：\n{% raw %}\n\\begin{equation}\n\\begin{aligned}\n\\theta_j & = \\theta_j + \\alpha\\dfrac{\\delta}{\\delta\\theta_j}J(\\theta) \\\\\\\n& = \\theta_j+\\alpha\\sum_{i=1}^{n}(y_i - h_{\\theta}(x_i))x_i^{(j)}\n\\end{aligned}\n\\end{equation}\n{% endraw %}\n\n- 1、首先我们把特征系数$\\theta$用m×1的列向量表示。(假设有n个输入向量，特征共有m维,$\\theta_0x_0当作常量偏移$)：\n\\begin{equation\\*}\n\\theta = \\begin{bmatrix}\n\\theta_0 \\\\\\\n\\theta_1 \\\\\\\n... \\\\\\\n\\theta_m\n\\end{bmatrix}\n\\end{equation\\*}\n\n输入数据用n×m的矩阵表示：\n\\begin{equation\\*}\nX = \\begin{bmatrix}\nx_1 \\\\\\\nx_2 \\\\\\\n... \\\\\\\nx_n\n\\end{bmatrix} = \n\\begin{bmatrix}\nx_1^{(0)},x_1^{(1)}...x_1^{(m)} \\\\\\\nx_2^{(0)},x_2^{(1)}...x_2^{(m)}  \\\\\\\n... \\\\\\\nx_n^{(0)},x_n^{(1)}...x_n^{(m)} \n\\end{bmatrix}\n\\end{equation\\*}\n所以$z_n = \\theta_0x_n^{(0)}+\\theta_1x_n^{(1)}+..+\\theta_nx_n^{(m)} $可以用$X\\cdot\\theta$得出的n×1列向量来表示：\n\\begin{equation\\*}\nZ = \\begin{bmatrix}\n\\theta_0x_1^{(0)}+\\theta_1x_1^{(1)}+...+\\theta_mx_1^{(m)} \\\\\\\n\\theta_0x_2^{(0)}+\\theta_1x_2^{(1)}+...+\\theta_mx_2^{(m)}   \\\\\\\n... \\\\\\\n\\theta_0x_n^{(0)}+\\theta_1x_n^{(1)}+...+\\theta_mx_n^{(m)}  \n\\end{bmatrix} =\\begin{bmatrix}\nz_1 \\\\\\\nz_2 \\\\\\\n... \\\\\\\nz_n\n\\end{bmatrix}\n\\end{equation\\*}\n用矩阵乘法的列向量分解思想来说，$X\\cdot\\theta$表示$X$中的每一个列向量以系数$\\theta$进行`线性组合`，符合公式的含义。\n- 2、我们再用列向量$Y = (y\\_1,y\\_2..y\\_n)^\\mathrm{T}$表示输入数据的类别，然后我们对$Z$进行sigmoid函数运算，所以更新过程公式中$(y\\_i - h\\_{\\theta}(x\\_i))$可以用n×1的列向量E表示：\n\\begin{equation\\*}\nE = \\begin{bmatrix}\ny_1-\\sigma(z_1) \\\\\\\ny_2-\\sigma(z_2)  \\\\\\\n... \\\\\\\ny_n-\\sigma(z_n)  \n\\end{bmatrix} =\\begin{bmatrix}\ne_1 \\\\\\\ne_2 \\\\\\\n... \\\\\\\ne_n\n\\end{bmatrix}\n\\end{equation\\*}\n(这里的E代表误差error)\n- 3、最后我们求解整个式子$\\theta\\_j  = \\theta\\_j+\\alpha\\sum\\_{i=1}^{n}(y\\_i - h\\_{\\theta}(x\\_i))x\\_i^{(j)}$. 式中的连加同样可以通过矩阵乘法解决。\n将$X$转置：\n{% raw %}\n\\begin{equation*}\nX^\\mathrm{T} = \\begin{bmatrix}\nx_1,\nx_2,\n... \nx_n\n\\end{bmatrix} = \n\\begin{bmatrix}\nx_1^{(0)},x_2^{(0)}...x_n^{(0)} \\\\\nx_1^{(1)},x_2^{(1)}...x_n^{(1)}  \\\\\n...... \\\\\nx_1^{(m)},x_2^{(m)}...x_n^{(m)} \n\\end{bmatrix}\n\\end{equation*}\n{% endraw %}\n\n转置后矩阵是m×n的，仔细观察一下转置后的X，每一个列向量是一条输入数据，而**每一个行向量是所有输入数据在某一个特征维度上的记录**，一共有m个行向量。\n所以，我们同时用每一个行向量$\\cdot$列向量$E$,用矩阵乘法即：\n{% raw %}\n\\begin{equation*}\nX^\\mathrm{T}\\cdot\\mathrm{E} = \n\\begin{bmatrix}\nx_1^{(0)}e_1+x_2^{(0)}e_2+...+x_n^{(0)}e_n \\\\\nx_1^{(1)}e_1+x_2^{(1)}e_2+...+x_n^{(1)}e_n \\\\\n...... \\\\\\\nx_1^{(m)}e_1,x_2^{(m)}e_2+...+x_n^{(m)}e_n \n\\end{bmatrix}     \n\\end{equation*}\n{% endraw %}\n(m*1)\n- 4、已知增长系数$\\alpha$，然后我们就可以用矩阵计算进行一次梯度上升迭代：\n{% raw %}\n\\begin{equation*}\n\\theta_{n+1} = \\theta_n +\\alpha \\cdot X^\\mathrm{T}\\cdot\\mathrm{E} = \n\\begin{bmatrix}\n\\theta_{n+1}^{(0)} \\\\\n\\theta_{n+1}^{(1)} \\\\\n... \\\\\n\\theta_{n+1}^{(m)}\n\\end{bmatrix}     \n\\end{equation*}\n{% endraw %}\n\n5. 设置合适的迭代次数，求得最终的特征系数$\\theta$,我们就得到了训练好的判别函数。\n##小结：\n- 看完我们会发现，逻辑回归也是基于简单的线性分类思想，只不过逻辑回归通过一个契合线性分类的sigmoid函数，将数据到分界限的距离巧妙地投影到了0-1区间，进而可以将距离转化为概率，通过最大斯然估计求解。\n- 向量化的思想其实充斥着机器学习的各个角落。就像MIT线性代数公开课中说的，矩阵并不是生来存在的，而是人们后天发明用来方便计算的产物。将数据向量化，通过矩阵运算求解，是解决问题的必经之路。\n---\n这次公式比较多，处理markdown语法和$\\mathrm{\\LaTeX}$的冲突時花了不少时间，不过也因此找到一个好方法，不用修改node.js的配置就可以处理好语法冲突，只需要在```公式的前后用raw标签注释就可以了```，原理应该是通过标签屏蔽了markdown语法的解释器，前端不是很懂，不过很有效哦～\n\n\n\n\n\n","source":"_posts/逻辑回归与计算中的向量化思想.md","raw":"title: 逻辑回归与计算中的向量化思想\ndate: 2015-08-13 17:36:33\ntags: \n- 逻辑回归\n- 判别模型\n- 分类\n- 机器学习\n- 线性代数\ncategories: \n- 技术\nmathjax: true \n---\n\n##引子\n- 这部分总结的有点晚了，不过在熟悉机器学习体系和几个其它模型后，再回过头来看逻辑回归，发现从原理到思想都有比较好的理解。\n- 理论主要参考NG的斯坦福课程和李航老师的《统计学习方法》，代码实现参见《机器学习实战》。\n- 向量化计算的思想在逻辑回归代码中体现的很好，利用矩阵计算上的便利性来解决复杂的循环过程。但由于《实战》书中的原理推导及其吝惜笔墨（与其简洁优美的代码对比鲜明），这里做一下补充。\n- 感谢[洞庭之子](http://blog.csdn.net/dongtingzhizi/article/details/15962797)这篇博客，解决了我的很多疑问，很佩服作者的思路和严谨的推导,但这篇博客中的有些写法不规范，容易引起误解。\n\n\n##逻辑回归\n###思想\n- **根据数据对分类边界线建立回归公式:** 与感知机乃至SVM大同小异，都是寻找一个`超平面`将数据集分为两部分。基于如此，逻辑回归一般只能处理两分类问题，同时两个类别线性可分。对于`多分类问题`，还是老思想，化用二分类（目标类为一类，剩余唯一类），构建多个分类器，寻找概率最大的那个类作为分类结果。\n- **通过分类函数（sigmoid函数）寻找分类超平面**： 具体sigmoid函数相关的内容下面有详细叙述.\n- **判别模型的老思路：**假设特征系数$\\theta$，构造预测函数 ——> 构造损失函数 ——> 求解最优化问题：寻找使损失函数最小時的特征系数$\\theta$ ——> 得到分类器（即超平面）。\n- **优缺点：** 计算简单，训练分类器后计算量小;准确度有限，容易欠拟合，只针对二分类问题。\n\n###分类函数Sigmoid\n- 逻辑回归选择`近似于阶越函数`的Sigmoid函数作为分类函数：\n$$\\sigma(z_i) = \\dfrac{1}{1+e^{-z}}，\n(z_i = \\theta_0x_i^{(0)}+\\theta_1x_i^{(1)}+..+\\theta_nx_i^{(m)} )$$\n- 函数图像：![sigmod](http://7xjz3b.com1.z0.glb.clouddn.com/blog4-1.png)\n- $\\theta$为特征系数向量, 每个特征都有一个特征系数$\\theta_n$。另外，$x_i^{(j)}$为输入向量$x_i$的第j个分量，\n- 作用：1. 逻辑回归的分类函数。 2. 将样本映射到0-1区间，进而巧妙地将数据到分界线的距离转化为概率，然后通过最大斯然估计等方法求解，这一些后面会谈到。\n\n###问题求解\n求解的过程这里简单讲讲，具体内容（比如阶梯求导结果等）不在赘述，可以参阅参考博客。\n判别模型的基本套路：\n**预测函数 ——> 构造损失函数or最大斯然估计求发生概率 ——> 求解最优化问题**：\n- 1、预测函数：\n$$h\\_{\\theta}(x)=\\sigma(z_i) = \\dfrac{1}{1+e^{-z}}，\n(z_i = \\theta_0x_i^{(0)}+\\theta_1x_i^{(1)}+..+\\theta_nx_i^{(m)} )$$\n\nsigmoid的函数值可以表示成分类概率：\n\\begin{equation\\*}\n\\begin{aligned}\nP(y=1|x,\\theta) & = h\\_\\theta(x) \\\\\\\nP(y=0|x,\\theta) & = 1-h\\_\\theta(x)\n\\end{aligned}\n\\end{equation\\*}\n\n- 2、通过最大斯然估计求发生概率，同时可构造损失函数：\n$$\nP(y|x,\\theta)=(h\\_{\\theta}(x))^y(1-h\\_{\\theta}(x))^{1-y}\n$$\n取似然函数：\n{% raw %}\n$$\nL(\\theta)  = \\prod_{i=1}^nP(y_i|x_i,\\theta) = \\prod_{i=1}^n (h_{\\theta}(x_i))^{y_i}(1-h_{\\theta}(x_i))^{1-y_i}\n$$\n{% endraw %}\n\n\n取对数：\n{% raw %}\n\\begin{equation*}\n\\begin{aligned}\nl(\\theta) & = \\log L(\\theta) \\\\\n& =\\sum_{i=1}^n(y_i(\\log h_{\\theta}(x_i)+(1-y_i)\\log{(1-h_{\\theta}(x_i))}\n\\end{aligned}\n\\end{equation*}\n{% endraw %}\n到这里，我们就可以用最优化方法求解$l(\\theta)$，本例用的是梯度上升法。\n在斯坦福ML的课程中，取$J(\\theta)=-\\dfrac{1}{m}l(\\theta)$,构建损失函数，然后利用梯度下降法求解$\\theta$，本质是完全一样的。\n- 3、借助梯度上升法求解最优值：\n {% raw %}\n \\begin{equation*}\n\\begin{aligned}\n\\theta_j & = \\theta_j + \\alpha\\dfrac{\\delta}{\\delta\\theta_j}J(\\theta) \\\\\n& = \\theta_j+\\alpha\\sum_{i=1}^{n}(y_i - h_{\\theta}(x_i))x_i^{(j)}\n\\end{aligned}\n\\end{equation*}\n($特征维数j = 0...m,、\\alpha为学习步长$)\n\\begin{equation*}\n\\begin{aligned}\n\\dfrac{\\delta}{\\delta\\theta_j}J(\\theta) & = \\sum_{i=1}^n\\left[y_i\\dfrac{1}{h_\\theta(x_i)}\\dfrac{\\delta}{\\delta\\theta_j} h_\\theta(x_i)-(1-y_i)\\dfrac{1}{1-h_\\theta(x_i)}\\dfrac{\\delta}{\\delta\\theta_j} h_\\theta(x_i) \\right]   \\\\\\\n& = .....\\\\\n& = \\theta_j+\\alpha\\sum_{i=1}^{n}(y_i - h_{\\theta}(x_i))x_i^{(j)}\n\\end{aligned}\n\\end{equation*}\n{% endraw %}\n这里省去了大部分求偏导的过程，详细的求解步骤，可以参见参考博客。博文虽用的是梯度下降法，但每一步的求偏导数过程是完全一致的。\n\n##计算中的向量化思想\n- 在求解最优化方法時，将数据向量化，用矩阵的方式计算，是一种很好的思想。\n- 借助numpy等包，使得以前针对单个数值编写的方法，对矩阵也有了很好的支持度。\n- 《机器学习实战》的实现代码中，使用梯度上升法求解$\\theta$時，就用了向量化思想，用矩阵乘法代替了循环。但书中直接给出了迭代求$\\theta$的公式，缺少了如上节类似的推导，初读还是有些令人费解。\n###上节已知求$\\theta$每一步的更新过程：\n{% raw %}\n\\begin{equation}\n\\begin{aligned}\n\\theta_j & = \\theta_j + \\alpha\\dfrac{\\delta}{\\delta\\theta_j}J(\\theta) \\\\\\\n& = \\theta_j+\\alpha\\sum_{i=1}^{n}(y_i - h_{\\theta}(x_i))x_i^{(j)}\n\\end{aligned}\n\\end{equation}\n{% endraw %}\n\n- 1、首先我们把特征系数$\\theta$用m×1的列向量表示。(假设有n个输入向量，特征共有m维,$\\theta_0x_0当作常量偏移$)：\n\\begin{equation\\*}\n\\theta = \\begin{bmatrix}\n\\theta_0 \\\\\\\n\\theta_1 \\\\\\\n... \\\\\\\n\\theta_m\n\\end{bmatrix}\n\\end{equation\\*}\n\n输入数据用n×m的矩阵表示：\n\\begin{equation\\*}\nX = \\begin{bmatrix}\nx_1 \\\\\\\nx_2 \\\\\\\n... \\\\\\\nx_n\n\\end{bmatrix} = \n\\begin{bmatrix}\nx_1^{(0)},x_1^{(1)}...x_1^{(m)} \\\\\\\nx_2^{(0)},x_2^{(1)}...x_2^{(m)}  \\\\\\\n... \\\\\\\nx_n^{(0)},x_n^{(1)}...x_n^{(m)} \n\\end{bmatrix}\n\\end{equation\\*}\n所以$z_n = \\theta_0x_n^{(0)}+\\theta_1x_n^{(1)}+..+\\theta_nx_n^{(m)} $可以用$X\\cdot\\theta$得出的n×1列向量来表示：\n\\begin{equation\\*}\nZ = \\begin{bmatrix}\n\\theta_0x_1^{(0)}+\\theta_1x_1^{(1)}+...+\\theta_mx_1^{(m)} \\\\\\\n\\theta_0x_2^{(0)}+\\theta_1x_2^{(1)}+...+\\theta_mx_2^{(m)}   \\\\\\\n... \\\\\\\n\\theta_0x_n^{(0)}+\\theta_1x_n^{(1)}+...+\\theta_mx_n^{(m)}  \n\\end{bmatrix} =\\begin{bmatrix}\nz_1 \\\\\\\nz_2 \\\\\\\n... \\\\\\\nz_n\n\\end{bmatrix}\n\\end{equation\\*}\n用矩阵乘法的列向量分解思想来说，$X\\cdot\\theta$表示$X$中的每一个列向量以系数$\\theta$进行`线性组合`，符合公式的含义。\n- 2、我们再用列向量$Y = (y\\_1,y\\_2..y\\_n)^\\mathrm{T}$表示输入数据的类别，然后我们对$Z$进行sigmoid函数运算，所以更新过程公式中$(y\\_i - h\\_{\\theta}(x\\_i))$可以用n×1的列向量E表示：\n\\begin{equation\\*}\nE = \\begin{bmatrix}\ny_1-\\sigma(z_1) \\\\\\\ny_2-\\sigma(z_2)  \\\\\\\n... \\\\\\\ny_n-\\sigma(z_n)  \n\\end{bmatrix} =\\begin{bmatrix}\ne_1 \\\\\\\ne_2 \\\\\\\n... \\\\\\\ne_n\n\\end{bmatrix}\n\\end{equation\\*}\n(这里的E代表误差error)\n- 3、最后我们求解整个式子$\\theta\\_j  = \\theta\\_j+\\alpha\\sum\\_{i=1}^{n}(y\\_i - h\\_{\\theta}(x\\_i))x\\_i^{(j)}$. 式中的连加同样可以通过矩阵乘法解决。\n将$X$转置：\n{% raw %}\n\\begin{equation*}\nX^\\mathrm{T} = \\begin{bmatrix}\nx_1,\nx_2,\n... \nx_n\n\\end{bmatrix} = \n\\begin{bmatrix}\nx_1^{(0)},x_2^{(0)}...x_n^{(0)} \\\\\nx_1^{(1)},x_2^{(1)}...x_n^{(1)}  \\\\\n...... \\\\\nx_1^{(m)},x_2^{(m)}...x_n^{(m)} \n\\end{bmatrix}\n\\end{equation*}\n{% endraw %}\n\n转置后矩阵是m×n的，仔细观察一下转置后的X，每一个列向量是一条输入数据，而**每一个行向量是所有输入数据在某一个特征维度上的记录**，一共有m个行向量。\n所以，我们同时用每一个行向量$\\cdot$列向量$E$,用矩阵乘法即：\n{% raw %}\n\\begin{equation*}\nX^\\mathrm{T}\\cdot\\mathrm{E} = \n\\begin{bmatrix}\nx_1^{(0)}e_1+x_2^{(0)}e_2+...+x_n^{(0)}e_n \\\\\nx_1^{(1)}e_1+x_2^{(1)}e_2+...+x_n^{(1)}e_n \\\\\n...... \\\\\\\nx_1^{(m)}e_1,x_2^{(m)}e_2+...+x_n^{(m)}e_n \n\\end{bmatrix}     \n\\end{equation*}\n{% endraw %}\n(m*1)\n- 4、已知增长系数$\\alpha$，然后我们就可以用矩阵计算进行一次梯度上升迭代：\n{% raw %}\n\\begin{equation*}\n\\theta_{n+1} = \\theta_n +\\alpha \\cdot X^\\mathrm{T}\\cdot\\mathrm{E} = \n\\begin{bmatrix}\n\\theta_{n+1}^{(0)} \\\\\n\\theta_{n+1}^{(1)} \\\\\n... \\\\\n\\theta_{n+1}^{(m)}\n\\end{bmatrix}     \n\\end{equation*}\n{% endraw %}\n\n5. 设置合适的迭代次数，求得最终的特征系数$\\theta$,我们就得到了训练好的判别函数。\n##小结：\n- 看完我们会发现，逻辑回归也是基于简单的线性分类思想，只不过逻辑回归通过一个契合线性分类的sigmoid函数，将数据到分界限的距离巧妙地投影到了0-1区间，进而可以将距离转化为概率，通过最大斯然估计求解。\n- 向量化的思想其实充斥着机器学习的各个角落。就像MIT线性代数公开课中说的，矩阵并不是生来存在的，而是人们后天发明用来方便计算的产物。将数据向量化，通过矩阵运算求解，是解决问题的必经之路。\n---\n这次公式比较多，处理markdown语法和$\\mathrm{\\LaTeX}$的冲突時花了不少时间，不过也因此找到一个好方法，不用修改node.js的配置就可以处理好语法冲突，只需要在```公式的前后用raw标签注释就可以了```，原理应该是通过标签屏蔽了markdown语法的解释器，前端不是很懂，不过很有效哦～\n\n\n\n\n\n","slug":"逻辑回归与计算中的向量化思想","published":1,"updated":"2015-08-17T02:59:26.000Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cih5p32wc000fasbg3jpafsgg","sticky":0},{"title":"迁移Hexo博客：GitHub至GitCafe","date":"2015-11-10T12:23:39.000Z","_content":"昨天着手将博客从GitHub迁移到GitCafe，心里长草多时终于付诸行动。整体过程还算顺利，迁移后功能基本正常，但中间遇到几点零碎问题，现记录如此，一供自己备忘，二可以帮助遇到同样困难的朋友。\n##引子\n这里主要谈谈迁移博客的原因，主要来自如下几点：\n- 搜索引擎检索：无法被百度检索是在Gitpages上部署博客最主要的问题，主要原因是Github服务器默认屏蔽了百度的爬虫。网上的几种解决方案并不触及本质，效果有限。\n- 访问速度：国内地址访问Github的速度还是不够完美\n- 屏蔽隐患：因为GFW有过暂时屏蔽Github的不良记录，所以博客被屏蔽的隐患始终存在。\n\n而相比较来说GitCafe因为是在国内的实现，这些问题基本都没有，而且两者本是同根生，迁移起来技术成本很小。但因为GitHub的影响力和作用也不想放弃，所以确定了迁移博客到GitCafe，同时两边同步更新的策略。\n\n##主要步骤\n共有如下几个关键点：\n### 项目迁移:\n需要在Gitcafe上生成一个相同的项目。GitCafe与GitHub本地环境相同，完全可以使用一套git软件，只是在远程项目配置上稍有不同。我们需要做的：\n- 注册GitCafe帐号，新建一个**与帐号同名的任务**，这样GitCafe会默认该项目为page项目。*(注意名称区分大小写，GitCafe的特点)*\n- 将本地博客项目push到Gitcafe项目下的gitcafe-pages分支：\n   - 进入hexo博客的默认部署目录 username.io/.deploy_git\n   - 添加新的远程仓库：\n   ```bash\n      $ git remote add gitcafe git@gitcafe.com:Username/Username.git\n   ```\n   *(add后用最好gitcafe而不是origin，否则与原来Github的远程仓库地址冲突；Username注意大小写)*\n   - push到GitCafe项目下的gitcafe-pages分支：\n   ```bash\n      $ git checkout gitcafe-pages\n      $ git push -u gitcafe gitcafe-pages\n   ```\n   *(注意push完再切换回master分支，因为hexo默认的部署文件我们都已master分支为主)*\n\n### 同步更新\n已完成迁移工作，下面需要配置同步更新Github和GitCafe的功能。我选择的修改hexo的配置文件，通过hexo命令完成同步更新的方法，很简单，感觉比再写一个同步更新脚本更舒服些。\n我们只需要修改一下根目录下hexo的配置文件_config.yml：\n```xml\ndeploy:\n  type: git\n  repo: \n    github: git@github.com:robinroar/robinroar.github.io,master\n    gitcafe: git@gitcafe.com:RobinROAR/RobinROAR.git,gitcafe-pages\n```\n*（可以看出一个是推送到GitHub项目的master分支，一个是推送到GitCafe项目的gitcafepages分支，省略了本地的默认分支master）*\n这样就可以用老方法**hexo d**同步更新两个项目.\n\n### 绑定域名\n最后需要绑定域名，这里注意几点：\n- 在服务商添加域名解析时不要使用A记录的方式了，因为网上原来的GitCafe服务器的Ip地址由于DOS攻击已经暂时在2015.5前后关闭，官网提供的解决方案是改用CNAME记录，指向Username.gitcafe.io即可。\n- 原来在博客文件目录中的CNAME文件需要移除，因为现在是一个项目两处同时更新，两处的CNAME文件指向同一个域名，好像会有点问题。解决方法很简单，只需要把CNAME文件移出根目录下的source目录即可，下此部署时该文件就不会部署到服务器了。\n\n*（但博客目录中的CNAME文件的作用我还有点疑惑，是将原本到username.github.io的访问导到CNAME中的地址？这样是否会引起循环？如有赐教不胜感激）*\n\n###It's done！\n\n\n\n\n\n##重要Tips\n- GitCafe区分大小写，包括url的地址，项目的名称等\n- Hexo通过配置文件进行push/pull操作的博客目录是的username.io/.deploy_git/\n- GitCafe下使用的分支是gitcafe-pages\n\n\n-Robin \n2015.11.10 夜","source":"_posts/迁移Hexo博客：GitHub至GitCafe.md","raw":"title: 迁移Hexo博客：GitHub至GitCafe\ndate: 2015-11-10 20:23:39\ntags: \n- hexo\n- Gitpages\n- GitCafe\n- 博客迁移\ncategories: \n- 技术\n---\n昨天着手将博客从GitHub迁移到GitCafe，心里长草多时终于付诸行动。整体过程还算顺利，迁移后功能基本正常，但中间遇到几点零碎问题，现记录如此，一供自己备忘，二可以帮助遇到同样困难的朋友。\n##引子\n这里主要谈谈迁移博客的原因，主要来自如下几点：\n- 搜索引擎检索：无法被百度检索是在Gitpages上部署博客最主要的问题，主要原因是Github服务器默认屏蔽了百度的爬虫。网上的几种解决方案并不触及本质，效果有限。\n- 访问速度：国内地址访问Github的速度还是不够完美\n- 屏蔽隐患：因为GFW有过暂时屏蔽Github的不良记录，所以博客被屏蔽的隐患始终存在。\n\n而相比较来说GitCafe因为是在国内的实现，这些问题基本都没有，而且两者本是同根生，迁移起来技术成本很小。但因为GitHub的影响力和作用也不想放弃，所以确定了迁移博客到GitCafe，同时两边同步更新的策略。\n\n##主要步骤\n共有如下几个关键点：\n### 项目迁移:\n需要在Gitcafe上生成一个相同的项目。GitCafe与GitHub本地环境相同，完全可以使用一套git软件，只是在远程项目配置上稍有不同。我们需要做的：\n- 注册GitCafe帐号，新建一个**与帐号同名的任务**，这样GitCafe会默认该项目为page项目。*(注意名称区分大小写，GitCafe的特点)*\n- 将本地博客项目push到Gitcafe项目下的gitcafe-pages分支：\n   - 进入hexo博客的默认部署目录 username.io/.deploy_git\n   - 添加新的远程仓库：\n   ```bash\n      $ git remote add gitcafe git@gitcafe.com:Username/Username.git\n   ```\n   *(add后用最好gitcafe而不是origin，否则与原来Github的远程仓库地址冲突；Username注意大小写)*\n   - push到GitCafe项目下的gitcafe-pages分支：\n   ```bash\n      $ git checkout gitcafe-pages\n      $ git push -u gitcafe gitcafe-pages\n   ```\n   *(注意push完再切换回master分支，因为hexo默认的部署文件我们都已master分支为主)*\n\n### 同步更新\n已完成迁移工作，下面需要配置同步更新Github和GitCafe的功能。我选择的修改hexo的配置文件，通过hexo命令完成同步更新的方法，很简单，感觉比再写一个同步更新脚本更舒服些。\n我们只需要修改一下根目录下hexo的配置文件_config.yml：\n```xml\ndeploy:\n  type: git\n  repo: \n    github: git@github.com:robinroar/robinroar.github.io,master\n    gitcafe: git@gitcafe.com:RobinROAR/RobinROAR.git,gitcafe-pages\n```\n*（可以看出一个是推送到GitHub项目的master分支，一个是推送到GitCafe项目的gitcafepages分支，省略了本地的默认分支master）*\n这样就可以用老方法**hexo d**同步更新两个项目.\n\n### 绑定域名\n最后需要绑定域名，这里注意几点：\n- 在服务商添加域名解析时不要使用A记录的方式了，因为网上原来的GitCafe服务器的Ip地址由于DOS攻击已经暂时在2015.5前后关闭，官网提供的解决方案是改用CNAME记录，指向Username.gitcafe.io即可。\n- 原来在博客文件目录中的CNAME文件需要移除，因为现在是一个项目两处同时更新，两处的CNAME文件指向同一个域名，好像会有点问题。解决方法很简单，只需要把CNAME文件移出根目录下的source目录即可，下此部署时该文件就不会部署到服务器了。\n\n*（但博客目录中的CNAME文件的作用我还有点疑惑，是将原本到username.github.io的访问导到CNAME中的地址？这样是否会引起循环？如有赐教不胜感激）*\n\n###It's done！\n\n\n\n\n\n##重要Tips\n- GitCafe区分大小写，包括url的地址，项目的名称等\n- Hexo通过配置文件进行push/pull操作的博客目录是的username.io/.deploy_git/\n- GitCafe下使用的分支是gitcafe-pages\n\n\n-Robin \n2015.11.10 夜","slug":"迁移Hexo博客：GitHub至GitCafe","published":1,"updated":"2015-11-12T01:45:33.000Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cih5p32wf000qasbg5lzvcnj4","sticky":0},{"title":"荆棘谷的青山","date":"2015-06-10T10:07:27.000Z","_content":"\n##正文\n\n- 在GitPages安家了，第一篇博文，又是新的一行＂Hello world＂，一次新的开始．\n\n- 建博客的念头由来已久，但又久未实行．总在挑什么语言，选什么框架，却忘了博客应该做什么．仔细想想，抛开技术因素，内容才是博客价值所在．\n\n- 博文会以技术为主，原创为主，偶尔也会写点风马牛不想及的东西，权且一乐。有时间也会逐步把有道云笔记的一直以来积累的东西，挑一挑转过来。希望能做到少而精，有些意义．就像荆棘谷里的那个知名任务，一片片零散的残章断卷后，讲述了一个美妙的故事．\n\n- 一直以来也拜读了许多大咖的博客，细致度与原创性都令人佩服，不求超越，只求也像他们一样有一颗沉静的心，能仔细的把好东西沉淀下来。\n\n- 读博路上人渐稀。正值毕业季，可惜这次我是一个孤独的旁观者，只好贪婪的呼吸点弥漫在空气之中的喜悦．前路漫漫，希望能从这篇博文开始，真正的做点事情.\n\n\n  \n 　　 \n　　\n---\n\n## 最后附一段Eminem的歌词，烂熟于心，很喜欢：\n\n> There's batteries in my Walkman nothing isthe matter with me\n> 即使生活充满悲剧，只要我Walkman还有电，困难算什么东西\n> Shit look on the bright side at least Iain't walking\n> 想想开心的事，至少我还有辆单车，是一个老司机\n> I bike ride through the neighborhood of myapartment\n> 我嗨皮的驶过邻居家门口\n> Complex on a ten speed which I've acquiredparts that I\n> 骑着这辆时速十码的组装车\n> Found in the garbage, a frame and put tireson it\n> 从垃圾堆里找的部件，然后把轮胎装到架子上\n> Headphones on straight ahead and kids tryto start shit\n> 耳机在耳边轰鸣，夹杂着熊孩子们的嘲笑\n> But if this all there is for me life offers\n> 如果这是生活赐予我的全部\n> Why bother even tryna put up a fight, it'snonsense\n> 那我为何还要与别人争斗？这毫无意义\n> But I think a lightbulb just lit up in my conscience\n> 突然有些东西，它照亮了我的脑海\n> What about those rhymes I've been jottin'\n> 是那些我随手记下的词句\n> They are kinda givin’ me confidence\n> 它一直给予我自信和希望\n> Instead of tryna escape through my comics,\n> 与其挣扎在这喜剧般的人生\n> Why don't I just blast a little somethinglike Onyx\n> 为何不像Onyx组合一样唱点什么？\n\n\n\nRobin\n2015.6.10   夜","source":"_posts/荆棘谷的青山.md","raw":"title: 荆棘谷的青山\ndate: 2015-06-10 18:07:27\ntags: 纪念\ncategories: 生活\n---\n\n##正文\n\n- 在GitPages安家了，第一篇博文，又是新的一行＂Hello world＂，一次新的开始．\n\n- 建博客的念头由来已久，但又久未实行．总在挑什么语言，选什么框架，却忘了博客应该做什么．仔细想想，抛开技术因素，内容才是博客价值所在．\n\n- 博文会以技术为主，原创为主，偶尔也会写点风马牛不想及的东西，权且一乐。有时间也会逐步把有道云笔记的一直以来积累的东西，挑一挑转过来。希望能做到少而精，有些意义．就像荆棘谷里的那个知名任务，一片片零散的残章断卷后，讲述了一个美妙的故事．\n\n- 一直以来也拜读了许多大咖的博客，细致度与原创性都令人佩服，不求超越，只求也像他们一样有一颗沉静的心，能仔细的把好东西沉淀下来。\n\n- 读博路上人渐稀。正值毕业季，可惜这次我是一个孤独的旁观者，只好贪婪的呼吸点弥漫在空气之中的喜悦．前路漫漫，希望能从这篇博文开始，真正的做点事情.\n\n\n  \n 　　 \n　　\n---\n\n## 最后附一段Eminem的歌词，烂熟于心，很喜欢：\n\n> There's batteries in my Walkman nothing isthe matter with me\n> 即使生活充满悲剧，只要我Walkman还有电，困难算什么东西\n> Shit look on the bright side at least Iain't walking\n> 想想开心的事，至少我还有辆单车，是一个老司机\n> I bike ride through the neighborhood of myapartment\n> 我嗨皮的驶过邻居家门口\n> Complex on a ten speed which I've acquiredparts that I\n> 骑着这辆时速十码的组装车\n> Found in the garbage, a frame and put tireson it\n> 从垃圾堆里找的部件，然后把轮胎装到架子上\n> Headphones on straight ahead and kids tryto start shit\n> 耳机在耳边轰鸣，夹杂着熊孩子们的嘲笑\n> But if this all there is for me life offers\n> 如果这是生活赐予我的全部\n> Why bother even tryna put up a fight, it'snonsense\n> 那我为何还要与别人争斗？这毫无意义\n> But I think a lightbulb just lit up in my conscience\n> 突然有些东西，它照亮了我的脑海\n> What about those rhymes I've been jottin'\n> 是那些我随手记下的词句\n> They are kinda givin’ me confidence\n> 它一直给予我自信和希望\n> Instead of tryna escape through my comics,\n> 与其挣扎在这喜剧般的人生\n> Why don't I just blast a little somethinglike Onyx\n> 为何不像Onyx组合一样唱点什么？\n\n\n\nRobin\n2015.6.10   夜","slug":"荆棘谷的青山","published":1,"updated":"2015-10-26T08:15:15.000Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cih5p32wh0010asbgazk6zdpz","sticky":0},{"title":"谈谈贝叶斯决策","date":"2015-07-14T01:44:57.000Z","mathjax":true,"_content":"\n\n**贝叶斯决策**, 一种常用的分类方法，复杂度低，准确度好，可以和多种分类方法结合。最高的理论正确率令人神往（上自动化所模式识别课中多次强调）。\n写一写学习贝叶斯方法中的几点理解，一个简单脉络而已，方便回顾，与大家分享。\n\n-------------------\n##1. 先验概率与后验概率\n两个概念：\n- `先验概率`： 事情还没有发生，要求这件事情发生的可能性的大小。如P(A).\n- `后验概率`： 事情已经发生，要求这件事情发生的原因`是由某个因素引起的`可能性的大小，是后验概率. \n\n所以我们可以使用贝叶斯公式求后验概率，进而达到分类的目的。\n\n\n##2. 贝叶斯公式\n\n$$ P(F\\_j|E)= \\dfrac{P(F\\_jE)}{P(E)} =  \\dfrac{P(E|F\\_j)P(F\\_j)}{ \\displaystyle{\\sum_{i=1}^{n} }P(E|F\\_i)P(F\\_i) } $$\n\n两个核心：\n- **推导**：条件概率公式。分解中间步骤分子，我们得出结果$P(F\\_j|E) \\cdot P(E)= P(F\\_jE) =   P(E|F\\_j) \\cdot P(F\\_j)$.   \n\n- **应用**：用于分类问题。分解中间步骤分母$P(E) = \\displaystyle{\\sum\\_{i=1}^{n} }P(E|F\\_i)P(F\\_i）$ ，每个$F\\_i$代表一个类别。\n\n扯点零碎，条件概率公式对任意两个事件都是存在的，当两事件独立(即$P(EF) = P(E)P(F)$)时，$P(E|F) = P(EF)/P(F) = P(E)$.\n\n\n\n##3. 例子\n看过很多讲解贝叶斯公式的例子，下面这个很好:\n> 一份信件等可能的在存在三个不同的文件夹中的任意一个。假设信件实际在文件夹$i$中$(i = 1,2,3)$而你经过对文件夹$i$的快速翻阅发现信件的概率记为$a\\_i$，问题是，假定你查看了文件夹1且没有发现此信，问信件在文件夹1中的概率是多少？\n                                                              ——[《应用随机过程：概率模型导论》](http://book.douban.com/subject/2309401/)\n\n**解**： 以$F\\_i(i = 1,2,3)$为信在文件中$i$中的事件`（先验概率）`，E是通过对文件夹1搜索而为看到信这个事件，我们求$P(F\\_1|E)$`(后验概率)`，通过贝叶斯公式：\n$$\tP(F\\_1|E)=  \\dfrac{P(E|F\\_1)P(F\\_1)}{ \\displaystyle{\\sum\\_{i=1}^{3} }P(E|F\\_i)P(F\\_i) } = \\dfrac{(1-a\\_1) \\frac{1}{3}}{ (1-a\\_1) \\frac{1}{3}+\\frac{1}{3}+\\frac{1}{3}} = \\dfrac{1-a\\_1}{3-a\\_1}$$\n\n把握两个重点：\n\t- 对问题中事件的假设\n\t- 对整体事件空间(分母)的划分\n\n\n##4. 一种简单实现：朴素贝叶斯\n贝叶斯思想只是一个框架，可结合许多假设与分布。\n朴素思分类就是一个通过先验概率求后验概率的方法，然后将实例点分到后验概率最大的类。就像街上有一个黑人，我们预测黑人来自非洲，为什么呢？因为黑人中非洲人的比率最高。\n####朴素贝叶斯的核心假设：\n1. 特征之间相互独立 ：这个“朴素”一词的来源。假设体现在向量的特征之间。所以有\n$$P(x|y\\_i)P(y\\_i)=P(a\\_1|y\\_i)P(a\\_2|y\\_i)...P(a\\_m|y\\_i)P(y\\_i)=P(y\\_i)\\prod^m_{j=1}P(a\\_j|y\\_i)$$\n\t- $y\\_i$为每一个类别，$a\\_i$ 为样本X的特征的每一维分量\n\t\n2. 每个特征同等重要。\n\n\n##5. 分类准则\n\n将输入向量分到后验概率最大的类，这就是`分类准则`。贝叶斯使用的是`最大后验概率准则`。\n在别的分类方法中，我们还用过`最小错误率`的分类准则，比如基于0-1损失的最小错误率，这两者是一样的。\n\n设损失函数\n\n$L(Y,f(X))=$\n\n\\begin{align\\*}\n1,Y \\not= f(X) \\\\\\\n0, Y= f(X) \\\\\\\n\\end{align\\*}\n\n\n- 期望风险函数为\n\n\\begin{equation\\*}\n\\begin{aligned}  \nR\\_e(f(x)) & = E[L(Y, f(x))]\\\\\\\n& =  \\iint\\_{X,Y}L(Y,f(X))P(X,Y)\\mathrm{d}x\\mathrm{d}y \\\\\\\n&= \\iint\\_{X,Y}L(Y,f(X))P(Y|X)P(X)\\mathrm{d}x\\mathrm{d}y\\\\\\\n&= \\int\\_{X} (\\sum\\_{k=1}^{k}L(Y= c\\_k,f(X))P(Y=c\\_k|X) )P(X)\\mathrm{d}x\\\\\\\n&= E\\_x(\\sum_{k=1}^{k}L(Y= c\\_k,f(X))P(Y=c\\_k|X) )\n\\end{aligned}\n\\end{equation\\*}\n\n- 为了使期望最小，对$X=x$逐个极小化：\n\n\\begin{equation\\*}\n\\begin{aligned} \nf(x)& = \\arg\\min \\limits\\_{y\\in\\mathcal{Y}}\\sum\\_{k=1}^{k}L(c\\_k,y)P(c\\_k|X=x) \\\\\\\n&=  \\arg\\min \\limits\\_{y\\in\\mathcal{Y}}\\sum\\_{k=1}^{k}P(y \\not = c\\_k|X=x) \\\\\\\n&=  \\arg\\min \\limits\\_{y\\in\\mathcal{Y}}\\sum\\_{k=1}^{k}(1-P(y  = c\\_k|X=x) )\\\\\\\n&=  \\arg\\max \\limits\\_{y\\in\\mathcal{Y}}P(y = c\\_k|X = x)\n\\end{aligned}\n\\end{equation\\*}\n\n可以看出，这就是`最大后验概率准则`，也是我们使用贝叶斯决策的分类准则。\n\n\n---\n\n暂时先写这几点，顺带说一句，Hexo上写$\\mathrm{\\LaTeX}$公式，冲突比较多，还在寻找比较好的解决方法，\n\nRobin \n2015.7.10 夜 \n","source":"_posts/简谈贝叶斯决策.md","raw":"title: 谈谈贝叶斯决策\ndate: 2015-07-14 09:44:57\ntags: \n- 贝叶斯\n- 分类\n- 机器学习\ncategories: \n- 技术\nmathjax: true \n---\n\n\n**贝叶斯决策**, 一种常用的分类方法，复杂度低，准确度好，可以和多种分类方法结合。最高的理论正确率令人神往（上自动化所模式识别课中多次强调）。\n写一写学习贝叶斯方法中的几点理解，一个简单脉络而已，方便回顾，与大家分享。\n\n-------------------\n##1. 先验概率与后验概率\n两个概念：\n- `先验概率`： 事情还没有发生，要求这件事情发生的可能性的大小。如P(A).\n- `后验概率`： 事情已经发生，要求这件事情发生的原因`是由某个因素引起的`可能性的大小，是后验概率. \n\n所以我们可以使用贝叶斯公式求后验概率，进而达到分类的目的。\n\n\n##2. 贝叶斯公式\n\n$$ P(F\\_j|E)= \\dfrac{P(F\\_jE)}{P(E)} =  \\dfrac{P(E|F\\_j)P(F\\_j)}{ \\displaystyle{\\sum_{i=1}^{n} }P(E|F\\_i)P(F\\_i) } $$\n\n两个核心：\n- **推导**：条件概率公式。分解中间步骤分子，我们得出结果$P(F\\_j|E) \\cdot P(E)= P(F\\_jE) =   P(E|F\\_j) \\cdot P(F\\_j)$.   \n\n- **应用**：用于分类问题。分解中间步骤分母$P(E) = \\displaystyle{\\sum\\_{i=1}^{n} }P(E|F\\_i)P(F\\_i）$ ，每个$F\\_i$代表一个类别。\n\n扯点零碎，条件概率公式对任意两个事件都是存在的，当两事件独立(即$P(EF) = P(E)P(F)$)时，$P(E|F) = P(EF)/P(F) = P(E)$.\n\n\n\n##3. 例子\n看过很多讲解贝叶斯公式的例子，下面这个很好:\n> 一份信件等可能的在存在三个不同的文件夹中的任意一个。假设信件实际在文件夹$i$中$(i = 1,2,3)$而你经过对文件夹$i$的快速翻阅发现信件的概率记为$a\\_i$，问题是，假定你查看了文件夹1且没有发现此信，问信件在文件夹1中的概率是多少？\n                                                              ——[《应用随机过程：概率模型导论》](http://book.douban.com/subject/2309401/)\n\n**解**： 以$F\\_i(i = 1,2,3)$为信在文件中$i$中的事件`（先验概率）`，E是通过对文件夹1搜索而为看到信这个事件，我们求$P(F\\_1|E)$`(后验概率)`，通过贝叶斯公式：\n$$\tP(F\\_1|E)=  \\dfrac{P(E|F\\_1)P(F\\_1)}{ \\displaystyle{\\sum\\_{i=1}^{3} }P(E|F\\_i)P(F\\_i) } = \\dfrac{(1-a\\_1) \\frac{1}{3}}{ (1-a\\_1) \\frac{1}{3}+\\frac{1}{3}+\\frac{1}{3}} = \\dfrac{1-a\\_1}{3-a\\_1}$$\n\n把握两个重点：\n\t- 对问题中事件的假设\n\t- 对整体事件空间(分母)的划分\n\n\n##4. 一种简单实现：朴素贝叶斯\n贝叶斯思想只是一个框架，可结合许多假设与分布。\n朴素思分类就是一个通过先验概率求后验概率的方法，然后将实例点分到后验概率最大的类。就像街上有一个黑人，我们预测黑人来自非洲，为什么呢？因为黑人中非洲人的比率最高。\n####朴素贝叶斯的核心假设：\n1. 特征之间相互独立 ：这个“朴素”一词的来源。假设体现在向量的特征之间。所以有\n$$P(x|y\\_i)P(y\\_i)=P(a\\_1|y\\_i)P(a\\_2|y\\_i)...P(a\\_m|y\\_i)P(y\\_i)=P(y\\_i)\\prod^m_{j=1}P(a\\_j|y\\_i)$$\n\t- $y\\_i$为每一个类别，$a\\_i$ 为样本X的特征的每一维分量\n\t\n2. 每个特征同等重要。\n\n\n##5. 分类准则\n\n将输入向量分到后验概率最大的类，这就是`分类准则`。贝叶斯使用的是`最大后验概率准则`。\n在别的分类方法中，我们还用过`最小错误率`的分类准则，比如基于0-1损失的最小错误率，这两者是一样的。\n\n设损失函数\n\n$L(Y,f(X))=$\n\n\\begin{align\\*}\n1,Y \\not= f(X) \\\\\\\n0, Y= f(X) \\\\\\\n\\end{align\\*}\n\n\n- 期望风险函数为\n\n\\begin{equation\\*}\n\\begin{aligned}  \nR\\_e(f(x)) & = E[L(Y, f(x))]\\\\\\\n& =  \\iint\\_{X,Y}L(Y,f(X))P(X,Y)\\mathrm{d}x\\mathrm{d}y \\\\\\\n&= \\iint\\_{X,Y}L(Y,f(X))P(Y|X)P(X)\\mathrm{d}x\\mathrm{d}y\\\\\\\n&= \\int\\_{X} (\\sum\\_{k=1}^{k}L(Y= c\\_k,f(X))P(Y=c\\_k|X) )P(X)\\mathrm{d}x\\\\\\\n&= E\\_x(\\sum_{k=1}^{k}L(Y= c\\_k,f(X))P(Y=c\\_k|X) )\n\\end{aligned}\n\\end{equation\\*}\n\n- 为了使期望最小，对$X=x$逐个极小化：\n\n\\begin{equation\\*}\n\\begin{aligned} \nf(x)& = \\arg\\min \\limits\\_{y\\in\\mathcal{Y}}\\sum\\_{k=1}^{k}L(c\\_k,y)P(c\\_k|X=x) \\\\\\\n&=  \\arg\\min \\limits\\_{y\\in\\mathcal{Y}}\\sum\\_{k=1}^{k}P(y \\not = c\\_k|X=x) \\\\\\\n&=  \\arg\\min \\limits\\_{y\\in\\mathcal{Y}}\\sum\\_{k=1}^{k}(1-P(y  = c\\_k|X=x) )\\\\\\\n&=  \\arg\\max \\limits\\_{y\\in\\mathcal{Y}}P(y = c\\_k|X = x)\n\\end{aligned}\n\\end{equation\\*}\n\n可以看出，这就是`最大后验概率准则`，也是我们使用贝叶斯决策的分类准则。\n\n\n---\n\n暂时先写这几点，顺带说一句，Hexo上写$\\mathrm{\\LaTeX}$公式，冲突比较多，还在寻找比较好的解决方法，\n\nRobin \n2015.7.10 夜 \n","slug":"简谈贝叶斯决策","published":1,"updated":"2015-11-09T11:22:52.000Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cih5p32wk0015asbg0uz30vni","sticky":0},{"title":"理解Linux的环境变量、启动文件和命令目录","date":"2015-06-22T03:53:59.000Z","_content":"\n\n在搭建hadoop 的环境 时配置环境变量的时候遇到些问题，于是系统总结了Linux中的环境变量、启动文件等相关知识发,以供备忘。\n\n##1.  环境变量的作用  \n在不同系统中，环境变量总是扮演着相同的角色，安装完新环境，新软件，第一件事总是配置环境变量。\n###环境变量 ： \n- 存储有关shell会话和`工作环境`的信息（如Java，hadoop，python等路径，主要是各个bin文件夹的绝对地址）\n- 变量储存在`内存`中，方便程序以及脚本访问\n\n\n##2.  环境变量的分类：\n\n###局部变量：\n- 特性：`作用于当前shell，`在子shell或者其它shell不可见（反映了局部变量的在当前进程可用的意义）  **见例1**\n- 定义：\n```bash \n        $  varname=abc       #局部变量名尽量使用小写; = 左右不要有空格，否则变量名会被解析为命令\n```\n- 显示： \n```bash\n        $  echo $varname     #echo命令要使用$变量名\n```\n- 删除：       \n```bash\n        $ unset varname   \n```\n\n###全局变量：\n- 特性：作用于当前shell`及所有shell创建的子进程`; 系统登录时已经默认设置了许多全局环境变量。  **见例2**\n- 定义：\n```bash\n        $  varname=abc \n        $  export varname    #先建立局部变量，用export导成全局变量\n```\n- 显示：\n```bash\n        $ printenv                 #显示系统环境变量，系统环境变量一律大写  \n        $ echo $varname      #显示单个变量\n```\n\n- 删除： \n```bash\n        $ unset varname      #只对子进程中变量有效，父进程中的全局变量仍然存在。\n```\n\n##3.  系统默认定义的全局环境变量\nPATH变量是我们经常使用和修改的环境变量，它定义了`提供命令解析的搜索路径`。所以每当我们安装了新的环境，总要在更新PATH变量，这样就可以直接在任何位置使用命令，而不会出现commond not found的问题。 <br />　　其它系统变量的修改与添加也大致一样。\n###PATH变量：  \n- 特性： 命令行输入命令的搜索路径, 如：\n```bash\n    $ start-dfs.sh   #启动HDFS，而不用进入../hadoop2.5.2/sbin/目录下\n```\n- 查看： \n```bash\n    $ echo $PATH\n    /usr/local/bin:/usr/bin:/bin:/usr/local/games:/usr/games    #不同值之间由:分割\n```\n- 添加值，修改：\n```bash\n    $ PATH=$PATH:/home/user/test  #添加目录/home/user/test, 只在当前shell有效\n    $ export PATH=.:/HOME/eobin:$PATH     #单点符在PATH变量中代表当前路径，变量名可以放在末尾\n```\n通常我们把对于PATH变量的改动`写在系统或用户的启动文件中`，比如说~/.bashrc中，在登陆系统或者打开新shell时自动加载。\n\n##4. 启动文件与系统环境变量的加载：\n\n###/etc/profile:    \n- 作用：系统默认的`主启动文件`，每个用户登录时都会加载这个文件。\n- 变量持续时间：声明的变量会在每个新shell中存在，除非在子进程中被修改\n\n###$HOME/.bash_profile:  \n- 作用：用户专属的启动文件， 定义用户专属的环境变量，该文件会`先检查并加载HOME目录中的.bashrc文件`（如果存在）\n\n\n\n###$HOME/.bashrc:   \n- 作用： `交互式shell的启动文件`，用于定制自己的命名别名和私有脚本\n- 这里有一点疑问，书中提到\n>如果bash是作为交互式shell启动的，它不去访问/etc/profile, 而会去用户的HOME目录检查.bashrc是否存在  - p116 《Linux命令行与Shell脚本大全》\n\n但/etc/profile是随系统登录时就启动的，任何`交互式shell`都是其子进程，所以我认为不用访问自然会加载其中变量，而此处特地指出，不明其所以然，也许没有理解交互式shell的意义。当然结果并没有什么不同，都会以.bashrc中的设置为准。\n\n### 一般来说Linux启动加载配置文件顺序如下：/etc/profile → /etc/profile.d/*.sh → ~/.bash_profile → ~/.bashrc → [/etc/bashrc]\n\n##5. 附例：\n\n- **例1**   局部变量只在当前进程中有效\n```bash\nrobin@esp:~$ varname=abc       定义变量varname\nrobin@esp:~$ echo $varname     显示，有值\nabc\nrobin@esp:~$ bash              新建子进程\nrobin@esp:~$ echo $varname     显示，无值\n\nrobin@esp:~$ exit              退出子进程\nexit\nrobin@esp:~$ echo $varname     显示，有值\nabc\n```\n\n- **例2**   全局变量的作用范围\n```bash\nrobin@esp:~$ echo $varname           显示变量，无值\n\nrobin@esp:~$ bash                    新建子进程2\nrobin@esp:~$ export varname=bash2    定义全局变量\nrobin@esp:~$ echo $varname           显示，有值\nbash2\nrobin@esp:~$ bash                    新建子进程3\nrobin@esp:~$ echo $varname           显示，有值\nbash2\nrobin@esp:~$ exit                    退出到bash2\nexit\nrobin@esp:~$ exit                    退出到原始进程\nexit\nrobin@esp:~$ echo $varname           显示，无值\n```\n\n##几个命令目录详解：/bin,/sbin,/usr/sbin,/usr/bin \n经过对环境变量的了解，我们发现上述四个目录经常被写在环境变量中。这些目录的作用，是存放各种'存放命令'。\n###/sbin：\n从名字来解释，bin是binary的所写，而s可以理解为super的所写。所以/sbin目录主要用于存放一些超级用户指令，同时，也必须要求用用有root权限才能使用。\n这些命令主要有：:cfdisk、dhcpcd、dump、e2fsck、fdisk、halt、ifconfig、ifup、 ifdown、init、insmod、lilo、lsmod、mke2fs、modprobe、quotacheck、reboot、rmmod、 runlevel、shutdown等。\n###/bin：\n与上面相同，/bin目录中也是系统命令，是一些较为普通的基本指令，一般来说管理员和普通用户都可以使用/bin目录下的命令。\n主要包括cat、cp、chmod df、dmesg、gzip、kill、ls、mkdir、more、mount、rm、su、tar等。\n###/usr/sbin:\n该目录主要存放依稀用用户资助安装的系统管理程序。\n包括dhcpd、httpd、imap、in.*d、inetd、lpd、named、netconfig、nmbd、samba、sendmail、squid、swap、tcpd、tcpdump等。\n###/usr/bin：\n这个目录的内容就比较广泛，基本包括了用户安装的各种软件运行脚本以及执行命令。\n如c++、g++、gcc、chdrv、diff、dig、du、eject、elm、free、gnome*、gzip、htpasswd、kfm、ktop、last、less、locale、m4、make、man、mcopy、ncftp、 newaliases、nslookup passwd、quota、smb*、wget等。\n###Tips：\n- 如果命令调用不到，就要考虑以上四个目录是否在你的PATH变量中.比如说，查看得知：PATH=$PATH:$HOME/bin。那么，我们则需如下改动：    PATH=$PATH:$HOME/bin:/sbin:/usr/bin:/usr/sbin\n- 善于创建链接：ln命令：\n比如我们安装包Node.js,由于各种原因，安装后/usr/bin中默认的调用命令是nodejs,而其他第三方包写在脚本中的调用方法是node，这时有两个解决方法：\n\t1. 使用alias命令：建立 alias node=nodejs   这种方法可以解决bash中输入命令的问题，但却在一些第三方包安装过程无效。\n\t2. 使用ln命令建立软链接： ln -s /usr/bin/nodejs /usr/bin/node  软链接可以完美解决第三方包安装的问题，具体使用可查询ln命令。 \n\n\n##END\n\nRobin \n2015.6.22 夜\n","source":"_posts/理解Linux的环境变量、启动文件与命令目录.md","raw":"title: 理解Linux的环境变量、启动文件和命令目录\ndate: 2015-06-22 11:53:59\ntags: \n- linux\n- 环境变量\ncategories: \n- 技术\n---\n\n\n在搭建hadoop 的环境 时配置环境变量的时候遇到些问题，于是系统总结了Linux中的环境变量、启动文件等相关知识发,以供备忘。\n\n##1.  环境变量的作用  \n在不同系统中，环境变量总是扮演着相同的角色，安装完新环境，新软件，第一件事总是配置环境变量。\n###环境变量 ： \n- 存储有关shell会话和`工作环境`的信息（如Java，hadoop，python等路径，主要是各个bin文件夹的绝对地址）\n- 变量储存在`内存`中，方便程序以及脚本访问\n\n\n##2.  环境变量的分类：\n\n###局部变量：\n- 特性：`作用于当前shell，`在子shell或者其它shell不可见（反映了局部变量的在当前进程可用的意义）  **见例1**\n- 定义：\n```bash \n        $  varname=abc       #局部变量名尽量使用小写; = 左右不要有空格，否则变量名会被解析为命令\n```\n- 显示： \n```bash\n        $  echo $varname     #echo命令要使用$变量名\n```\n- 删除：       \n```bash\n        $ unset varname   \n```\n\n###全局变量：\n- 特性：作用于当前shell`及所有shell创建的子进程`; 系统登录时已经默认设置了许多全局环境变量。  **见例2**\n- 定义：\n```bash\n        $  varname=abc \n        $  export varname    #先建立局部变量，用export导成全局变量\n```\n- 显示：\n```bash\n        $ printenv                 #显示系统环境变量，系统环境变量一律大写  \n        $ echo $varname      #显示单个变量\n```\n\n- 删除： \n```bash\n        $ unset varname      #只对子进程中变量有效，父进程中的全局变量仍然存在。\n```\n\n##3.  系统默认定义的全局环境变量\nPATH变量是我们经常使用和修改的环境变量，它定义了`提供命令解析的搜索路径`。所以每当我们安装了新的环境，总要在更新PATH变量，这样就可以直接在任何位置使用命令，而不会出现commond not found的问题。 <br />　　其它系统变量的修改与添加也大致一样。\n###PATH变量：  \n- 特性： 命令行输入命令的搜索路径, 如：\n```bash\n    $ start-dfs.sh   #启动HDFS，而不用进入../hadoop2.5.2/sbin/目录下\n```\n- 查看： \n```bash\n    $ echo $PATH\n    /usr/local/bin:/usr/bin:/bin:/usr/local/games:/usr/games    #不同值之间由:分割\n```\n- 添加值，修改：\n```bash\n    $ PATH=$PATH:/home/user/test  #添加目录/home/user/test, 只在当前shell有效\n    $ export PATH=.:/HOME/eobin:$PATH     #单点符在PATH变量中代表当前路径，变量名可以放在末尾\n```\n通常我们把对于PATH变量的改动`写在系统或用户的启动文件中`，比如说~/.bashrc中，在登陆系统或者打开新shell时自动加载。\n\n##4. 启动文件与系统环境变量的加载：\n\n###/etc/profile:    \n- 作用：系统默认的`主启动文件`，每个用户登录时都会加载这个文件。\n- 变量持续时间：声明的变量会在每个新shell中存在，除非在子进程中被修改\n\n###$HOME/.bash_profile:  \n- 作用：用户专属的启动文件， 定义用户专属的环境变量，该文件会`先检查并加载HOME目录中的.bashrc文件`（如果存在）\n\n\n\n###$HOME/.bashrc:   \n- 作用： `交互式shell的启动文件`，用于定制自己的命名别名和私有脚本\n- 这里有一点疑问，书中提到\n>如果bash是作为交互式shell启动的，它不去访问/etc/profile, 而会去用户的HOME目录检查.bashrc是否存在  - p116 《Linux命令行与Shell脚本大全》\n\n但/etc/profile是随系统登录时就启动的，任何`交互式shell`都是其子进程，所以我认为不用访问自然会加载其中变量，而此处特地指出，不明其所以然，也许没有理解交互式shell的意义。当然结果并没有什么不同，都会以.bashrc中的设置为准。\n\n### 一般来说Linux启动加载配置文件顺序如下：/etc/profile → /etc/profile.d/*.sh → ~/.bash_profile → ~/.bashrc → [/etc/bashrc]\n\n##5. 附例：\n\n- **例1**   局部变量只在当前进程中有效\n```bash\nrobin@esp:~$ varname=abc       定义变量varname\nrobin@esp:~$ echo $varname     显示，有值\nabc\nrobin@esp:~$ bash              新建子进程\nrobin@esp:~$ echo $varname     显示，无值\n\nrobin@esp:~$ exit              退出子进程\nexit\nrobin@esp:~$ echo $varname     显示，有值\nabc\n```\n\n- **例2**   全局变量的作用范围\n```bash\nrobin@esp:~$ echo $varname           显示变量，无值\n\nrobin@esp:~$ bash                    新建子进程2\nrobin@esp:~$ export varname=bash2    定义全局变量\nrobin@esp:~$ echo $varname           显示，有值\nbash2\nrobin@esp:~$ bash                    新建子进程3\nrobin@esp:~$ echo $varname           显示，有值\nbash2\nrobin@esp:~$ exit                    退出到bash2\nexit\nrobin@esp:~$ exit                    退出到原始进程\nexit\nrobin@esp:~$ echo $varname           显示，无值\n```\n\n##几个命令目录详解：/bin,/sbin,/usr/sbin,/usr/bin \n经过对环境变量的了解，我们发现上述四个目录经常被写在环境变量中。这些目录的作用，是存放各种'存放命令'。\n###/sbin：\n从名字来解释，bin是binary的所写，而s可以理解为super的所写。所以/sbin目录主要用于存放一些超级用户指令，同时，也必须要求用用有root权限才能使用。\n这些命令主要有：:cfdisk、dhcpcd、dump、e2fsck、fdisk、halt、ifconfig、ifup、 ifdown、init、insmod、lilo、lsmod、mke2fs、modprobe、quotacheck、reboot、rmmod、 runlevel、shutdown等。\n###/bin：\n与上面相同，/bin目录中也是系统命令，是一些较为普通的基本指令，一般来说管理员和普通用户都可以使用/bin目录下的命令。\n主要包括cat、cp、chmod df、dmesg、gzip、kill、ls、mkdir、more、mount、rm、su、tar等。\n###/usr/sbin:\n该目录主要存放依稀用用户资助安装的系统管理程序。\n包括dhcpd、httpd、imap、in.*d、inetd、lpd、named、netconfig、nmbd、samba、sendmail、squid、swap、tcpd、tcpdump等。\n###/usr/bin：\n这个目录的内容就比较广泛，基本包括了用户安装的各种软件运行脚本以及执行命令。\n如c++、g++、gcc、chdrv、diff、dig、du、eject、elm、free、gnome*、gzip、htpasswd、kfm、ktop、last、less、locale、m4、make、man、mcopy、ncftp、 newaliases、nslookup passwd、quota、smb*、wget等。\n###Tips：\n- 如果命令调用不到，就要考虑以上四个目录是否在你的PATH变量中.比如说，查看得知：PATH=$PATH:$HOME/bin。那么，我们则需如下改动：    PATH=$PATH:$HOME/bin:/sbin:/usr/bin:/usr/sbin\n- 善于创建链接：ln命令：\n比如我们安装包Node.js,由于各种原因，安装后/usr/bin中默认的调用命令是nodejs,而其他第三方包写在脚本中的调用方法是node，这时有两个解决方法：\n\t1. 使用alias命令：建立 alias node=nodejs   这种方法可以解决bash中输入命令的问题，但却在一些第三方包安装过程无效。\n\t2. 使用ln命令建立软链接： ln -s /usr/bin/nodejs /usr/bin/node  软链接可以完美解决第三方包安装的问题，具体使用可查询ln命令。 \n\n\n##END\n\nRobin \n2015.6.22 夜\n","slug":"理解Linux的环境变量、启动文件与命令目录","published":1,"updated":"2015-11-19T02:40:59.000Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cih5p32wm001basbgxu1yt0lh","sticky":0},{"title":"卷积的理解","date":"2015-11-17T13:32:28.000Z","mathjax":true,"_content":"最近计算机视觉课上讲到了卷积，之前没有很好理解，趁此做一个总结。\n\n##定义：\n\n定义一种卷积运算，将两个函数$g(x)$与$f(x)$通过该运算得到$h(x)$\n- 卷积有多重解释。泛函中解释为两个函数生成第三个函数的数学算子；概率论解释为两变量叠加的概率密度等，这些解释的特点是两个卷积函数地位是相同的，对应$(f\\otimes g) (x) = (g\\otimes f) (x)$，具体也有推导，我们不详细研究。\n- 这里主要从图像处理和信号方面理解，所以区分原函数与响应函数会比较直观。\n\n##公式：\n假设$f(x)$为原函数，$g(x)$为卷积函数，$h(x)$为卷积运算后新生成的函数\n- 一维连续： \n$$\nh(x) = \\int_{-\\infty} ^ \\infty g(\\tau)f(x-\\tau)d\\tau\n$$\n- 一维离散：\n$$\nh(x) = \\sum_\\tau ^ \\infty g(\\tau)f(x-\\tau)\n$$\n- 从公式理解：\n\t- 无限多的离散点就构成了连续，两公式所含意义相同。但往往通过离散公式举例更容易理解。\n\t- 谈一下$\\tau$。卷积可以理解为**对冲击响应的持续叠加**，比如向水中投石头的例子，那这里$-\\tau$代表的是时间维度上的残余影响，及过去的单位刺激（负的时间）对现在的的影响，也就是所谓的\"翻转\"。\n\n##图像：\n四幅图对照下文，分别解释了离散和连续情况的卷积运算,后三幅图的三个函数图像分别为原函数$f(x)$，卷积响应函数$g(x)$，和最终卷积运算的结果$h(x)$。\n其中图1直接取自百度，图2-4是用matplotlib绘成，使用离散点模拟的连续函数。\n<table border=\"1\" align=\"center\"><span style=\"font-size:24px;color:#006600;\"></span>  \n<caption align=\"top\">函数图像</caption>  \n<tr><td>图一<img src=\"http://7xjz3b.com1.z0.glb.clouddn.com/juanji1.jpg\" width = \"300\" height = \"200\" alt=\"\" align=center /></td> <td>图2<img src=\"http://7xjz3b.com1.z0.glb.clouddn.com/figure_3.png\" width = \"300\" height = \"200\" alt=\"\" align=center /></td> </tr>\n<tr><td>图3<img src=\"http://7xjz3b.com1.z0.glb.clouddn.com/figure_1.png\" width = \"300\" height = \"200\" alt=\"\" align=center /></td> <td>图4<img src=\"http://7xjz3b.com1.z0.glb.clouddn.com/figure_2.png\" width = \"300\" height = \"200\" alt=\"\" align=center /></td> </tr>\n</table>  \n\n##解释：\n对卷积的理解很多，比较有直观的有[打拳](http://www.zhihu.com/question/22298352)、[向水中投石](http://www.zhihu.com/question/20500497)这两个例子。这些例子有几个共同的特点：\n- 都有一个单位刺激的响应函数。如打一拳引起的疼痛，投一个石头引起的水花；\n- 若力度相同（或是石头重量一致），原函数都是$f(x)=1$。把时间推移理解为横轴x；如果拳头的力量或是石头的大小有改变，则原函数$f(x)$变为图4中有波动的原函数。\n- 例子中每个时间点观察的结果都是前面所有时间单位响应到目前结果的叠加，与卷积运算得出新函数的意义相同。\n- 都是以离散点举例，最后想象无限多个点，进而演变成连续的解释。\n\n结合函数图像来理解，离散的情况如图1，连续情况如图2,3,4。在图2中，g(x)单位高斯函数，在原函数f(x)的上的卷积运算，用引用的帖子中总结概括：**各个时间点上的单位响应的加权叠加**。这里的加权，就是指的原函数f(x)中的函数值。\n\n##我的理解：\n- 理解x轴为时间尺度，如果原函数是$f(x)$，响应函数$g(x)$是（0，1）这个点，那么卷积计算结果$h(x) = g(x) \\otimes f(x)$的卷积结果是原函数$f(x)$。这是因为响应函数只存在于一点，所以，不会对后面的时间有影响，所以不存在随x轴的叠加————证明了叠加性（图2）\n- 如果原函数f(x)是一个常数函数，那么无论单位响应函数形式如何，最后的卷积结果h(x)一定会趋向不变。这是因为原函数值不变，所以权值相同，时间尺度上的叠加都相同，所以函数值稳定后会不变———证明了加权（图3）。\n\n总结一下: **卷积运算可以理解成以原函数$f(x)$为尺度，以响应函数$g(x)$为单位的，随时间尺度的叠加。**\n\n\n##图像处理的卷积：\n在图像中，卷积的使用方式主要是卷积模板的形式。原始的图像可以看做二维数字信号，则可以看做原始图像每个像素与固定长宽的卷积模板做卷积运算。\n$$\nG(x,y) \\otimes I(x,y) = \\sum\\_{u=-M}^M \\sum\\_{v=-N}^N G(u,v)I(x-u,y-v)\n$$\n其中$I(x,y)$代表图像中每一个像素，$G(u,v)$表示尺寸为(2M+1)*(2N+1)的模板\n![](http://7xjz3b.com1.z0.glb.clouddn.com/juanji5.jpg)\n这些卷积模板是对连续卷积核函数的数字采样近似，相当于上面用离散点近似连续卷积函数。\n不同的模板决定了卷积的不同功能：去噪，平滑，匹配等。\n\n##End.\n\n国内的理解与言论貌似只有知乎值得一看，想得到一些更直观的理解时，这里的看法往往独到且深入。而像百度百科、各种博客等大众性解释确误导性严重，看来普及性和质量往往难以共存。\n\n\n\n\n","source":"_posts/卷积的理解.md","raw":"title: 卷积的理解\ndate: 2015-11-17 21:32:28\ntags:\n- 卷积\n- 图像处理\ncategories: \n- 基础\nmathjax: true \n---\n最近计算机视觉课上讲到了卷积，之前没有很好理解，趁此做一个总结。\n\n##定义：\n\n定义一种卷积运算，将两个函数$g(x)$与$f(x)$通过该运算得到$h(x)$\n- 卷积有多重解释。泛函中解释为两个函数生成第三个函数的数学算子；概率论解释为两变量叠加的概率密度等，这些解释的特点是两个卷积函数地位是相同的，对应$(f\\otimes g) (x) = (g\\otimes f) (x)$，具体也有推导，我们不详细研究。\n- 这里主要从图像处理和信号方面理解，所以区分原函数与响应函数会比较直观。\n\n##公式：\n假设$f(x)$为原函数，$g(x)$为卷积函数，$h(x)$为卷积运算后新生成的函数\n- 一维连续： \n$$\nh(x) = \\int_{-\\infty} ^ \\infty g(\\tau)f(x-\\tau)d\\tau\n$$\n- 一维离散：\n$$\nh(x) = \\sum_\\tau ^ \\infty g(\\tau)f(x-\\tau)\n$$\n- 从公式理解：\n\t- 无限多的离散点就构成了连续，两公式所含意义相同。但往往通过离散公式举例更容易理解。\n\t- 谈一下$\\tau$。卷积可以理解为**对冲击响应的持续叠加**，比如向水中投石头的例子，那这里$-\\tau$代表的是时间维度上的残余影响，及过去的单位刺激（负的时间）对现在的的影响，也就是所谓的\"翻转\"。\n\n##图像：\n四幅图对照下文，分别解释了离散和连续情况的卷积运算,后三幅图的三个函数图像分别为原函数$f(x)$，卷积响应函数$g(x)$，和最终卷积运算的结果$h(x)$。\n其中图1直接取自百度，图2-4是用matplotlib绘成，使用离散点模拟的连续函数。\n<table border=\"1\" align=\"center\"><span style=\"font-size:24px;color:#006600;\"></span>  \n<caption align=\"top\">函数图像</caption>  \n<tr><td>图一<img src=\"http://7xjz3b.com1.z0.glb.clouddn.com/juanji1.jpg\" width = \"300\" height = \"200\" alt=\"\" align=center /></td> <td>图2<img src=\"http://7xjz3b.com1.z0.glb.clouddn.com/figure_3.png\" width = \"300\" height = \"200\" alt=\"\" align=center /></td> </tr>\n<tr><td>图3<img src=\"http://7xjz3b.com1.z0.glb.clouddn.com/figure_1.png\" width = \"300\" height = \"200\" alt=\"\" align=center /></td> <td>图4<img src=\"http://7xjz3b.com1.z0.glb.clouddn.com/figure_2.png\" width = \"300\" height = \"200\" alt=\"\" align=center /></td> </tr>\n</table>  \n\n##解释：\n对卷积的理解很多，比较有直观的有[打拳](http://www.zhihu.com/question/22298352)、[向水中投石](http://www.zhihu.com/question/20500497)这两个例子。这些例子有几个共同的特点：\n- 都有一个单位刺激的响应函数。如打一拳引起的疼痛，投一个石头引起的水花；\n- 若力度相同（或是石头重量一致），原函数都是$f(x)=1$。把时间推移理解为横轴x；如果拳头的力量或是石头的大小有改变，则原函数$f(x)$变为图4中有波动的原函数。\n- 例子中每个时间点观察的结果都是前面所有时间单位响应到目前结果的叠加，与卷积运算得出新函数的意义相同。\n- 都是以离散点举例，最后想象无限多个点，进而演变成连续的解释。\n\n结合函数图像来理解，离散的情况如图1，连续情况如图2,3,4。在图2中，g(x)单位高斯函数，在原函数f(x)的上的卷积运算，用引用的帖子中总结概括：**各个时间点上的单位响应的加权叠加**。这里的加权，就是指的原函数f(x)中的函数值。\n\n##我的理解：\n- 理解x轴为时间尺度，如果原函数是$f(x)$，响应函数$g(x)$是（0，1）这个点，那么卷积计算结果$h(x) = g(x) \\otimes f(x)$的卷积结果是原函数$f(x)$。这是因为响应函数只存在于一点，所以，不会对后面的时间有影响，所以不存在随x轴的叠加————证明了叠加性（图2）\n- 如果原函数f(x)是一个常数函数，那么无论单位响应函数形式如何，最后的卷积结果h(x)一定会趋向不变。这是因为原函数值不变，所以权值相同，时间尺度上的叠加都相同，所以函数值稳定后会不变———证明了加权（图3）。\n\n总结一下: **卷积运算可以理解成以原函数$f(x)$为尺度，以响应函数$g(x)$为单位的，随时间尺度的叠加。**\n\n\n##图像处理的卷积：\n在图像中，卷积的使用方式主要是卷积模板的形式。原始的图像可以看做二维数字信号，则可以看做原始图像每个像素与固定长宽的卷积模板做卷积运算。\n$$\nG(x,y) \\otimes I(x,y) = \\sum\\_{u=-M}^M \\sum\\_{v=-N}^N G(u,v)I(x-u,y-v)\n$$\n其中$I(x,y)$代表图像中每一个像素，$G(u,v)$表示尺寸为(2M+1)*(2N+1)的模板\n![](http://7xjz3b.com1.z0.glb.clouddn.com/juanji5.jpg)\n这些卷积模板是对连续卷积核函数的数字采样近似，相当于上面用离散点近似连续卷积函数。\n不同的模板决定了卷积的不同功能：去噪，平滑，匹配等。\n\n##End.\n\n国内的理解与言论貌似只有知乎值得一看，想得到一些更直观的理解时，这里的看法往往独到且深入。而像百度百科、各种博客等大众性解释确误导性严重，看来普及性和质量往往难以共存。\n\n\n\n\n","slug":"卷积的理解","published":1,"updated":"2015-11-19T11:37:41.000Z","_id":"cih5p32wo001hasbgnesakks1","comments":1,"layout":"post","photos":[],"link":"","sticky":0},{"title":"scikt-learn示例解析 色彩量化Color Quantization using K-Means","date":"2015-10-21T12:04:03.000Z","_content":"翻译并解释了scikt-learn官网示例，留作备忘。\n#Scikt-learn Example：Color Quantization using K-Means \n通过k-Means进行图像的色彩量化\nurl： http://scikit-learn.org/stable/auto_examples/cluster/plot_color_quantization.html\n##简介\n**色彩量化**是数据压缩的一个有效手段。这个示例提供了一个基于像素的色彩量化例子，将一副图片从96615个独立颜色降低到64色，大幅减小了图像大小，同时保留图片的基本外观。\n示例通过聚类方法K-Means，从每一点的像素值中选取64个聚类中心，然后将所有点按照距离度量最接近的像素值分配，从而达到色彩量化的目的。在实际操作中，图像是以三维数组存储的（w，h，d），每一个RGB像素也需要一个长度为3的数组来表示（红绿蓝）。除此之外，示例使用了另一个方法random codebook，即随机挑选像素值作为聚类中心，然后再将所有像素分类，对比实验。\n- 以下是示例的结果：我们可以发现，96615色降到64色后图片外观基本可以保持不变，而图3由于是随机生成的聚类中心，所以色彩有变化。\n<table border=\"1\" align=\"center\"><span style=\"font-size:24px;color:#006600;\"></span>  \n<caption align=\"top\">示例结果</caption>  \n <tr><td><img src=\"http://7xjz3b.com1.z0.glb.clouddn.com/plot_color_quantization_0011.png\" width = \"400\" height = \"300\" alt=\"\" align=center /></td>  <td></td>  </tr>  \n<tr><td><img src=\"http://7xjz3b.com1.z0.glb.clouddn.com/plot_color_quantization_002.png\" width = \"400\" height = \"300\" alt=\"\" align=center /></td> <td><img src=\"http://7xjz3b.com1.z0.glb.clouddn.com/plot_color_quantization_003.png\" width = \"400\" height = \"300\" alt=\"\" align=center /></td>  </tr>  \n</table>  \n\n##代码分析\n对源代码分段解释一下，其实英文文档的注释已经比较全面，只是有些细节需要补充。\n```python\n# Authors: Robert Layton <robertlayton@gmail.com>\n#          Olivier Grisel <olivier.grisel@ensta.org>\n#          Mathieu Blondel <mathieu@mblondel.org>\n#\n# License: BSD 3 clause\n\nprint(__doc__)\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.cluster import KMeans\nfrom sklearn.metrics import pairwise_distances_argmin\nfrom sklearn.datasets import load_sample_image\nfrom sklearn.utils import shuffle\nfrom time import time\n```\n- 几个包的引用，对矩阵处理肯定需要numpy，然后最后绘图的步骤需要matplotlib，time包是用来统计每一步的执行时间的。\n- 然后是引用scikt-learn相关组组件，KMeans是实现聚类的方法，pairwise_distances_argmin是一个计算相似度的方法，后面细说。load_sample_image是读取sklearn内置图片的方法，shuffle是一个随机排列数组的方法。\n\n```python\nn_colors = 64\n# Load the Summer Palace photo\nchina = load_sample_image(\"china.jpg\")\n# Convert to floats instead of the default 8 bits integer coding. Dividing by\n# 255 is important so that plt.imshow behaves works well on float data (need to\n# be in the range [0-1]\nchina = np.array(china, dtype=np.float64) / 255\n# Load Image and transform to a 2D numpy array.\nw, h, d = original_shape = tuple(china.shape)\nassert d == 3\nimage_array = np.reshape(china, (w * h, d))\nprint(\"Fitting model on a small sub-sample of the data\")\nt0 = time()\nimage_array_sample = shuffle(image_array, random_state=0)[:1000]\nkmeans = KMeans(n_clusters=n_colors, random_state=0).fit(image_array_sample)\nprint(\"done in %0.3fs.\" % (time() - t0))\n# Get labels for all points\nprint(\"Predicting color indices on the full image (k-means)\")\nt0 = time()\nlabels = kmeans.predict(image_array)\nprint(\"done in %0.3fs.\" % (time() - t0))\n```\n英文注释已经比较详细了，这里只补充一下要点。\n- 原始图像读出的三维数组是int8类型的，红绿蓝各为0-255，这里用numpy转成了float64类型并/255是为了后面plt画图包的显示。\n- 中间reshape了一下原始数组，从3维转成2维，是为了方便后面的聚类和分类，实际像素的排列是没有变化的。\n- 这里让K-means模型学习的数据并不是原始图像，而是从原始图像**随机选取**了1000个像素的颜色值，这里就用到刚才提到的shuffle方法，具体功能可以点[源码](http://scikit-learn.org/stable/modules/generated/sklearn.utils.shuffle.html#sklearn.utils.shuffle)，这样可以大大减少计算时间，并得到几乎一样的结果。\n- 然后就是根据聚类中心对原始图像每一个像素分类，用所属聚类中心的颜色代替。**有一点注意**，K-means方法在预测时，由于要计算欧式距离，sklearn的实现十分消耗内存，对于大一点的图像就会出现MemoryError的问题，解决方法可以用类似的MiniBatchKmeans方法代替K-Means。\n\n```python\ncodebook_random = shuffle(image_array, random_state=0)[:n_colors + 1]\nprint(\"Predicting color indices on the full image (random)\")\nt0 = time()\nlabels_random = pairwise_distances_argmin(codebook_random,image_array,axis=0)\nprint(\"done in %0.3fs.\" % (time() - t0))\n```\n这里实现的是random-codebook对比方法。\n- 这里codebook-random的作用相当于K-means模型中生成的聚类中心，只不过这里时完全随机罢了。\n- 然后后面用到了pairwise_distances_argmin方法，作用相当于上面means.predict(image_array)，就是将原始图像每一个像素的颜色归属到距离大最近的codebook中的颜色。具体功能可以看[源码](http://scikit-learn.org/stable/modules/generated/sklearn.metrics.pairwise_distances_argmin.html#sklearn.metrics.pairwise_distances_argmin).\n\n```python\ndef recreate_image(codebook, labels, w, h):\n    \"\"\"Recreate the (compressed) image from the code book & labels\"\"\"\n    d = codebook.shape[1]\n    image = np.zeros((w, h, d))\n    label_idx = 0\n    for i in range(w):\n        for j in range(h):\n            image[i][j] = codebook[labels[label_idx]]\n            label_idx += 1\n    return image\n```\n这里实现了一个画图方法。返回的是一个三维数组，然后通过后面的matplotlib包实现画图。\n- 具体很简单，就是用numpy生成一个原始图像大小的0矩阵，然后按照每个聚类中心的颜色codebook，和原始图像每一个像素点归属的聚类中心labels，重新生成描述图像的三维数组。\n- 这里完成了2D到3D的转变，也就是从分类标签到图像数组的转换，又回到了原始的三维数组。\n\n```python\n# Display all results, alongside original image\nplt.figure(1)\nplt.clf()\nax = plt.axes([0, 0, 1, 1])\nplt.axis('off')\nplt.title('Original image (96,615 colors)')\nplt.imshow(china)\n\nplt.figure(2)\nplt.clf()\nax = plt.axes([0, 0, 1, 1])\nplt.axis('off')\nplt.title('Quantized image (64 colors, K-Means)')\nplt.imshow(recreate_image(kmeans.cluster_centers_, labels, w, h))\n\nplt.figure(3)\nplt.clf()\nax = plt.axes([0, 0, 1, 1])\nplt.axis('off')\nplt.title('Quantized image (64 colors, Random)')\nplt.imshow(recreate_image(codebook_random, labels_random, w, h))\nplt.show()\n```\n最后就是通过matpolt画出原始图和两张处理后的图，都是基本方法，可以自己了解matplotlib包，不再详细记录。\n\n###End\n\n-Robin \n2015.10.21 夜\n<!--end-->","source":"_posts/scikt-learn示例解析 色彩量化Color Quantization using K-Means.md","raw":"title: scikt-learn示例解析 色彩量化Color Quantization using K-Means\ndate: 2015-10-21 20:04:03\ntags: \n- sklearn\n- 机器学习\n- kmeans\ncategories: \n- 技术\n\n---\n翻译并解释了scikt-learn官网示例，留作备忘。\n#Scikt-learn Example：Color Quantization using K-Means \n通过k-Means进行图像的色彩量化\nurl： http://scikit-learn.org/stable/auto_examples/cluster/plot_color_quantization.html\n##简介\n**色彩量化**是数据压缩的一个有效手段。这个示例提供了一个基于像素的色彩量化例子，将一副图片从96615个独立颜色降低到64色，大幅减小了图像大小，同时保留图片的基本外观。\n示例通过聚类方法K-Means，从每一点的像素值中选取64个聚类中心，然后将所有点按照距离度量最接近的像素值分配，从而达到色彩量化的目的。在实际操作中，图像是以三维数组存储的（w，h，d），每一个RGB像素也需要一个长度为3的数组来表示（红绿蓝）。除此之外，示例使用了另一个方法random codebook，即随机挑选像素值作为聚类中心，然后再将所有像素分类，对比实验。\n- 以下是示例的结果：我们可以发现，96615色降到64色后图片外观基本可以保持不变，而图3由于是随机生成的聚类中心，所以色彩有变化。\n<table border=\"1\" align=\"center\"><span style=\"font-size:24px;color:#006600;\"></span>  \n<caption align=\"top\">示例结果</caption>  \n <tr><td><img src=\"http://7xjz3b.com1.z0.glb.clouddn.com/plot_color_quantization_0011.png\" width = \"400\" height = \"300\" alt=\"\" align=center /></td>  <td></td>  </tr>  \n<tr><td><img src=\"http://7xjz3b.com1.z0.glb.clouddn.com/plot_color_quantization_002.png\" width = \"400\" height = \"300\" alt=\"\" align=center /></td> <td><img src=\"http://7xjz3b.com1.z0.glb.clouddn.com/plot_color_quantization_003.png\" width = \"400\" height = \"300\" alt=\"\" align=center /></td>  </tr>  \n</table>  \n\n##代码分析\n对源代码分段解释一下，其实英文文档的注释已经比较全面，只是有些细节需要补充。\n```python\n# Authors: Robert Layton <robertlayton@gmail.com>\n#          Olivier Grisel <olivier.grisel@ensta.org>\n#          Mathieu Blondel <mathieu@mblondel.org>\n#\n# License: BSD 3 clause\n\nprint(__doc__)\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.cluster import KMeans\nfrom sklearn.metrics import pairwise_distances_argmin\nfrom sklearn.datasets import load_sample_image\nfrom sklearn.utils import shuffle\nfrom time import time\n```\n- 几个包的引用，对矩阵处理肯定需要numpy，然后最后绘图的步骤需要matplotlib，time包是用来统计每一步的执行时间的。\n- 然后是引用scikt-learn相关组组件，KMeans是实现聚类的方法，pairwise_distances_argmin是一个计算相似度的方法，后面细说。load_sample_image是读取sklearn内置图片的方法，shuffle是一个随机排列数组的方法。\n\n```python\nn_colors = 64\n# Load the Summer Palace photo\nchina = load_sample_image(\"china.jpg\")\n# Convert to floats instead of the default 8 bits integer coding. Dividing by\n# 255 is important so that plt.imshow behaves works well on float data (need to\n# be in the range [0-1]\nchina = np.array(china, dtype=np.float64) / 255\n# Load Image and transform to a 2D numpy array.\nw, h, d = original_shape = tuple(china.shape)\nassert d == 3\nimage_array = np.reshape(china, (w * h, d))\nprint(\"Fitting model on a small sub-sample of the data\")\nt0 = time()\nimage_array_sample = shuffle(image_array, random_state=0)[:1000]\nkmeans = KMeans(n_clusters=n_colors, random_state=0).fit(image_array_sample)\nprint(\"done in %0.3fs.\" % (time() - t0))\n# Get labels for all points\nprint(\"Predicting color indices on the full image (k-means)\")\nt0 = time()\nlabels = kmeans.predict(image_array)\nprint(\"done in %0.3fs.\" % (time() - t0))\n```\n英文注释已经比较详细了，这里只补充一下要点。\n- 原始图像读出的三维数组是int8类型的，红绿蓝各为0-255，这里用numpy转成了float64类型并/255是为了后面plt画图包的显示。\n- 中间reshape了一下原始数组，从3维转成2维，是为了方便后面的聚类和分类，实际像素的排列是没有变化的。\n- 这里让K-means模型学习的数据并不是原始图像，而是从原始图像**随机选取**了1000个像素的颜色值，这里就用到刚才提到的shuffle方法，具体功能可以点[源码](http://scikit-learn.org/stable/modules/generated/sklearn.utils.shuffle.html#sklearn.utils.shuffle)，这样可以大大减少计算时间，并得到几乎一样的结果。\n- 然后就是根据聚类中心对原始图像每一个像素分类，用所属聚类中心的颜色代替。**有一点注意**，K-means方法在预测时，由于要计算欧式距离，sklearn的实现十分消耗内存，对于大一点的图像就会出现MemoryError的问题，解决方法可以用类似的MiniBatchKmeans方法代替K-Means。\n\n```python\ncodebook_random = shuffle(image_array, random_state=0)[:n_colors + 1]\nprint(\"Predicting color indices on the full image (random)\")\nt0 = time()\nlabels_random = pairwise_distances_argmin(codebook_random,image_array,axis=0)\nprint(\"done in %0.3fs.\" % (time() - t0))\n```\n这里实现的是random-codebook对比方法。\n- 这里codebook-random的作用相当于K-means模型中生成的聚类中心，只不过这里时完全随机罢了。\n- 然后后面用到了pairwise_distances_argmin方法，作用相当于上面means.predict(image_array)，就是将原始图像每一个像素的颜色归属到距离大最近的codebook中的颜色。具体功能可以看[源码](http://scikit-learn.org/stable/modules/generated/sklearn.metrics.pairwise_distances_argmin.html#sklearn.metrics.pairwise_distances_argmin).\n\n```python\ndef recreate_image(codebook, labels, w, h):\n    \"\"\"Recreate the (compressed) image from the code book & labels\"\"\"\n    d = codebook.shape[1]\n    image = np.zeros((w, h, d))\n    label_idx = 0\n    for i in range(w):\n        for j in range(h):\n            image[i][j] = codebook[labels[label_idx]]\n            label_idx += 1\n    return image\n```\n这里实现了一个画图方法。返回的是一个三维数组，然后通过后面的matplotlib包实现画图。\n- 具体很简单，就是用numpy生成一个原始图像大小的0矩阵，然后按照每个聚类中心的颜色codebook，和原始图像每一个像素点归属的聚类中心labels，重新生成描述图像的三维数组。\n- 这里完成了2D到3D的转变，也就是从分类标签到图像数组的转换，又回到了原始的三维数组。\n\n```python\n# Display all results, alongside original image\nplt.figure(1)\nplt.clf()\nax = plt.axes([0, 0, 1, 1])\nplt.axis('off')\nplt.title('Original image (96,615 colors)')\nplt.imshow(china)\n\nplt.figure(2)\nplt.clf()\nax = plt.axes([0, 0, 1, 1])\nplt.axis('off')\nplt.title('Quantized image (64 colors, K-Means)')\nplt.imshow(recreate_image(kmeans.cluster_centers_, labels, w, h))\n\nplt.figure(3)\nplt.clf()\nax = plt.axes([0, 0, 1, 1])\nplt.axis('off')\nplt.title('Quantized image (64 colors, Random)')\nplt.imshow(recreate_image(codebook_random, labels_random, w, h))\nplt.show()\n```\n最后就是通过matpolt画出原始图和两张处理后的图，都是基本方法，可以自己了解matplotlib包，不再详细记录。\n\n###End\n\n-Robin \n2015.10.21 夜\n<!--end-->","slug":"scikt-learn示例解析 色彩量化Color Quantization using K-Means","published":1,"updated":"2015-11-19T03:50:55.000Z","_id":"cih5p32wq001oasbgbuiad7k9","comments":1,"layout":"post","photos":[],"link":"","sticky":0}],"PostAsset":[],"PostCategory":[{"post_id":"cih5p32wc000fasbg3jpafsgg","category_id":"cih5p32ta0001asbgolg84eqq","_id":"cih5p32wd000gasbg2x9f97wb"},{"post_id":"cih5p32wf000qasbg5lzvcnj4","category_id":"cih5p32ta0001asbgolg84eqq","_id":"cih5p32wg000rasbgi1829pnl"},{"post_id":"cih5p32wh0010asbgazk6zdpz","category_id":"cih5p32wi0011asbgtl15nheq","_id":"cih5p32wj0014asbghkhhsram"},{"post_id":"cih5p32wk0015asbg0uz30vni","category_id":"cih5p32ta0001asbgolg84eqq","_id":"cih5p32wl0016asbg1zga1rd9"},{"post_id":"cih5p32wm001basbgxu1yt0lh","category_id":"cih5p32ta0001asbgolg84eqq","_id":"cih5p32wn001casbgh4syij7v"},{"post_id":"cih5p32wq001oasbgbuiad7k9","category_id":"cih5p32ta0001asbgolg84eqq","_id":"cih5pab760000ekbg2jixigk5"},{"post_id":"cih5p32wo001hasbgnesakks1","category_id":"cih5p32wp001iasbgsmynufkp","_id":"cih65ykuq0000ivbgfp5g59qn"}],"PostTag":[{"post_id":"cih5p32wc000fasbg3jpafsgg","tag_id":"cih5p32wd000hasbgus9jqzcw","_id":"cih5p32we000lasbg5o2bhlnl"},{"post_id":"cih5p32wc000fasbg3jpafsgg","tag_id":"cih5p32we000iasbg7tyrzd8b","_id":"cih5p32we000masbg09cgbml9"},{"post_id":"cih5p32wc000fasbg3jpafsgg","tag_id":"cih5p32we000jasbgtrjch8vu","_id":"cih5p32we000nasbgzlyiqx4p"},{"post_id":"cih5p32wc000fasbg3jpafsgg","tag_id":"cih5p32tc0003asbg55lo776r","_id":"cih5p32we000oasbg9cu59bpm"},{"post_id":"cih5p32wc000fasbg3jpafsgg","tag_id":"cih5p32we000kasbgv4g50ce1","_id":"cih5p32we000pasbgdarr0as1"},{"post_id":"cih5p32wf000qasbg5lzvcnj4","tag_id":"cih5p32wg000sasbgkcdi6tau","_id":"cih5p32wg000wasbgqeoub8zp"},{"post_id":"cih5p32wf000qasbg5lzvcnj4","tag_id":"cih5p32wg000tasbg4prqc55x","_id":"cih5p32wh000xasbg0fzt3j0x"},{"post_id":"cih5p32wf000qasbg5lzvcnj4","tag_id":"cih5p32wg000uasbgbs2livsp","_id":"cih5p32wh000yasbg4eimb93m"},{"post_id":"cih5p32wf000qasbg5lzvcnj4","tag_id":"cih5p32wg000vasbgt97mftfw","_id":"cih5p32wh000zasbg03mumgqf"},{"post_id":"cih5p32wh0010asbgazk6zdpz","tag_id":"cih5p32wj0012asbg64dqhlf9","_id":"cih5p32wj0013asbg1cjhftx6"},{"post_id":"cih5p32wk0015asbg0uz30vni","tag_id":"cih5p32wl0017asbgqmkpveu3","_id":"cih5p32wl0018asbgt9z8hadw"},{"post_id":"cih5p32wk0015asbg0uz30vni","tag_id":"cih5p32we000jasbgtrjch8vu","_id":"cih5p32wl0019asbgka5maqvd"},{"post_id":"cih5p32wk0015asbg0uz30vni","tag_id":"cih5p32tc0003asbg55lo776r","_id":"cih5p32wl001aasbgp9cdlmz8"},{"post_id":"cih5p32wm001basbgxu1yt0lh","tag_id":"cih5p32wn001dasbgntdemszs","_id":"cih5p32wn001fasbg32qsv1nc"},{"post_id":"cih5p32wm001basbgxu1yt0lh","tag_id":"cih5p32wn001easbg7xgm0tvj","_id":"cih5p32wn001gasbg04zt83yp"},{"post_id":"cih5p32wo001hasbgnesakks1","tag_id":"cih5p32wp001jasbgyqbl5bol","_id":"cih5p32wp001masbgu7dsok6k"},{"post_id":"cih5p32wo001hasbgnesakks1","tag_id":"cih5p32wp001kasbgf5724mg5","_id":"cih5p32wp001nasbge2fppnze"},{"post_id":"cih5p32wq001oasbgbuiad7k9","tag_id":"cih5p32tb0002asbgtkvihgc8","_id":"cih5p32wr001qasbgt0gamy2i"},{"post_id":"cih5p32wq001oasbgbuiad7k9","tag_id":"cih5p32tc0003asbg55lo776r","_id":"cih5p32ws001rasbgc412863f"},{"post_id":"cih5p32wq001oasbgbuiad7k9","tag_id":"cih5p32tc0005asbg214mx3h0","_id":"cih5p32ws001sasbg4uoewx19"}],"Tag":[{"name":"sklearn","_id":"cih5p32tb0002asbgtkvihgc8"},{"name":"机器学习","_id":"cih5p32tc0003asbg55lo776r"},{"name":"kmeans","_id":"cih5p32tc0005asbg214mx3h0"},{"name":"逻辑回归","_id":"cih5p32wd000hasbgus9jqzcw"},{"name":"判别模型","_id":"cih5p32we000iasbg7tyrzd8b"},{"name":"分类","_id":"cih5p32we000jasbgtrjch8vu"},{"name":"线性代数","_id":"cih5p32we000kasbgv4g50ce1"},{"name":"hexo","_id":"cih5p32wg000sasbgkcdi6tau"},{"name":"Gitpages","_id":"cih5p32wg000tasbg4prqc55x"},{"name":"GitCafe","_id":"cih5p32wg000uasbgbs2livsp"},{"name":"博客迁移","_id":"cih5p32wg000vasbgt97mftfw"},{"name":"纪念","_id":"cih5p32wj0012asbg64dqhlf9"},{"name":"贝叶斯","_id":"cih5p32wl0017asbgqmkpveu3"},{"name":"linux","_id":"cih5p32wn001dasbgntdemszs"},{"name":"环境变量","_id":"cih5p32wn001easbg7xgm0tvj"},{"name":"卷积","_id":"cih5p32wp001jasbgyqbl5bol"},{"name":"图像处理","_id":"cih5p32wp001kasbgf5724mg5"}]}}