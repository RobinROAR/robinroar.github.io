<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  
  <title><![CDATA[Lion's Pride Inn   - Robin's blog]]></title>
  
  <link href="/atom.xml" rel="self"/>
  <link href="http://robinzheng.com//"/>
  <updated>2016-01-16T07:12:49.000Z</updated>
  <id>http://robinzheng.com//</id>
  
  <author>
    <name><![CDATA[Robin Zheng]]></name>
    <email><![CDATA[zrb915@live.com]]></email>
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title><![CDATA[从AdaBoost谈谈基于权值的投票方法]]></title>
    <link href="http://robinzheng.com/2015/12/29/%E4%BB%8EAdaboost%E8%B0%88%E8%B0%88%E5%87%A0%E7%A7%8D%E4%BD%BF%E7%94%A8%E6%9D%83%E5%80%BC%E7%9A%84%E6%96%B9%E6%B3%95/"/>
    <id>http://robinzheng.com/2015/12/29/从Adaboost谈谈几种使用权值的方法/</id>
    <published>2015-12-29T07:51:34.000Z</published>
    <updated>2016-01-16T07:12:49.000Z</updated>
    <content type="html"><![CDATA[<p>这周计算机视觉课在讲运动分析时重温了AdaBoost方法，感觉比之前的理解又深入了些，在这里谈谈体会。</p>
<h2 id="AdaBoost">AdaBoost</h2><h3 id="Boosting思想">Boosting思想</h3><p>通过融合多个弱分类器，用加权投票的方式构建一个准确率更高的强分类器。</p>
<ul>
<li>弱分类器：准确率大于0.5,即仅比随机猜测略好</li>
<li>强分类器：准确率高，并能在多项式时间（计算时间$m(n)=O(n^k)$,k为常数）内解决问题。</li>
</ul>
<h3 id="理论依据">理论依据</h3><ul>
<li><a href="http://www.sciencedirect.com/science/article/pii/0890540189900023" target="_blank" rel="external">A general lower bound on the number of examples needed for learning[J]. Information and Computation, 1989</a><br>M Kearns and L Valiant数学上证明了Boosting弱方法强化思想</li>
<li><a href="http://www.public.asu.edu/~jye02/CLASSES/Fall-2005/PAPERS/boosting-icml.pdf" target="_blank" rel="external"> Experiments with a new boosting algorithm[C]//ICML. 1996</a><br>Freund and Schapire 提出AdaBoost方法</li>
<li><a href="http://www.merl.com/papers/docs/tr2004-043.pdf" target="_blank" rel="external">Viola P, Jones M. Rapid object detection using a boosted cascade of simple features[C]//Computer Vision and Pattern Recognition, 2001. CVPR 2001</a><br>Viola and Jones  在人脸检测中应用Adaboost，引起轰动</li>
</ul>
<h3 id="核心">核心</h3><ol>
<li>对<strong>样本加权</strong>：每个样本赋予权重，某次迭代中没有被正确分类的样本权重提高</li>
<li>对<strong>每个弱分类器加权</strong>：若某一分类准确率高，其权重也越高。</li>
<li>根据规则迭代，直到收敛。</li>
</ol>
<h3 id="算法">算法</h3><p>假设输入m个样本$(x_1,y_1),…,(x_m,y_m),其中x_i\in{X}, y_i={1,-1}$</p>
<h4 id="算法-1">算法</h4><ol>
<li>初始化样本权重： $D_1(i)=\frac{1}{m}$</li>
<li>开始循环：For $t$=1 to T:<ol>
<li>训练一个弱分类器</li>
<li>循环判断。如果得出的弱分类器分类误差 /&lt;1/2或 准确率 = 1,中止循环</li>
<li>计算分类器权重</li>
<li>更新样本权重</li>
</ol>
</li>
<li>构建基本分类器的线性组合，得到最终的强分类器。</li>
</ol>
<h4 id="重点">重点</h4><h5 id="1-_训练（挑选）弱分类器">1. 训练（挑选）弱分类器</h5><h5 id="2-_计算分类器权重">2. 计算分类器权重</h5><h5 id="3-_更新样本权重">3. 更新样本权重</h5><h4 id="总结：">总结：</h4><ol>
<li>每次迭代是在当前样本下训练一个新的分类器，这些分类器<strong>是由一种分类模型根据加权的样本数据学习出的结果</strong>，是独立且固定的，并计算出一个固定的分类器权重，不会随着过程而更改。</li>
<li>每次迭代后样本的权重也会有变化，并重归一化到0-1区间，同时所有样本权重加和值为1。</li>
</ol>
<h2 id="谈谈其它基于权值的投票方法">谈谈其它基于权值的投票方法</h2><p>在上课的过程中想到，这种Adaboost的提升方法朝向似曾相识，和前面背景提取时构建的混合高斯模型，以及神经网络等都有相似之处，这里简单比较一下。</p>
<h2 id="未写完">未写完</h2>]]></content>
    <summary type="html">
    <![CDATA[<p>这周计算机视觉课在讲运动分析时重温了AdaBoost方法，感觉比之前的理解又深入了些，在这里谈谈体会。</p>
<h2 id="AdaBoost">AdaBoost</h2><h3 id="Boosting思想">Boosting思想</h3><p>通过融合多个弱分类器，用]]>
    </summary>
    
      <category term="AdaBoost" scheme="http://robinzheng.com/tags/AdaBoost/"/>
    
      <category term="分类" scheme="http://robinzheng.com/tags/%E5%88%86%E7%B1%BB/"/>
    
      <category term="机器学习" scheme="http://robinzheng.com/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="理论" scheme="http://robinzheng.com/categories/%E7%90%86%E8%AE%BA/"/>
    
  </entry>
  
  <entry>
    <title><![CDATA[从相机投影模型谈谈矩阵组成]]></title>
    <link href="http://robinzheng.com/2015/12/11/%E4%BB%8E%E7%9B%B8%E6%9C%BA%E6%8A%95%E5%BD%B1%E6%A8%A1%E5%9E%8B%E8%B0%88%E8%B0%88%E7%9F%A9%E9%98%B5%E7%BB%84%E6%88%90/"/>
    <id>http://robinzheng.com/2015/12/11/从相机投影模型谈谈矩阵组成/</id>
    <published>2015-12-11T06:56:07.000Z</published>
    <updated>2016-01-19T06:13:20.000Z</updated>
    <content type="html"><![CDATA[<p>部分插图来自自动化所CV讲义课件。</p>
<h2 id="1-_引子">1. 引子</h2><p>这里主要从相机成像的过程入手，解析相机将<strong>三维世界中的真实物体转换成照片底片上的二维图像</strong>的具体过程，重点是坐标系的转换，同时对投影矩阵进行拆解，以更好的理解矩阵的意义。</p>
<h3 id="依赖知识">依赖知识</h3><p>这些和主题并无太大关系，但有必要稍加解释和记录。</p>
<h4 id="齐次坐标：">齐次坐标：</h4><p>齐次坐标是在欧式坐标后面加上泛化的常数项，是一种坐标定义形式。主要为了在射影空间的相关应用，以及简化矩阵计算。通常在坐标转换中我们都是用齐次坐标。详细的介绍可以参见<a href="http://robinzheng.com/2015/11/27/%E5%87%A0%E7%A7%8D%E4%BA%8C%E7%BB%B4%E5%8F%98%E6%8D%A2%E5%92%8C%E5%AF%B9%E5%BA%94%E7%9A%84%E7%9F%A9%E9%98%B5%E8%A1%A8%E7%A4%BA/">这里</a></p>
<h4 id="相机的内参数">相机的内参数</h4><p>每台相机虽然都标注了焦距等信息，但这些信息是一个大概值，并不准确。每台相机很多自己的具体属性，主要包括<strong>光心位置，焦距，单位像素长宽，畸变值</strong>，我们称之为内参数，构成的矩阵叫<strong>内参数矩阵</strong>,这些值虽然微小，但也可以直接影响投影的结果</p>
<ul>
<li>焦距f：焦距指的是焦点到光心的距离。</li>
<li>畸变值s:</li>
<li>光心位置：</li>
<li>单位像素长宽：<h4 id="其他相机属性：">其他相机属性：</h4></li>
<li>景深：</li>
<li>视角：与焦距f和胶片尺寸d有关。公式：<h4 id="小孔成像模型：">小孔成像模型：</h4>小孔成像原理小时的自然课就曾接触过，这里主要讲一下</li>
</ul>
<h2 id="2-_相机投影模型">2. 相机投影模型</h2><h3 id="相机几何">相机几何</h3><h3 id="未写完">未写完</h3>]]></content>
    <summary type="html">
    <![CDATA[<p>部分插图来自自动化所CV讲义课件。</p>
<h2 id="1-_引子">1. 引子</h2><p>这里主要从相机成像的过程入手，解析相机将<strong>三维世界中的真实物体转换成照片底片上的二维图像</strong>的具体过程，重点是坐标系的转换，同时对投影矩阵进行拆解]]>
    </summary>
    
      <category term="矩阵" scheme="http://robinzheng.com/tags/%E7%9F%A9%E9%98%B5/"/>
    
      <category term="计算机视觉" scheme="http://robinzheng.com/tags/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89/"/>
    
      <category term="理论" scheme="http://robinzheng.com/categories/%E7%90%86%E8%AE%BA/"/>
    
  </entry>
  
  <entry>
    <title><![CDATA[总结几种二维变换和对应的矩阵表示]]></title>
    <link href="http://robinzheng.com/2015/11/27/%E5%87%A0%E7%A7%8D%E4%BA%8C%E7%BB%B4%E5%8F%98%E6%8D%A2%E5%92%8C%E5%AF%B9%E5%BA%94%E7%9A%84%E7%9F%A9%E9%98%B5%E8%A1%A8%E7%A4%BA/"/>
    <id>http://robinzheng.com/2015/11/27/几种二维变换和对应的矩阵表示/</id>
    <published>2015-11-27T11:53:14.000Z</published>
    <updated>2015-12-24T09:00:36.000Z</updated>
    <content type="html"><![CDATA[<p><strong>矩阵其实代表一种线性变换</strong>，是理解线性代数的核心之一。而线性变换有着各种各样的用途，所以矩阵这种人造产物又有了许多新的意义。<br>不同的矩阵代表不同变换，许多变换都有统一的特征，可以归为一类；若把矩阵看成多个列向量的罗列，则更容易理解。这里简单总结几种常见的二维变换，及对应的矩阵方程。</p>
<h2 id="齐次坐标">齐次坐标</h2><p>将各种的变换表达成矩阵的形式，需要引入<strong>齐次坐标 Homogeneous coordinates</strong>的概念。</p>
<ul>
<li>构建齐次坐标：在普通坐标中增加一个泛化的常数项，一般为1。 
$$
二维：(x,y)\Rightarrow\left(\begin{array}{c}
x \\
y \\
1
\end{array}
\right),
\quad 三维： (x,y,z) \Rightarrow \left(\begin{array}{c}
x\\
y\\
z\\1
\end{array}\right)
$$
- 转换成欧式坐标：
$$二维：
\left(\begin{array}{c}x\\y\\w
\end{array}\right) \Rightarrow (\frac{x}{w},\frac{y}{w})，
\quad 三维：
\left(\begin{array}{c}x\\y\\z\\w
\end{array}\right) \Rightarrow (\frac{x}{w},\frac{y}{w},\frac{z}{w})
$$
</li>
<li>意义主要有两个方面: </li>
</ul>
<ol>
<li>一个是在射影空间中代表无穷远。射影空间相较于欧式空间引入了无穷远的概念，即无穷远点，无穷远直线等，这些<strong>无穷远点的齐次坐标最后一位是0,而有限点最后一位是1</strong>，这些是射影几何中的概念，最后有个例子，这里不详细讨论。</li>
<li>另一点就是方便运算，通过引入齐次坐标可以简单的将<a href="#add">加法</a>和<a href="dev">除法</a>通过矩阵相乘的方式表示，后面的例子会有印证。</li>
</ol>
<hr>
<h2 id="几种二维变换">几种二维变换</h2><ul>
<li>简述：<ul>
<li>随范围的递增，主要有四种变换类型：欧式变换群，相似变换群，仿射变换群，射影变换群。</li>
<li>这些变换都是二维坐标的变换，是一种一矩阵形式体现的广义线性变换。</li>
<li>这些变换基本上由几种原子变换组成：平移，旋转，伸缩</li>
</ul>
</li>
<li><text id="add">几种原子变换<text>：<ul>
<li>平移 Translation<br><img src="http://7xjz3b.com1.z0.glb.clouddn.com/translate.jpg" alt="">
$$坐标方程： 
\begin{align*}
x'= x+t_x\\
y'=y+y_x
\end{align*}
，
\quad 转换矩阵:
\begin{bmatrix}
x'\\y'\\1
\end{bmatrix}=
\begin{bmatrix}1&0&t_x\\0&1&t_y\\0&0&1\end{bmatrix}
\begin{bmatrix}x\\y\\1\end{bmatrix}
$$

这里体现了齐次坐标<strong>方便计算加减法的性质。</strong></li>
<li>伸缩 scale<br><img src="http://7xjz3b.com1.z0.glb.clouddn.com/scale.jpg" alt="">
$$坐标方程： 
\begin{align*}
x'= sx\\
y'=sy
\end{align*}
,
\quad\quad 转换矩阵:
\begin{bmatrix}
x'\\y'\\1
\end{bmatrix}=
\begin{bmatrix}s&0&0\\0&s&0\\0&0&1\end{bmatrix}
\begin{bmatrix}x\\y\\1\end{bmatrix}
$$
</li>
<li>旋转 rotation<br>旋转的角度$\theta$默认是逆时针方向。<br><img src="http://7xjz3b.com1.z0.glb.clouddn.com/rotation.jpg" alt="">
$$坐标方程： 
\begin{align*}
x'= x\cos\theta-y\sin\theta\\
y'=x\sin\theta+y\cos\theta
\end{align*}
,
\quad\quad 转换矩阵:
\begin{bmatrix}
x'\\y'\\1
\end{bmatrix}=
\begin{bmatrix}\cos\theta&-\sin\theta&0\\\sin\theta&\cos\theta&0\\0&0&1\end{bmatrix}
\begin{bmatrix}x\\y\\1\end{bmatrix}
$$

</li>
</ul>
</text></text></li>
</ul>
<h3 id="欧式变换_(Euclidean_transformations)：">欧式变换 (Euclidean transformations)：</h3><ul>
<li>组成：旋转+平移。 （该图例为先旋转，再平移）</li>
<li>3自由度：1旋转，2平移 （自由度即影响矩阵的自由数的个数，这里有一个角度，xy两个平移坐标, 关系到空间中至少需要几个不相关的点可以描述一个变换）</li>
<li>不变量：长度，角度，面积，点线关系<br><img src="http://7xjz3b.com1.z0.glb.clouddn.com/Euclid1.jpg" alt=""></li>
<li>表示：
欧式坐标方程：
$$
x' = x_i\cos\theta-y_i\sin\theta+t_x\\
y' = x_i\sin\theta+y_i\cos\theta+t_y\\
$$
齐次坐标矩阵表示
$$
\begin{bmatrix}
x'\\y'\\1
\end{bmatrix}=
\begin{bmatrix}\cos\theta&-\sin\theta&t_x\\\sin\theta&\cos\theta&t_y\\0&0&1\end{bmatrix}
\begin{bmatrix}x\\y\\1\end{bmatrix}\\ 
$$
化简，设p为原向量,$H_E$为变换矩阵
$$
p' = H_Ep = \begin{bmatrix}R&t\\0^\mathrm{T}&1\end{bmatrix}p, 其中旋转矩阵R是正交矩阵
$$

</li>
</ul>
<h3 id="相似变换_(Similiarity_transformations)：">相似变换 (Similiarity transformations)：</h3><ul>
<li>组成：尺度因子+旋转+平移</li>
<li>4自由度：1尺度，1旋转，2平移 </li>
<li>不变量：长度比，角度，面积比，点线关系<br><img src="http://7xjz3b.com1.z0.glb.clouddn.com/similarity1.jpg" alt=""></li>
<li>表示：
欧式坐标方程（a(x,y)为原向量）：
$$
a' = sRa+t
$$
齐次坐标矩阵表示：
$$
\begin{bmatrix}
x'\\y'\\1
\end{bmatrix}=
\begin{bmatrix}s\cos\theta&-s\sin\theta&t_x\\s\sin\theta&s\cos\theta&t_y\\0&0&1\end{bmatrix}
\begin{bmatrix}x\\y\\1\end{bmatrix}\\ 
$$
化减，设p为原向量的齐次坐标,$H_s$为变换矩阵
$$
p' = H_sp = \begin{bmatrix}sR&t\\0^\mathrm{T}&1\end{bmatrix}p, 其中旋转矩阵R是正交矩阵
$$

</li>
</ul>
<h3 id="仿射变换_(Affine_transformations)：">仿射变换 (Affine transformations)：</h3><ul>
<li>组成：非均匀缩放。是相似变换的推广，使夹角不再保持不变；仿射变换对平面的作用是均匀的。</li>
<li>6自由度：2尺度，2旋转，2平移 </li>
<li>不变量：平行线，平行线段长度比，面积比，点线关系</li>
<li>点的转换：经过仿射变换，有限点只能转换成有限点，但无穷远点只能还是无穷远点（通过转换矩阵揭示）。<br><img src="http://7xjz3b.com1.z0.glb.clouddn.com/affine.jpg" alt="图和方程无关"></li>
<li>表示：
欧式坐标方程（a(x,y)为原向量）：
$$
a' = Aa+t
$$
化简，设p为原向量的齐次坐标向量,$H_A$为变换矩阵
$$
p' = H_Ap = Ap+t = \begin{bmatrix}A&t\\0^\mathrm{T}&1\end{bmatrix}p
$$
其中
$$
A = R(\theta)R(-\varphi)DR(\varphi), \quad D= \left(\begin{array}{cc}\lambda_1&0\\0&\lambda_2\end{array}\right)\\
$$
同时
$$
A = UDV^\mathrm{T} = (UV^\mathrm{T})(VDV^\mathrm{T})=R(\theta)R(-\varphi)DR(\varphi)
$$

</li>
</ul>
<h3 id="射影变换_(Projective_transformations)：">射影变换 (Projective transformations)：</h3><ul>
<li>组成：需要对射影几何中各概念详细理解</li>
<li>8自由度：2维中变换矩阵是3x3的，除了泛化的常数$H_{33}=1$,其他均自由 </li>
<li>不变量：交比，射影空间中概念，射影空间中交比不变，点线关系</li>
<li>点的转换：经过仿射变换，有限点可以被转换成无穷远点，但<strong>无穷远点可以转换为有限点</strong>。<br><img src="http://7xjz3b.com1.z0.glb.clouddn.com/project.jpg" alt=""></li>
<li>表示：<br>欧式坐标方程（a(x,y)为原向量）：
$$
a'=\frac{Aa+t}{c^\mathrm{T}a+1}，\quad 
 \begin{bmatrix}a'\\1\end{bmatrix}\sim\begin{bmatrix}A&t\\c^\mathrm{T}&1\end{bmatrix}\begin{bmatrix}a\\1\end{bmatrix}
$$
<text id="dev">用$\sim$的原因是经过射影变换齐次坐标的常数项会发生变化。<text>
这里也体现齐次坐标方便了**除法**的运算，通过矩阵影响齐次坐标最后的常数，通过齐次坐标与欧式坐标的转换$\left(\begin{array}{c}x\\y\\w
\end{array}\right) \Rightarrow (\frac{x}{w},\frac{y}{w})$，完成除法运算。
$$
$$
化为齐次坐标和矩阵表示,经过射影转换后，齐次坐标最后一项不一定为1：
$$
\begin{bmatrix}
x'\\y'\\w
\end{bmatrix}=
\begin{bmatrix}h_{11}&h_{12}&h_{13}\\h_{21}&h_{22}&h_{23}\\h{31}&h{32}&h{33}\end{bmatrix}
\begin{bmatrix}x\\y\\1\end{bmatrix}\\ 
$$
化简，设p为原向量的齐次坐标，$H_p$为变换矩阵
$$
p' = H_pp = \begin{bmatrix}A&t\\c^\mathrm{T}&b\end{bmatrix}p\sim\begin{bmatrix}A&t\\c^\mathrm{T}&1\end{bmatrix}p
$$
射影变换与仿射变换根本别在于： $c^\mathrm{T}\neq 0$，因此，对于无穷远点$\left(\begin{array}{c}x\\y\\0\end{array}\right)$，经过仿射变换后齐次坐标变为$\left(\begin{array}{c}x\\y\\c_1x+c_2y\end{array}\right)$，如此将无穷远点转化为有限点。

</text></text></li>
</ul>
<hr>
<p>-Robin<br>December 7, 2015 4:52 PM</p>
]]></content>
    <summary type="html">
    <![CDATA[<p><strong>矩阵其实代表一种线性变换</strong>，是理解线性代数的核心之一。而线性变换有着各种各样的用途，所以矩阵这种人造产物又有了许多新的意义。<br>不同的矩阵代表不同变换，许多变换都有统一的特征，可以归为一类；若把矩阵看成多个列向量的罗列，则更容易理解。这里]]>
    </summary>
    
      <category term="变换" scheme="http://robinzheng.com/tags/%E5%8F%98%E6%8D%A2/"/>
    
      <category term="矩阵" scheme="http://robinzheng.com/tags/%E7%9F%A9%E9%98%B5/"/>
    
      <category term="计算机视觉" scheme="http://robinzheng.com/tags/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89/"/>
    
      <category term="齐次坐标" scheme="http://robinzheng.com/tags/%E9%BD%90%E6%AC%A1%E5%9D%90%E6%A0%87/"/>
    
      <category term="理论" scheme="http://robinzheng.com/categories/%E7%90%86%E8%AE%BA/"/>
    
  </entry>
  
  <entry>
    <title><![CDATA[卷积的理解与图像处理中的应用]]></title>
    <link href="http://robinzheng.com/2015/11/17/%E5%8D%B7%E7%A7%AF%E7%9A%84%E7%90%86%E8%A7%A3%E4%B8%8E%E5%9B%BE%E5%83%8F%E5%A4%84%E7%90%86%E4%B8%AD%E7%9A%84%E5%BA%94%E7%94%A8/"/>
    <id>http://robinzheng.com/2015/11/17/卷积的理解与图像处理中的应用/</id>
    <published>2015-11-17T13:32:28.000Z</published>
    <updated>2016-02-26T03:03:56.000Z</updated>
    <content type="html"><![CDATA[<p>对卷积内容的一些学习和整理，从基本的图像滤波到CNN，供备忘和交流，个别图借用了自动化所董老师的课件。</p>
<h2 id="一、卷积">一、卷积</h2><h3 id="1-_定义：">1. 定义：</h3><p>定义一种卷积运算，将两个函数$g(x)$与$f(x)$通过该运算得到$h(x)$</p>
<ul>
<li>卷积有多重解释。泛函中解释为两个函数生成第三个函数的数学算子；概率论解释为两变量叠加的概率密度等，这些解释的特点是两个卷积函数地位是相同的，对应$(f\otimes g) (x) = (g\otimes f) (x)$，具体也有推导，我们不详细研究。</li>
<li>这里主要从图像处理和信号方面理解，所以区分原函数与响应函数会比较直观。</li>
</ul>
<h3 id="2-_公式：">2. 公式：</h3><p>假设$f(x)$为原函数，$g(x)$为卷积函数，$h(x)$为卷积运算后新生成的函数；<br>注意：积分和累加的范围实际是卷积函数$g(x)$的定义域。<br>
- 一维连续： 
$$
h(x) = \int g(\tau)f(x-\tau)d\tau
$$
- 一维离散：
$$
h(x) = \sum_\tau g(\tau)f(x-\tau)
$$
</p>
<ul>
<li>从公式理解：<ul>
<li>无限多的离散点就构成了连续，两公式所含意义相同，大多问题是连续存在的。注意这里从卷积函数$g(x)$却别离散与连续。</li>
<li>从公式角度谈一下$\tau$。卷积可以理解为<strong>对冲击响应的持续叠加</strong>。现在我们求的是$h(x)$的值，这一点的值由所有这点之前和之后的值对这点的影响叠加而成。把$x$理解成时间维度，这里$f(x-\tau)$定位到$x$点之前$\tau$的时间，$g(\tau)$表示该时间的卷积函数对x点影响，积分或叠加所有这些影响，得到$h(x)$的值。（形象理解可以看图下面引的例子）</li>
</ul>
</li>
</ul>
<h3 id="3-函数图像：">3.函数图像：</h3><p>四幅图对照下文，分别解释了离散和连续情况的卷积运算,后三幅图的三个函数图像分别为原函数$f(x)$，卷积响应函数$g(x)$，和最终卷积运算的结果$h(x)$。<br>其中图1直接取自百度，图2-4是用matplotlib绘成，使用离散点模拟的连续函数。</p>
<table border="1" align="center"><span style="font-size:24px;color:#006600;"></span><caption align="top">函数图像</caption><br><tr><td>图一<img src="http://7xjz3b.com1.z0.glb.clouddn.com/juanji1.jpg" width="300" height="200" alt="" align="center"></td> <td>图2<img src="http://7xjz3b.com1.z0.glb.clouddn.com/figure_3.png" width="300" height="200" alt="" align="center"></td> </tr><br><tr><td>图3<img src="http://7xjz3b.com1.z0.glb.clouddn.com/figure_1.png" width="300" height="200" alt="" align="center"></td> <td>图4<img src="http://7xjz3b.com1.z0.glb.clouddn.com/figure_2.png" width="300" height="200" alt="" align="center"></td> </tr><br></table>  

<h4 id="图像解释：">图像解释：</h4><p>对卷积的理解很多，比较有直观的有<a href="http://www.zhihu.com/question/22298352" target="_blank" rel="external">打拳</a>、<a href="http://www.zhihu.com/question/20500497" target="_blank" rel="external">向水中投石</a>这两个例子。这些例子有几个共同的特点：</p>
<ul>
<li>都有一个单位刺激的响应函数。如打一拳引起的疼痛，投一个石头引起的水花；</li>
<li>若力度相同（或是石头重量一致），原函数都是$f(x)=1$。把时间推移理解为横轴X；如果拳头的力量或是石头的大小有改变，则原函数$f(x)$变为图4中有波动的原函数。</li>
<li>例子中每个时间点观察的结果都是前面所有时间单位响应到目前结果的叠加，与卷积运算得出新函数的意义相同。</li>
<li>都是以离散点举例，最后想象无限多个点，进而演变成连续的解释。</li>
</ul>
<p>结合函数图像来理解，离散的情况如<strong>图1</strong>，连续情况如<strong>图2,3,4</strong>。在<strong>图2</strong>中，$g(x)$单位高斯函数，在原函数$f(x)$的上的卷积运算.<br>用引用的帖子中总结概括：<strong>各个时间点上的单位响应的加权叠加</strong>。这里的加权，就是指的原函数$f(x)$中的函数值。</p>
<h3 id="4-_总结：">4. 总结：</h3><ul>
<li>理解$X$轴为时间尺度，如果原函数是$f(x)$，响应函数$g(x)$是（0，1）这个点，那么卷积计算结果$h(x) = g(x) \otimes f(x)$的卷积结果是原函数$f(x)$。这是因为响应函数只存在于一点，所以，不会对后面的时间有影响，所以不存在随x轴的叠加————<strong>证明了叠加性</strong>（图2）</li>
<li>如果原函数$f(x)$是一个常数函数，那么无论响应函数形式如何，最后的卷积结果$h(x)$一定会趋向不变。这是因为原函数值不变，权值相同，时间尺度上的叠加都相同，所以函数值稳定后会不变———<strong>证明了加权性</strong>（图3）。</li>
</ul>
<p>总结:<br><strong>一维卷积运算可以理解成以原函数值$f(x)$为尺度，以响应函数$g(x)$为单位，在$x$轴方向上的的叠加。</strong></p>
<h3 id="5-_卷积运算特性">5. 卷积运算特性</h3><h4 id="1-_$\frac{\partial}{\partial{x}}(h\otimes{f})_=_(\frac{\partial}{\partial{x}}h)\otimes{}f$">1. $\frac{\partial}{\partial{x}}(h\otimes{f}) = (\frac{\partial}{\partial{x}}h)\otimes{}f$</h4><p>证明：<br>$$<br>\frac{d}{dt}[h(t)\otimes f(t)] = \frac{d}{dt}\int{f(\tau)h(t-\tau)}d\tau = \int{f(\tau)\frac{d}{dt}h(t-\tau)d\tau} = f(t)\otimes \frac{d}{dt}h(t)<br>$$<br>这个特性可以简化图像处理的运算，将多个步骤融合成一个卷积模板，比如LOG，如下图示意。</p>
<table border="1" align="center"><span style="font-size:24px;color:#006600;"></span><br><caption align="top"></caption><br><tr><td><img src="http://7xjz3b.com1.z0.glb.clouddn.com/juanji6.jpg" width="600" height="300" alt="" align="center"></td> <td><img src="http://7xjz3b.com1.z0.glb.clouddn.com/juanji7.jpg" width="600" height="300" alt="" align="center"></td> </tr><br></table>  

<h2 id="二、图像处理的卷积：">二、图像处理的卷积：</h2><p>简单谈谈图像卷积应用中的自己的理解。<br>在图像中，卷积的使用方式主要是卷积核，也就是一个卷积模板。原始的图像是一个2D数字信号，应用一个2D卷积模板，可以看做原始图像每个像素与固定长宽的卷积模板做卷积运算，相当于对图像进行一次滤波。<br>
$$
G(x,y) \otimes I(x,y) = \sum_{u=-M}^M \sum_{v=-N}^N G(u,v)I(x-u,y-v)
$$
其中$I(x,y)$代表图像中每一个像素，$G(u,v)$表示尺寸为(2M+1)*(2N+1)的模板
<br><img src="http://7xjz3b.com1.z0.glb.clouddn.com/juanji5.jpg" alt=""></p>
<h3 id="关于卷积核">关于卷积核</h3><h4 id="1-_卷积函数与卷积核：">1. 卷积函数与卷积核：</h4><ol>
<li>卷积的定义是基于函数的，原始的卷积核模板上所有位置都对应同一个函数。</li>
<li>实际使用中，我们常使用是某一函数的近似模板，模板上每个位置都有一个常数值。这种模板主要体现形状近似于卷积函数，计算也较为方便。</li>
</ol>
<h4 id="2-_模板的特点">2. 模板的特点</h4><ul>
<li>模板大小：一般卷积近似模板都是奇数个，如$3*3,5*5$等，这样每个模板有一个中心点，也有半径，大小的区别在于<strong>对于单点被模板考虑的像素数的多少</strong>。</li>
<li>模板上所有元素的加和：<ul>
<li>模板上所有元素（即卷积矩阵）的加和不同决定了模板的作用。当加和为1时，卷积计算前后图像的亮度不变。    </li>
<li>检测边缘的卷积模板的加和往往为0，因为这些模板只是为了检测边缘，只关注像素之间的梯度关系，与像素值无关，无需原来像素点的像素。应用的结果是图像的亮度变得很暗。</li>
</ul>
</li>
<li>这些卷积模板是对连续卷积核函数的数字采样近似，相当于上面用离散点近似连续卷积函数。不同的卷积模板决定了卷积的不同功能：去噪，平滑，凸显某些特征等，不详细叙述。</li>
</ul>
<h4 id="3-_卷积运算的一种加速思想">3. 卷积运算的一种加速思想</h4>
形如高斯核函数,$\sigma$为窗宽，决定平滑尺度：
$$
G_\sigma(x,y) = \frac{1}{\sqrt{2\pi\sigma^2}}exp(-\frac{x^2+y^2}{2\sigma^2})
$$
忽略规范化系数$\sqrt{2\pi\sigma^2}$：
$$
G_\sigma(x,y) = exp(\frac{-x^2}{2\sigma^2})exp(\frac{-y^2}{2\sigma^2})
$$
通过(2M+1)*(2N+1)的模板g(x,y)应用到图像中：
$$
\begin{align*}
g(x,y)\otimes I(x,y) &=\sum_{u=-M}^M \sum_{v=-N}^N g(u,v)I(x-u,y-v)\\&= \sum_{u=-M}^M \sum_{v=-N}^N exp(\frac{-u^2}{2\sigma^2})exp(\frac{-v^2}{2\sigma^2})I(x-u,y-v)\\
&=\sum_{u=-M}^M exp(\frac{-u^2}{2\sigma^2})[\sum_{v=-N}^Nexp(\frac{-v^2}{2\sigma^2})I(x-u,y-v)]
\end{align*}
$$

通过公式可以看出，假设原始图像大小为$A*B$, 模板大小为$M*N$，正常需要计算$A*B*M*N$次，现在转化为计算 $(M+N)*A*B $次，大大减小了运算时间。

<p>如图:<br><img src="http://7xjz3b.com1.z0.glb.clouddn.com/juanji8.jpg" alt=""></p>
<h2 id="End-">End.</h2><p>对知识的理解上，国内的知乎值得一看，看法往往独到且深入。不过有时百度百科、各种博客等大众性解释有很大的导性，看来普及性和质量往往难以共存。</p>
<p>Robin<br>November 11, 2015 10:57 PM    初写<br>January 7, 2016 9:56 PM   修改卷积公式描述，补充卷积性质<br>January 13, 2016 10:08 AM 增加卷积模板的加速运算。<br>February 22, 2016 3:07 PM 补充图像卷积核的一些特点。</p>
]]></content>
    <summary type="html">
    <![CDATA[<p>对卷积内容的一些学习和整理，从基本的图像滤波到CNN，供备忘和交流，个别图借用了自动化所董老师的课件。</p>
<h2 id="一、卷积">一、卷积</h2><h3 id="1-_定义：">1. 定义：</h3><p>定义一种卷积运算，将两个函数$g(x)$与$f(x)$通]]>
    </summary>
    
      <category term="卷积" scheme="http://robinzheng.com/tags/%E5%8D%B7%E7%A7%AF/"/>
    
      <category term="计算机视觉" scheme="http://robinzheng.com/tags/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89/"/>
    
      <category term="理论" scheme="http://robinzheng.com/categories/%E7%90%86%E8%AE%BA/"/>
    
  </entry>
  
  <entry>
    <title><![CDATA[迁移Hexo博客：GitHub至GitCafe]]></title>
    <link href="http://robinzheng.com/2015/11/10/%E8%BF%81%E7%A7%BBHexo%E5%8D%9A%E5%AE%A2%EF%BC%9AGitHub%E8%87%B3GitCafe/"/>
    <id>http://robinzheng.com/2015/11/10/迁移Hexo博客：GitHub至GitCafe/</id>
    <published>2015-11-10T12:23:39.000Z</published>
    <updated>2015-11-12T01:45:33.000Z</updated>
    <content type="html"><![CDATA[<p>昨天着手将博客从GitHub迁移到GitCafe，心里长草多时终于付诸行动。整体过程还算顺利，迁移后功能基本正常，但中间遇到几点零碎问题，现记录如此，一供自己备忘，二可以帮助遇到同样困难的朋友。</p>
<h2 id="引子">引子</h2><p>这里主要谈谈迁移博客的原因，主要来自如下几点：</p>
<ul>
<li>搜索引擎检索：无法被百度检索是在Gitpages上部署博客最主要的问题，主要原因是Github服务器默认屏蔽了百度的爬虫。网上的几种解决方案并不触及本质，效果有限。</li>
<li>访问速度：国内地址访问Github的速度还是不够完美</li>
<li>屏蔽隐患：因为GFW有过暂时屏蔽Github的不良记录，所以博客被屏蔽的隐患始终存在。</li>
</ul>
<p>而相比较来说GitCafe因为是在国内的实现，这些问题基本都没有，而且两者本是同根生，迁移起来技术成本很小。但因为GitHub的影响力和作用也不想放弃，所以确定了迁移博客到GitCafe，同时两边同步更新的策略。</p>
<h2 id="主要步骤">主要步骤</h2><p>共有如下几个关键点：</p>
<h3 id="项目迁移:">项目迁移:</h3><p>需要在Gitcafe上生成一个相同的项目。GitCafe与GitHub本地环境相同，完全可以使用一套git软件，只是在远程项目配置上稍有不同。我们需要做的：</p>
<ul>
<li>注册GitCafe帐号，新建一个<strong>与帐号同名的任务</strong>，这样GitCafe会默认该项目为page项目。<em>(注意名称区分大小写，GitCafe的特点)</em></li>
<li><p>将本地博客项目push到Gitcafe项目下的gitcafe-pages分支：</p>
<ul>
<li>进入hexo博客的默认部署目录 username.io/.deploy_git</li>
<li><p>添加新的远程仓库：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ git remote add gitcafe git@gitcafe.com:Username/Username.git</span><br></pre></td></tr></table></figure>
<p><em>(add后用最好gitcafe而不是origin，否则与原来Github的远程仓库地址冲突；Username注意大小写)</em></p>
</li>
<li><p>push到GitCafe项目下的gitcafe-pages分支：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">$ git checkout gitcafe-pages</span><br><span class="line">$ git push -u gitcafe gitcafe-pages</span><br></pre></td></tr></table></figure>
<p><em>(注意push完再切换回master分支，因为hexo默认的部署文件我们都已master分支为主)</em></p>
</li>
</ul>
</li>
</ul>
<h3 id="同步更新">同步更新</h3><p>已完成迁移工作，下面需要配置同步更新Github和GitCafe的功能。我选择的修改hexo的配置文件，通过hexo命令完成同步更新的方法，很简单，感觉比再写一个同步更新脚本更舒服些。<br>我们只需要修改一下根目录下hexo的配置文件_config.yml：<br><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">deploy:</span><br><span class="line">  type: git</span><br><span class="line">  repo: </span><br><span class="line">    github: git@github.com:robinroar/robinroar.github.io,master</span><br><span class="line">    gitcafe: git@gitcafe.com:RobinROAR/RobinROAR.git,gitcafe-pages</span><br></pre></td></tr></table></figure></p>
<p><em>（可以看出一个是推送到GitHub项目的master分支，一个是推送到GitCafe项目的gitcafepages分支，省略了本地的默认分支master）</em><br>这样就可以用老方法<strong>hexo d</strong>同步更新两个项目.</p>
<h3 id="绑定域名">绑定域名</h3><p>最后需要绑定域名，这里注意几点：</p>
<ul>
<li>在服务商添加域名解析时不要使用A记录的方式了，因为网上原来的GitCafe服务器的Ip地址由于DOS攻击已经暂时在2015.5前后关闭，官网提供的解决方案是改用CNAME记录，指向Username.gitcafe.io即可。</li>
<li>原来在博客文件目录中的CNAME文件需要移除，因为现在是一个项目两处同时更新，两处的CNAME文件指向同一个域名，好像会有点问题。解决方法很简单，只需要把CNAME文件移出根目录下的source目录即可，下此部署时该文件就不会部署到服务器了。</li>
</ul>
<p><em>（但博客目录中的CNAME文件的作用我还有点疑惑，是将原本到username.github.io的访问导到CNAME中的地址？这样是否会引起循环？如有赐教不胜感激）</em></p>
<h3 id="It’s_done！">It’s done！</h3><h2 id="重要Tips">重要Tips</h2><ul>
<li>GitCafe区分大小写，包括url的地址，项目的名称等</li>
<li>Hexo通过配置文件进行push/pull操作的博客目录是的username.io/.deploy_git/</li>
<li>GitCafe下使用的分支是gitcafe-pages</li>
</ul>
<p>-Robin<br>2015.11.10 夜</p>
]]></content>
    <summary type="html">
    <![CDATA[<p>昨天着手将博客从GitHub迁移到GitCafe，心里长草多时终于付诸行动。整体过程还算顺利，迁移后功能基本正常，但中间遇到几点零碎问题，现记录如此，一供自己备忘，二可以帮助遇到同样困难的朋友。</p>
<h2 id="引子">引子</h2><p>这里主要谈谈迁移博客的原因]]>
    </summary>
    
      <category term="GitCafe" scheme="http://robinzheng.com/tags/GitCafe/"/>
    
      <category term="Gitpages" scheme="http://robinzheng.com/tags/Gitpages/"/>
    
      <category term="hexo" scheme="http://robinzheng.com/tags/hexo/"/>
    
      <category term="博客迁移" scheme="http://robinzheng.com/tags/%E5%8D%9A%E5%AE%A2%E8%BF%81%E7%A7%BB/"/>
    
      <category term="技术" scheme="http://robinzheng.com/categories/%E6%8A%80%E6%9C%AF/"/>
    
  </entry>
  
  <entry>
    <title><![CDATA[scikt-learn示例解析 色彩量化Color Quantization using K-Means]]></title>
    <link href="http://robinzheng.com/2015/10/21/scikt-learn%E7%A4%BA%E4%BE%8B%E8%A7%A3%E6%9E%90%20%E8%89%B2%E5%BD%A9%E9%87%8F%E5%8C%96Color%20Quantization%20using%20K-Means/"/>
    <id>http://robinzheng.com/2015/10/21/scikt-learn示例解析 色彩量化Color Quantization using K-Means/</id>
    <published>2015-10-21T12:04:03.000Z</published>
    <updated>2015-11-23T08:04:47.000Z</updated>
    <content type="html"><![CDATA[<p>翻译并解释了scikt-learn官网示例，留作备忘。</p>
<h1 id="Scikt-learn_Example：Color_Quantization_using_K-Means">Scikt-learn Example：Color Quantization using K-Means</h1><p>通过k-Means进行图像的色彩量化<br>url： <a href="http://scikit-learn.org/stable/auto_examples/cluster/plot_color_quantization.html" target="_blank" rel="external">http://scikit-learn.org/stable/auto_examples/cluster/plot_color_quantization.html</a></p>
<h2 id="简介">简介</h2><p><strong>色彩量化</strong>是数据压缩的一个有效手段。这个示例提供了一个基于像素的色彩量化例子，将一副图片从96615个独立颜色降低到64色，大幅减小了图像大小，同时保留图片的基本外观。<br>示例通过聚类方法K-Means，从每一点的像素值中选取64个聚类中心，然后将所有点按照距离度量最接近的像素值分配，从而达到色彩量化的目的。在实际操作中，图像是以三维数组存储的（w，h，d），每一个RGB像素也需要一个长度为3的数组来表示（红绿蓝）。除此之外，示例使用了另一个方法random codebook，即随机挑选像素值作为聚类中心，然后再将所有像素分类，对比实验。</p>
<ul>
<li>以下是示例的结果：我们可以发现，96615色降到64色后图片外观基本可以保持不变，而图3由于是随机生成的聚类中心，所以色彩有变化。<table border="1" align="center"><span style="font-size:24px;color:#006600;"></span><br><caption align="top">示例结果</caption><br><tr><td><img src="http://7xjz3b.com1.z0.glb.clouddn.com/plot_color_quantization_0011.png" width="400" height="300" alt="" align="center"></td>  <td></td>  </tr><br><tr><td><img src="http://7xjz3b.com1.z0.glb.clouddn.com/plot_color_quantization_002.png" width="400" height="300" alt="" align="center"></td> <td><img src="http://7xjz3b.com1.z0.glb.clouddn.com/plot_color_quantization_003.png" width="400" height="300" alt="" align="center"></td>  </tr><br></table>  

</li>
</ul>
<h2 id="代码分析">代码分析</h2><p>对源代码分段解释一下，其实英文文档的注释已经比较全面，只是有些细节需要补充。<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Authors: Robert Layton &lt;robertlayton@gmail.com&gt;</span></span><br><span class="line"><span class="comment">#          Olivier Grisel &lt;olivier.grisel@ensta.org&gt;</span></span><br><span class="line"><span class="comment">#          Mathieu Blondel &lt;mathieu@mblondel.org&gt;</span></span><br><span class="line"><span class="comment">#</span></span><br><span class="line"><span class="comment"># License: BSD 3 clause</span></span><br><span class="line"></span><br><span class="line">print(__doc__)</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">from</span> sklearn.cluster <span class="keyword">import</span> KMeans</span><br><span class="line"><span class="keyword">from</span> sklearn.metrics <span class="keyword">import</span> pairwise_distances_argmin</span><br><span class="line"><span class="keyword">from</span> sklearn.datasets <span class="keyword">import</span> load_sample_image</span><br><span class="line"><span class="keyword">from</span> sklearn.utils <span class="keyword">import</span> shuffle</span><br><span class="line"><span class="keyword">from</span> time <span class="keyword">import</span> time</span><br></pre></td></tr></table></figure></p>
<ul>
<li>几个包的引用，对矩阵处理肯定需要numpy，然后最后绘图的步骤需要matplotlib，time包是用来统计每一步的执行时间的。</li>
<li>然后是引用scikt-learn相关组组件，KMeans是实现聚类的方法，pairwise_distances_argmin是一个计算相似度的方法，后面细说。load_sample_image是读取sklearn内置图片的方法，shuffle是一个随机排列数组的方法。</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line">n_colors = <span class="number">64</span></span><br><span class="line"><span class="comment"># Load the Summer Palace photo</span></span><br><span class="line">china = load_sample_image(<span class="string">"china.jpg"</span>)</span><br><span class="line"><span class="comment"># Convert to floats instead of the default 8 bits integer coding. Dividing by</span></span><br><span class="line"><span class="comment"># 255 is important so that plt.imshow behaves works well on float data (need to</span></span><br><span class="line"><span class="comment"># be in the range [0-1]</span></span><br><span class="line">china = np.array(china, dtype=np.float64) / <span class="number">255</span></span><br><span class="line"><span class="comment"># Load Image and transform to a 2D numpy array.</span></span><br><span class="line">w, h, d = original_shape = tuple(china.shape)</span><br><span class="line"><span class="keyword">assert</span> d == <span class="number">3</span></span><br><span class="line">image_array = np.reshape(china, (w * h, d))</span><br><span class="line">print(<span class="string">"Fitting model on a small sub-sample of the data"</span>)</span><br><span class="line">t0 = time()</span><br><span class="line">image_array_sample = shuffle(image_array, random_state=<span class="number">0</span>)[:<span class="number">1000</span>]</span><br><span class="line">kmeans = KMeans(n_clusters=n_colors, random_state=<span class="number">0</span>).fit(image_array_sample)</span><br><span class="line">print(<span class="string">"done in %0.3fs."</span> % (time() - t0))</span><br><span class="line"><span class="comment"># Get labels for all points</span></span><br><span class="line">print(<span class="string">"Predicting color indices on the full image (k-means)"</span>)</span><br><span class="line">t0 = time()</span><br><span class="line">labels = kmeans.predict(image_array)</span><br><span class="line">print(<span class="string">"done in %0.3fs."</span> % (time() - t0))</span><br></pre></td></tr></table></figure>
<p>英文注释已经比较详细了，这里只补充一下要点。</p>
<ul>
<li>原始图像读出的三维数组是int8类型的，红绿蓝各为0-255，这里用numpy转成了float64类型并/255是为了后面plt画图包的显示。</li>
<li>中间reshape了一下原始数组，从3维转成2维，是为了方便后面的聚类和分类，实际像素的排列是没有变化的。</li>
<li>这里让K-means模型学习的数据并不是原始图像，而是从原始图像<strong>随机选取</strong>了1000个像素的颜色值，这里就用到刚才提到的shuffle方法，具体功能可以点<a href="http://scikit-learn.org/stable/modules/generated/sklearn.utils.shuffle.html#sklearn.utils.shuffle" target="_blank" rel="external">源码</a>，这样可以大大减少计算时间，并得到几乎一样的结果。</li>
<li>然后就是根据聚类中心对原始图像每一个像素分类，用所属聚类中心的颜色代替。<strong>有一点注意</strong>，K-means方法在预测时，由于要计算欧式距离，sklearn的实现十分消耗内存，对于大一点的图像就会出现MemoryError的问题，解决方法可以用类似的MiniBatchKmeans方法代替K-Means。</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">codebook_random = shuffle(image_array, random_state=<span class="number">0</span>)[:n_colors + <span class="number">1</span>]</span><br><span class="line">print(<span class="string">"Predicting color indices on the full image (random)"</span>)</span><br><span class="line">t0 = time()</span><br><span class="line">labels_random = pairwise_distances_argmin(codebook_random,image_array,axis=<span class="number">0</span>)</span><br><span class="line">print(<span class="string">"done in %0.3fs."</span> % (time() - t0))</span><br></pre></td></tr></table></figure>
<p>这里实现的是random-codebook对比方法。</p>
<ul>
<li>这里codebook-random的作用相当于K-means模型中生成的聚类中心，只不过这里时完全随机罢了。</li>
<li>然后后面用到了pairwise_distances_argmin方法，作用相当于上面means.predict(image_array)，就是将原始图像每一个像素的颜色归属到距离大最近的codebook中的颜色。具体功能可以看<a href="http://scikit-learn.org/stable/modules/generated/sklearn.metrics.pairwise_distances_argmin.html#sklearn.metrics.pairwise_distances_argmin" target="_blank" rel="external">源码</a>.</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">recreate_image</span><span class="params">(codebook, labels, w, h)</span>:</span></span><br><span class="line">    <span class="string">"""Recreate the (compressed) image from the code book &amp; labels"""</span></span><br><span class="line">    d = codebook.shape[<span class="number">1</span>]</span><br><span class="line">    image = np.zeros((w, h, d))</span><br><span class="line">    label_idx = <span class="number">0</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(w):</span><br><span class="line">        <span class="keyword">for</span> j <span class="keyword">in</span> range(h):</span><br><span class="line">            image[i][j] = codebook[labels[label_idx]]</span><br><span class="line">            label_idx += <span class="number">1</span></span><br><span class="line">    <span class="keyword">return</span> image</span><br></pre></td></tr></table></figure>
<p>这里实现了一个画图方法。返回的是一个三维数组，然后通过后面的matplotlib包实现画图。</p>
<ul>
<li>具体很简单，就是用numpy生成一个原始图像大小的0矩阵，然后按照每个聚类中心的颜色codebook，和原始图像每一个像素点归属的聚类中心labels，重新生成描述图像的三维数组。</li>
<li>这里完成了2D到3D的转变，也就是从分类标签到图像数组的转换，又回到了原始的三维数组。</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Display all results, alongside original image</span></span><br><span class="line">plt.figure(<span class="number">1</span>)</span><br><span class="line">plt.clf()</span><br><span class="line">ax = plt.axes([<span class="number">0</span>, <span class="number">0</span>, <span class="number">1</span>, <span class="number">1</span>])</span><br><span class="line">plt.axis(<span class="string">'off'</span>)</span><br><span class="line">plt.title(<span class="string">'Original image (96,615 colors)'</span>)</span><br><span class="line">plt.imshow(china)</span><br><span class="line"></span><br><span class="line">plt.figure(<span class="number">2</span>)</span><br><span class="line">plt.clf()</span><br><span class="line">ax = plt.axes([<span class="number">0</span>, <span class="number">0</span>, <span class="number">1</span>, <span class="number">1</span>])</span><br><span class="line">plt.axis(<span class="string">'off'</span>)</span><br><span class="line">plt.title(<span class="string">'Quantized image (64 colors, K-Means)'</span>)</span><br><span class="line">plt.imshow(recreate_image(kmeans.cluster_centers_, labels, w, h))</span><br><span class="line"></span><br><span class="line">plt.figure(<span class="number">3</span>)</span><br><span class="line">plt.clf()</span><br><span class="line">ax = plt.axes([<span class="number">0</span>, <span class="number">0</span>, <span class="number">1</span>, <span class="number">1</span>])</span><br><span class="line">plt.axis(<span class="string">'off'</span>)</span><br><span class="line">plt.title(<span class="string">'Quantized image (64 colors, Random)'</span>)</span><br><span class="line">plt.imshow(recreate_image(codebook_random, labels_random, w, h))</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>
<p>最后就是通过matpolt画出原始图和两张处理后的图，都是基本方法，可以自己了解matplotlib包，不再详细记录。</p>
<h3 id="End">End</h3><p>-Robin<br>2015.10.21 夜<br><!--end--></p>
]]></content>
    <summary type="html">
    <![CDATA[<p>翻译并解释了scikt-learn官网示例，留作备忘。</p>
<h1 id="Scikt-learn_Example：Color_Quantization_using_K-Means">Scikt-learn Example：Color Quantization usin]]>
    </summary>
    
      <category term="kmeans" scheme="http://robinzheng.com/tags/kmeans/"/>
    
      <category term="scikt-learn" scheme="http://robinzheng.com/tags/scikt-learn/"/>
    
      <category term="机器学习" scheme="http://robinzheng.com/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="技术" scheme="http://robinzheng.com/categories/%E6%8A%80%E6%9C%AF/"/>
    
  </entry>
  
  <entry>
    <title><![CDATA[逻辑回归与计算中的向量化思想]]></title>
    <link href="http://robinzheng.com/2015/08/13/%E9%80%BB%E8%BE%91%E5%9B%9E%E5%BD%92%E4%B8%8E%E8%AE%A1%E7%AE%97%E4%B8%AD%E7%9A%84%E5%90%91%E9%87%8F%E5%8C%96%E6%80%9D%E6%83%B3/"/>
    <id>http://robinzheng.com/2015/08/13/逻辑回归与计算中的向量化思想/</id>
    <published>2015-08-13T09:36:33.000Z</published>
    <updated>2016-02-24T08:02:25.000Z</updated>
    <content type="html"><![CDATA[<h2 id="引子">引子</h2><ul>
<li>这部分总结的有点晚了，不过在熟悉机器学习体系和几个其它模型后，再回过头来看逻辑回归，发现从原理到思想都有比较好的理解。</li>
<li>理论主要参考NG的斯坦福课程和李航老师的《统计学习方法》，代码实现参见《机器学习实战》。</li>
<li>向量化计算的思想在逻辑回归代码中体现的很好，利用矩阵计算上的便利性来解决复杂的循环过程。但由于《实战》书中的原理推导及其吝惜笔墨（与其简洁优美的代码对比鲜明），这里做一下补充。</li>
<li>感谢<a href="http://blog.csdn.net/dongtingzhizi/article/details/15962797" target="_blank" rel="external">洞庭之子</a>这篇博客，解决了我的很多疑问，很佩服作者的思路和严谨的推导,但这篇博客中的有些写法不规范，容易引起误解。</li>
</ul>
<h2 id="逻辑回归">逻辑回归</h2><h3 id="思想">思想</h3><ul>
<li><strong>根据数据对分类边界线建立回归公式:</strong> 与感知机乃至SVM大同小异，都是寻找一个<code>超平面</code>将数据集分为两部分。基于如此，逻辑回归一般只能处理两分类问题，同时两个类别线性可分。对于<code>多分类问题</code>，还是老思想，化用二分类（目标类为一类，剩余唯一类），构建多个分类器，寻找概率最大的那个类作为分类结果。</li>
<li><strong>通过分类函数（sigmoid函数）寻找分类超平面</strong>： 具体sigmoid函数相关的内容下面有详细叙述.</li>
<li><strong>判别模型的老思路：</strong>假设特征系数$\theta$，构造预测函数 ——&gt; 构造损失函数 ——&gt; 求解最优化问题：寻找使损失函数最小時的特征系数$\theta$ ——&gt; 得到分类器（即超平面）。</li>
<li><strong>优缺点：</strong> 计算简单，训练分类器后计算量小;准确度有限，容易欠拟合，只针对二分类问题。</li>
</ul>
<h3 id="分类函数Sigmoid">分类函数Sigmoid</h3><ul>
<li>逻辑回归选择<code>近似于阶越函数</code>的Sigmoid函数作为分类函数：<br>$$\sigma(z_i) = \dfrac{1}{1+e^{-z}}，<br>(z_i = \theta_0x_i^{(0)}+\theta_1x_i^{(1)}+..+\theta_nx_i^{(m)} )$$</li>
<li>函数图像：<img src="http://7xjz3b.com1.z0.glb.clouddn.com/blog4-1.png" alt="sigmod"></li>
<li>$\theta$为特征系数向量, 每个特征都有一个特征系数$\theta_n$。另外，$x_i^{(j)}$为输入向量$x_i$的第j个分量，</li>
<li>作用：1. 逻辑回归的分类函数。 2. 将样本映射到0-1区间，进而巧妙地将数据到分界线的距离转化为概率，然后通过最大斯然估计等方法求解，这一些后面会谈到。</li>
</ul>
<h3 id="问题求解">问题求解</h3><p>求解的过程这里简单讲讲，具体内容（比如阶梯求导结果等）不在赘述，可以参阅参考博客。<br>判别模型的基本套路：<br><strong>预测函数 ——&gt; 构造损失函数or最大斯然估计求发生概率 ——&gt; 求解最优化问题</strong>：</p>
<ul>
<li>1、预测函数：<br>$$h_{\theta}(x)=\sigma(z_i) = \dfrac{1}{1+e^{-z}}，<br>(z_i = \theta_0x_i^{(0)}+\theta_1x_i^{(1)}+..+\theta_nx_i^{(m)} )$$</li>
</ul>
<p>sigmoid的函数值可以表示成分类概率：<br>\begin{equation*}<br>\begin{aligned}<br>P(y=1|x,\theta) &amp; = h_\theta(x) \\<br>P(y=0|x,\theta) &amp; = 1-h_\theta(x)<br>\end{aligned}<br>\end{equation*}</p>
<ul>
<li>2、通过最大斯然估计求发生概率，单个样本发生的概率为：<br>$$<br>P(y|x,\theta)=(h_{\theta}(x))^y(1-h_{\theta}(x))^{1-y}<br>$$<br>最大似然估计，让所有样本的发生概率最大化，取似然函数：
$$
L(\theta)  = \prod_{i=1}^nP(y_i|x_i,\theta) = \prod_{i=1}^n (h_{\theta}(x_i))^{y_i}(1-h_{\theta}(x_i))^{1-y_i}
$$

</li>
</ul>
<p>取对数：<br>
\begin{equation*}
\begin{aligned}
l(\theta) & = \log L(\theta) \\
& =\sum_{i=1}^n(y_i(\log h_{\theta}(x_i)+(1-y_i)\log{(1-h_{\theta}(x_i))}
\end{aligned}
\end{equation*}
<br>到这里，我们就可以用最优化方法求解$l(\theta)$，本例用的是梯度上升法。<br>在斯坦福ML的课程中，取$J(\theta)=-\dfrac{1}{m}l(\theta)$,构建损失函数，然后利用梯度下降法求解$\theta$，本质是完全一样的。</p>
<ul>
<li>3、借助梯度上升法求解最优值：
 \begin{equation*}
\begin{aligned}
\theta_j & = \theta_j + \alpha\dfrac{\delta}{\delta\theta_j}J(\theta) \\
& = \theta_j+\alpha\sum_{i=1}^{n}(y_i - h_{\theta}(x_i))x_i^{(j)}
\end{aligned}
\end{equation*}
($特征维数j = 0...m,、\alpha为学习步长$)
\begin{equation*}
\begin{aligned}
\dfrac{\delta}{\delta\theta_j}J(\theta) & = \sum_{i=1}^n\left[y_i\dfrac{1}{h_\theta(x_i)}\dfrac{\delta}{\delta\theta_j} h_\theta(x_i)-(1-y_i)\dfrac{1}{1-h_\theta(x_i)}\dfrac{\delta}{\delta\theta_j} h_\theta(x_i) \right]   \\\
& = .....\\
& = \theta_j+\alpha\sum_{i=1}^{n}(y_i - h_{\theta}(x_i))x_i^{(j)}
\end{aligned}
\end{equation*}

这里省去了大部分求偏导的过程，详细的求解步骤，可以参见参考博客。博文虽用的是梯度下降法，但每一步的求偏导数过程是完全一致的。</li>
</ul>
<h2 id="计算中的向量化思想">计算中的向量化思想</h2><ul>
<li>在求解最优化方法時，将数据向量化，用矩阵的方式计算，是一种很好的思想。</li>
<li>借助numpy等包，使得以前针对单个数值编写的方法，对矩阵也有了很好的支持度。</li>
<li><p>《机器学习实战》的实现代码中，使用梯度上升法求解$\theta$時，就用了向量化思想，用矩阵乘法代替了循环。但书中直接给出了迭代求$\theta$的公式，缺少了如上节类似的推导，初读还是有些令人费解。</p>
<h3 id="上节已知求$\theta$每一步的更新过程：">上节已知求$\theta$每一步的更新过程：</h3>
\begin{equation}
\begin{aligned}
\theta_j & = \theta_j + \alpha\dfrac{\delta}{\delta\theta_j}J(\theta) \\\
& = \theta_j+\alpha\sum_{i=1}^{n}(y_i - h_{\theta}(x_i))x_i^{(j)}
\end{aligned}
\end{equation}

</li>
<li><p>1、首先我们把特征系数$\theta$用m×1的列向量表示。(假设有n个输入向量，特征共有m维,$\theta_0x_0当作常量偏移$)：<br>\begin{equation*}<br>\theta = \begin{bmatrix}<br>\theta_0 \\<br>\theta_1 \\<br>… \\<br>\theta_m<br>\end{bmatrix}<br>\end{equation*}</p>
</li>
</ul>
<p>输入数据用n×m的矩阵表示：<br>\begin{equation*}<br>X = \begin{bmatrix}<br>x_1 \\<br>x_2 \\<br>… \\<br>x_n<br>\end{bmatrix} =<br>\begin{bmatrix}<br>x_1^{(0)},x_1^{(1)}…x_1^{(m)} \\<br>x_2^{(0)},x_2^{(1)}…x_2^{(m)}  \\<br>… \\<br>x_n^{(0)},x_n^{(1)}…x_n^{(m)}<br>\end{bmatrix}<br>\end{equation*}<br>所以$z_n = \theta_0x_n^{(0)}+\theta_1x_n^{(1)}+..+\theta_nx_n^{(m)} $可以用$X\cdot\theta$得出的n×1列向量来表示：<br>\begin{equation*}<br>Z = \begin{bmatrix}<br>\theta_0x_1^{(0)}+\theta_1x_1^{(1)}+…+\theta_mx_1^{(m)} \\<br>\theta_0x_2^{(0)}+\theta_1x_2^{(1)}+…+\theta_mx_2^{(m)}   \\<br>… \\<br>\theta_0x_n^{(0)}+\theta_1x_n^{(1)}+…+\theta_mx_n^{(m)}<br>\end{bmatrix} =\begin{bmatrix}<br>z_1 \\<br>z_2 \\<br>… \\<br>z_n<br>\end{bmatrix}<br>\end{equation*}<br>用矩阵乘法的列向量分解思想来说，$X\cdot\theta$表示$X$中的每一个列向量以系数$\theta$进行<code>线性组合</code>，符合公式的含义。</p>
<ul>
<li>2、我们再用列向量$Y = (y_1,y_2..y_n)^\mathrm{T}$表示输入数据的类别，然后我们对$Z$进行sigmoid函数运算，所以更新过程公式中$(y_i - h_{\theta}(x_i))$可以用n×1的列向量E表示：<br>\begin{equation*}<br>E = \begin{bmatrix}<br>y_1-\sigma(z_1) \\<br>y_2-\sigma(z_2)  \\<br>… \\<br>y_n-\sigma(z_n)<br>\end{bmatrix} =\begin{bmatrix}<br>e_1 \\<br>e_2 \\<br>… \\<br>e_n<br>\end{bmatrix}<br>\end{equation*}<br>(这里的E代表误差error)</li>
<li>3、最后我们求解整个式子$\theta_j  = \theta_j+\alpha\sum_{i=1}^{n}(y_i - h_{\theta}(x_i))x_i^{(j)}$. 式中的连加同样可以通过矩阵乘法解决。<br>将$X$转置：
\begin{equation*}
X^\mathrm{T} = \begin{bmatrix}
x_1,
x_2,
... 
x_n
\end{bmatrix} = 
\begin{bmatrix}
x_1^{(0)},x_2^{(0)}...x_n^{(0)} \\
x_1^{(1)},x_2^{(1)}...x_n^{(1)}  \\
...... \\
x_1^{(m)},x_2^{(m)}...x_n^{(m)} 
\end{bmatrix}
\end{equation*}

</li>
</ul>
<p>转置后矩阵是m×n的，仔细观察一下转置后的X，每一个列向量是一条输入数据，而<strong>每一个行向量是所有输入数据在某一个特征维度上的记录</strong>，一共有m个行向量。<br>所以，我们同时用每一个行向量$\cdot$列向量$E$,用矩阵乘法即：<br>
\begin{equation*}
X^\mathrm{T}\cdot\mathrm{E} = 
\begin{bmatrix}
x_1^{(0)}e_1+x_2^{(0)}e_2+...+x_n^{(0)}e_n \\
x_1^{(1)}e_1+x_2^{(1)}e_2+...+x_n^{(1)}e_n \\
...... \\\
x_1^{(m)}e_1,x_2^{(m)}e_2+...+x_n^{(m)}e_n 
\end{bmatrix}     
\end{equation*}
<br>(m*1)</p>
<ul>
<li>4、已知增长系数$\alpha$，然后我们就可以用矩阵计算进行一次梯度上升迭代：
\begin{equation*}
\theta_{n+1} = \theta_n +\alpha \cdot X^\mathrm{T}\cdot\mathrm{E} = 
\begin{bmatrix}
\theta_{n+1}^{(0)} \\
\theta_{n+1}^{(1)} \\
... \\
\theta_{n+1}^{(m)}
\end{bmatrix}     
\end{equation*}

</li>
</ul>
<ol>
<li>设置合适的迭代次数，求得最终的特征系数$\theta$,我们就得到了训练好的判别函数。<h2 id="小结：">小结：</h2></li>
</ol>
<ul>
<li>看完我们会发现，逻辑回归也是基于简单的线性分类思想，只不过逻辑回归通过一个契合线性分类的sigmoid函数，将数据到分界限的距离巧妙地投影到了0-1区间，进而可以将距离转化为概率，通过最大斯然估计求解。</li>
<li>向量化的思想其实充斥着机器学习的各个角落。就像MIT线性代数公开课中说的，矩阵并不是生来存在的，而是人们后天发明用来方便计算的产物。将数据向量化，通过矩阵运算求解，是解决问题的必经之路。</li>
</ul>
<hr>
<p>这次公式比较多，处理markdown语法和$\mathrm{\LaTeX}$的冲突時花了不少时间，不过也因此找到一个好方法，不用修改node.js的配置就可以处理好语法冲突，只需要在<code>公式的前后用raw标签注释就可以了</code>，原理应该是通过标签屏蔽了markdown语法的解释器，前端不是很懂，不过很有效～</p>
]]></content>
    <summary type="html">
    <![CDATA[<h2 id="引子">引子</h2><ul>
<li>这部分总结的有点晚了，不过在熟悉机器学习体系和几个其它模型后，再回过头来看逻辑回归，发现从原理到思想都有比较好的理解。</li>
<li>理论主要参考NG的斯坦福课程和李航老师的《统计学习方法》，代码实现参见《机器学习实战》]]>
    </summary>
    
      <category term="分类" scheme="http://robinzheng.com/tags/%E5%88%86%E7%B1%BB/"/>
    
      <category term="判别模型" scheme="http://robinzheng.com/tags/%E5%88%A4%E5%88%AB%E6%A8%A1%E5%9E%8B/"/>
    
      <category term="机器学习" scheme="http://robinzheng.com/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="线性代数" scheme="http://robinzheng.com/tags/%E7%BA%BF%E6%80%A7%E4%BB%A3%E6%95%B0/"/>
    
      <category term="逻辑回归" scheme="http://robinzheng.com/tags/%E9%80%BB%E8%BE%91%E5%9B%9E%E5%BD%92/"/>
    
      <category term="技术" scheme="http://robinzheng.com/categories/%E6%8A%80%E6%9C%AF/"/>
    
  </entry>
  
  <entry>
    <title><![CDATA[谈谈贝叶斯决策]]></title>
    <link href="http://robinzheng.com/2015/07/14/%E7%AE%80%E8%B0%88%E8%B4%9D%E5%8F%B6%E6%96%AF%E5%86%B3%E7%AD%96/"/>
    <id>http://robinzheng.com/2015/07/14/简谈贝叶斯决策/</id>
    <published>2015-07-14T01:44:57.000Z</published>
    <updated>2015-12-02T09:45:04.000Z</updated>
    <content type="html"><![CDATA[<p><strong>贝叶斯决策</strong>, 一种常用的分类方法，复杂度低，准确度好，可以和多种分类方法结合。最高的理论正确率令人神往（上自动化所模式识别课中多次强调）。<br>写一写学习贝叶斯方法中的几点理解，一个简单脉络而已，方便回顾，与大家分享。</p>
<hr>
<h2 id="先验概率与后验概率">先验概率与后验概率</h2><p>两个概念：</p>
<ul>
<li><code>先验概率</code>： 事情还没有发生，要求这件事情发生的可能性的大小。如P(A).</li>
<li><code>后验概率</code>： 事情已经发生，要求这件事情发生的原因<code>是由某个因素引起的</code>可能性的大小，是后验概率. </li>
</ul>
<p>所以我们可以使用贝叶斯公式求后验概率，进而达到分类的目的。</p>
<h2 id="贝叶斯公式">贝叶斯公式</h2><p>$$ P(F_j|E)= \dfrac{P(F_jE)}{P(E)} =  \dfrac{P(E|F_j)P(F_j)}{ \displaystyle{\sum_{i=1}^{n} }P(E|F_i)P(F_i) } $$</p>
<p>两个核心：</p>
<ul>
<li><p><strong>推导</strong>：条件概率公式。分解中间步骤分子，我们得出结果$P(F_j|E) \cdot P(E)= P(F_jE) =   P(E|F_j) \cdot P(F_j)$.   </p>
</li>
<li><p><strong>应用</strong>：用于分类问题。分解中间步骤分母$P(E) = \displaystyle{\sum_{i=1}^{n} }P(E|F_i)P(F_i）$ ，每个$F_i$代表一个类别。</p>
</li>
</ul>
<p>扯点零碎，条件概率公式对任意两个事件都是存在的，当两事件独立(即$P(EF) = P(E)P(F)$)时，$P(E|F) = P(EF)/P(F) = P(E)$.</p>
<h2 id="例子">例子</h2><p>看过很多讲解贝叶斯公式的例子，下面这个很好:</p>
<blockquote>
<p>一份信件等可能的在存在三个不同的文件夹中的任意一个。假设信件实际在文件夹$i$中$(i = 1,2,3)$而你经过对文件夹$i$的快速翻阅发现信件的概率记为$a_i$，问题是，假定你查看了文件夹1且没有发现此信，问信件在文件夹1中的概率是多少？<br>                                                              ——<a href="http://book.douban.com/subject/2309401/" target="_blank" rel="external">《应用随机过程：概率模型导论》</a></p>
</blockquote>
<p><strong>解</strong>： 以$F_i(i = 1,2,3)$为信在文件中$i$中的事件<code>（先验概率）</code>，E是通过对文件夹1搜索而为看到信这个事件，我们求$P(F_1|E)$<code>(后验概率)</code>，通过贝叶斯公式：<br>$$    P(F_1|E)=  \dfrac{P(E|F_1)P(F_1)}{ \displaystyle{\sum_{i=1}^{3} }P(E|F_i)P(F_i) } = \dfrac{(1-a_1) \frac{1}{3}}{ (1-a_1) \frac{1}{3}+\frac{1}{3}+\frac{1}{3}} = \dfrac{1-a_1}{3-a_1}$$</p>
<p>把握两个重点：</p>
<pre><code>-<span class="ruby"> 对问题中事件的假设
</span>-<span class="ruby"> 对整体事件空间(分母)的划分</span>
</code></pre><h2 id="简单实现：朴素贝叶斯">简单实现：朴素贝叶斯</h2><p>贝叶斯思想只是一个框架，可结合许多假设与分布。<br>朴素思分类就是一个通过先验概率求后验概率的方法，然后将实例点分到后验概率最大的类。就像街上有一个黑人，我们预测黑人来自非洲，为什么呢？因为黑人中非洲人的比率最高。</p>
<h4 id="朴素贝叶斯的核心假设：">朴素贝叶斯的核心假设：</h4><ol>
<li><p>特征之间相互独立 ：这个“朴素”一词的来源。假设体现在向量的特征之间。所以有<br>$$P(x|y_i)P(y_i)=P(a_1|y_i)P(a_2|y_i)…P(a_m|y_i)P(y_i)=P(y_i)\prod^m_{j=1}P(a_j|y_i)$$</p>
<ul>
<li>$y_i$为每一个类别，$a_i$ 为样本X的特征的每一维分量</li>
</ul>
</li>
<li><p>每个特征同等重要。</p>
</li>
</ol>
<h2 id="分类准则">分类准则</h2><p>将输入向量分到后验概率最大的类，这就是<code>分类准则</code>。贝叶斯使用的是<code>最大后验概率准则</code>。<br>在别的分类方法中，我们还用过<code>最小错误率</code>的分类准则，比如基于0-1损失的最小错误率，这两者是一样的。</p>
<p>设损失函数</p>
<p>$L(Y,f(X))=$</p>
<p>\begin{align*}<br>1,Y \not= f(X) \\<br>0, Y= f(X) \\<br>\end{align*}</p>
<ul>
<li>期望风险函数为</li>
</ul>
<p>\begin{equation*}<br>\begin{aligned}<br>R_e(f(x)) &amp; = E[L(Y, f(x))]\\<br>&amp; =  \iint_{X,Y}L(Y,f(X))P(X,Y)\mathrm{d}x\mathrm{d}y \\<br>&amp;= \iint_{X,Y}L(Y,f(X))P(Y|X)P(X)\mathrm{d}x\mathrm{d}y\\<br>&amp;= \int_{X} (\sum_{k=1}^{k}L(Y= c_k,f(X))P(Y=c_k|X) )P(X)\mathrm{d}x\\<br>&amp;= E_x(\sum_{k=1}^{k}L(Y= c_k,f(X))P(Y=c_k|X) )<br>\end{aligned}<br>\end{equation*}</p>
<ul>
<li>为了使期望最小，对$X=x$逐个极小化：</li>
</ul>
<p>\begin{equation*}<br>\begin{aligned}<br>f(x)&amp; = \arg\min \limits_{y\in\mathcal{Y}}\sum_{k=1}^{k}L(c_k,y)P(c_k|X=x) \\<br>&amp;=  \arg\min \limits_{y\in\mathcal{Y}}\sum_{k=1}^{k}P(y \not = c_k|X=x) \\<br>&amp;=  \arg\min \limits_{y\in\mathcal{Y}}\sum_{k=1}^{k}(1-P(y  = c_k|X=x) )\\<br>&amp;=  \arg\max \limits_{y\in\mathcal{Y}}P(y = c_k|X = x)<br>\end{aligned}<br>\end{equation*}</p>
<p>可以看出，这就是<code>最大后验概率准则</code>，也是我们使用贝叶斯决策的分类准则。</p>
<hr>
<p>暂时先写这几点，顺带说一句，Hexo上写$\mathrm{\LaTeX}$公式，冲突比较多，还在寻找比较好的解决方法，</p>
<p>Robin<br>2015.7.10 夜 </p>
]]></content>
    <summary type="html">
    <![CDATA[<p><strong>贝叶斯决策</strong>, 一种常用的分类方法，复杂度低，准确度好，可以和多种分类方法结合。最高的理论正确率令人神往（上自动化所模式识别课中多次强调）。<br>写一写学习贝叶斯方法中的几点理解，一个简单脉络而已，方便回顾，与大家分享。</p>
<hr>
]]>
    </summary>
    
      <category term="分类" scheme="http://robinzheng.com/tags/%E5%88%86%E7%B1%BB/"/>
    
      <category term="机器学习" scheme="http://robinzheng.com/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="贝叶斯" scheme="http://robinzheng.com/tags/%E8%B4%9D%E5%8F%B6%E6%96%AF/"/>
    
      <category term="技术" scheme="http://robinzheng.com/categories/%E6%8A%80%E6%9C%AF/"/>
    
  </entry>
  
  <entry>
    <title><![CDATA[理解Linux的环境变量、启动文件和命令目录]]></title>
    <link href="http://robinzheng.com/2015/06/22/%E7%90%86%E8%A7%A3Linux%E7%9A%84%E7%8E%AF%E5%A2%83%E5%8F%98%E9%87%8F%E3%80%81%E5%90%AF%E5%8A%A8%E6%96%87%E4%BB%B6%E4%B8%8E%E5%91%BD%E4%BB%A4%E7%9B%AE%E5%BD%95/"/>
    <id>http://robinzheng.com/2015/06/22/理解Linux的环境变量、启动文件与命令目录/</id>
    <published>2015-06-22T03:53:59.000Z</published>
    <updated>2016-02-26T02:45:14.000Z</updated>
    <content type="html"><![CDATA[<p>在搭建hadoop 的环境 时配置环境变量的时候遇到些问题，于是系统总结了Linux中的环境变量、启动文件等相关知识发,以供备忘。</p>
<h2 id="1-_环境变量的作用">1.  环境变量的作用</h2><p>在不同系统中，环境变量总是扮演着相同的角色，安装完新环境，新软件，第一件事总是配置环境变量。</p>
<h3 id="环境变量_：">环境变量 ：</h3><ul>
<li>存储有关shell会话和<code>工作环境</code>的信息（如Java，hadoop，python等路径，主要是各个bin文件夹的绝对地址）</li>
<li>变量储存在<code>内存</code>中，方便程序以及脚本访问</li>
</ul>
<h2 id="2-_环境变量的分类：">2.  环境变量的分类：</h2><h3 id="局部变量：">局部变量：</h3><ul>
<li>特性：<code>作用于当前shell，</code>在子shell或者其它shell不可见（反映了局部变量的在当前进程可用的意义）  <strong>见例1</strong></li>
<li><p>定义：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$  varname=abc       <span class="comment">#局部变量名尽量使用小写; = 左右不要有空格，否则变量名会被解析为命令</span></span><br></pre></td></tr></table></figure>
</li>
<li><p>显示： </p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$  <span class="built_in">echo</span> <span class="variable">$varname</span>     <span class="comment">#echo命令要使用$变量名</span></span><br></pre></td></tr></table></figure>
</li>
<li><p>删除：       </p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ <span class="built_in">unset</span> varname</span><br></pre></td></tr></table></figure>
</li>
</ul>
<h3 id="全局变量：">全局变量：</h3><ul>
<li>特性：作用于当前shell<code>及所有shell创建的子进程</code>; 系统登录时已经默认设置了许多全局环境变量。  <strong>见例2</strong></li>
<li><p>定义：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">$  varname=abc </span><br><span class="line">$  <span class="built_in">export</span> varname    <span class="comment">#先建立局部变量，用export导成全局变量</span></span><br></pre></td></tr></table></figure>
</li>
<li><p>显示：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">$ printenv                 <span class="comment">#显示系统环境变量，系统环境变量一律大写  </span></span><br><span class="line">$ <span class="built_in">echo</span> <span class="variable">$varname</span>      <span class="comment">#显示单个变量</span></span><br></pre></td></tr></table></figure>
</li>
<li><p>删除： </p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ <span class="built_in">unset</span> varname      <span class="comment">#只对子进程中变量有效，父进程中的全局变量仍然存在。</span></span><br></pre></td></tr></table></figure>
</li>
</ul>
<h2 id="3-_系统默认定义的全局环境变量">3.  系统默认定义的全局环境变量</h2><p>PATH变量是我们经常使用和修改的环境变量，它定义了<code>提供命令解析的搜索路径</code>。所以每当我们安装了新的环境，总要在更新PATH变量，这样就可以直接在任何位置使用命令，而不会出现commond not found的问题。 <br>　　其它系统变量的修改与添加也大致一样。</p>
<h3 id="PATH变量：">PATH变量：</h3><ul>
<li><p>特性： 命令行输入命令的搜索路径, 如：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ start-dfs.sh   <span class="comment">#启动HDFS，而不用进入../hadoop2.5.2/sbin/目录下</span></span><br></pre></td></tr></table></figure>
</li>
<li><p>查看： </p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">$ <span class="built_in">echo</span> <span class="variable">$PATH</span></span><br><span class="line">/usr/<span class="built_in">local</span>/bin:/usr/bin:/bin:/usr/<span class="built_in">local</span>/games:/usr/games    <span class="comment">#不同值之间由:分割</span></span><br></pre></td></tr></table></figure>
</li>
<li><p>添加值，修改：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">$ PATH=<span class="variable">$PATH</span>:/home/user/<span class="built_in">test</span>  <span class="comment">#添加目录/home/user/test, 只在当前shell有效</span></span><br><span class="line">$ <span class="built_in">export</span> PATH=.:/HOME/eobin:<span class="variable">$PATH</span>     <span class="comment">#单点符在PATH变量中代表当前路径，变量名可以放在末尾</span></span><br></pre></td></tr></table></figure>
</li>
</ul>
<p>通常我们把对于PATH变量的改动<code>写在系统或用户的启动文件中</code>，比如说~/.bashrc中，在登陆系统或者打开新shell时自动加载。</p>
<h2 id="4-_启动文件与系统环境变量的加载：">4. 启动文件与系统环境变量的加载：</h2><h3 id="/etc/profile:">/etc/profile:</h3><ul>
<li>作用：系统默认的<code>主启动文件</code>，每个用户登录时都会加载这个文件。</li>
<li>变量持续时间：声明的变量会在每个新shell中存在，除非在子进程中被修改</li>
</ul>
<h3 id="$HOME/-bash_profile:">$HOME/.bash_profile:</h3><ul>
<li>作用：用户专属的启动文件， 定义用户专属的环境变量，该文件会<code>先检查并加载HOME目录中的.bashrc文件</code>（如果存在）</li>
</ul>
<h3 id="$HOME/-bashrc:">$HOME/.bashrc:</h3><ul>
<li>作用： <code>交互式shell的启动文件</code>，用于定制自己的命名别名和私有脚本</li>
<li>这里有一点疑问，书中提到<blockquote>
<p>如果bash是作为交互式shell启动的，它不去访问/etc/profile, 而会去用户的HOME目录检查.bashrc是否存在  - p116 《Linux命令行与Shell脚本大全》</p>
</blockquote>
</li>
</ul>
<p>但/etc/profile是随系统登录时就启动的，任何<code>交互式shell</code>都是其子进程，所以我认为不用访问自然会加载其中变量，而此处特地指出，不明其所以然，也许没有理解交互式shell的意义。当然结果并没有什么不同，都会以.bashrc中的设置为准。</p>
<h3 id="一般来说Linux启动加载配置文件顺序如下：/etc/profile_→_/etc/profile-d/*-sh_→_~/-bash_profile_→_~/-bashrc_→_[/etc/bashrc]">一般来说Linux启动加载配置文件顺序如下：/etc/profile → /etc/profile.d/*.sh → ~/.bash_profile → ~/.bashrc → [/etc/bashrc]</h3><h2 id="5-_附例：">5. 附例：</h2><ul>
<li><p><strong>例1</strong>   局部变量只在当前进程中有效</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">robin@esp:~$ varname=abc       定义变量varname</span><br><span class="line">robin@esp:~$ <span class="built_in">echo</span> <span class="variable">$varname</span>     显示，有值</span><br><span class="line">abc</span><br><span class="line">robin@esp:~$ bash              新建子进程</span><br><span class="line">robin@esp:~$ <span class="built_in">echo</span> <span class="variable">$varname</span>     显示，无值</span><br><span class="line"></span><br><span class="line">robin@esp:~$ <span class="built_in">exit</span>              退出子进程</span><br><span class="line"><span class="built_in">exit</span></span><br><span class="line">robin@esp:~$ <span class="built_in">echo</span> <span class="variable">$varname</span>     显示，有值</span><br><span class="line">abc</span><br></pre></td></tr></table></figure>
</li>
<li><p><strong>例2</strong>   全局变量的作用范围</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">robin@esp:~$ <span class="built_in">echo</span> <span class="variable">$varname</span>           显示变量，无值</span><br><span class="line"></span><br><span class="line">robin@esp:~$ bash                    新建子进程<span class="number">2</span></span><br><span class="line">robin@esp:~$ <span class="built_in">export</span> varname=bash2    定义全局变量</span><br><span class="line">robin@esp:~$ <span class="built_in">echo</span> <span class="variable">$varname</span>           显示，有值</span><br><span class="line">bash2</span><br><span class="line">robin@esp:~$ bash                    新建子进程<span class="number">3</span></span><br><span class="line">robin@esp:~$ <span class="built_in">echo</span> <span class="variable">$varname</span>           显示，有值</span><br><span class="line">bash2</span><br><span class="line">robin@esp:~$ <span class="built_in">exit</span>                    退出到bash2</span><br><span class="line"><span class="built_in">exit</span></span><br><span class="line">robin@esp:~$ <span class="built_in">exit</span>                    退出到原始进程</span><br><span class="line"><span class="built_in">exit</span></span><br><span class="line">robin@esp:~$ <span class="built_in">echo</span> <span class="variable">$varname</span>           显示，无值</span><br></pre></td></tr></table></figure>
</li>
</ul>
<h2 id="几个命令目录详解：/bin,/sbin,/usr/sbin,/usr/bin">几个命令目录详解：/bin,/sbin,/usr/sbin,/usr/bin</h2><p>经过对环境变量的了解，我们发现上述四个目录经常被写在环境变量中。这些目录的作用，是存放各种’存放命令’。</p>
<h3 id="/sbin：">/sbin：</h3><p>从名字来解释，bin是binary的所写，而s可以理解为super的所写。所以/sbin目录主要用于存放一些超级用户指令，同时，也必须要求用用有root权限才能使用。<br>这些命令主要有：:cfdisk、dhcpcd、dump、e2fsck、fdisk、halt、ifconfig、ifup、 ifdown、init、insmod、lilo、lsmod、mke2fs、modprobe、quotacheck、reboot、rmmod、 runlevel、shutdown等。</p>
<h3 id="/bin：">/bin：</h3><p>与上面相同，/bin目录中也是系统命令，是一些较为普通的基本指令，一般来说管理员和普通用户都可以使用/bin目录下的命令。<br>主要包括cat、cp、chmod df、dmesg、gzip、kill、ls、mkdir、more、mount、rm、su、tar等。</p>
<h3 id="/usr/sbin:">/usr/sbin:</h3><p>该目录主要存放一些用户自主安装的系统管理程序。<br>包括dhcpd、httpd、imap、in.*d、inetd、lpd、named、netconfig、nmbd、samba、sendmail、squid、swap、tcpd、tcpdump等。</p>
<h3 id="/usr/bin：">/usr/bin：</h3><p>这个目录的内容就比较广泛，基本包括了用户安装的各种软件运行脚本以及执行命令。<br>如c++、g++、gcc、chdrv、diff、dig、du、eject、elm、free、gnome<em>、gzip、htpasswd、kfm、ktop、last、less、locale、m4、make、man、mcopy、ncftp、 newaliases、nslookup passwd、quota、smb</em>、wget等。</p>
<h3 id="Tips：">Tips：</h3><ul>
<li>如果命令调用不到，就要考虑以上四个目录是否在你的PATH变量中.比如说，查看得知：PATH=$PATH:$HOME/bin。那么，我们则需如下改动：    PATH=$PATH:$HOME/bin:/sbin:/usr/bin:/usr/sbin</li>
<li>善于创建链接：ln命令：<br>比如我们安装包Node.js,由于各种原因，安装后/usr/bin中默认的调用命令是nodejs,而其他第三方包写在脚本中的调用方法是node，这时有两个解决方法：<ol>
<li>使用alias命令：建立 alias node=nodejs   这种方法可以解决bash中输入命令的问题，但却在一些第三方包安装过程无效。</li>
<li>使用ln命令建立软链接： ln -s /usr/bin/nodejs /usr/bin/node  软链接可以完美解决第三方包安装的问题，具体使用可查询ln命令。 </li>
</ol>
</li>
</ul>
<h2 id="END">END</h2><p>Robin<br>2015.6.22 夜</p>
]]></content>
    <summary type="html">
    <![CDATA[<p>在搭建hadoop 的环境 时配置环境变量的时候遇到些问题，于是系统总结了Linux中的环境变量、启动文件等相关知识发,以供备忘。</p>
<h2 id="1-_环境变量的作用">1.  环境变量的作用</h2><p>在不同系统中，环境变量总是扮演着相同的角色，安装完新环境]]>
    </summary>
    
      <category term="linux" scheme="http://robinzheng.com/tags/linux/"/>
    
      <category term="环境变量" scheme="http://robinzheng.com/tags/%E7%8E%AF%E5%A2%83%E5%8F%98%E9%87%8F/"/>
    
      <category term="技术" scheme="http://robinzheng.com/categories/%E6%8A%80%E6%9C%AF/"/>
    
  </entry>
  
  <entry>
    <title><![CDATA[荆棘谷的青山]]></title>
    <link href="http://robinzheng.com/2015/06/10/%E8%8D%86%E6%A3%98%E8%B0%B7%E7%9A%84%E9%9D%92%E5%B1%B1/"/>
    <id>http://robinzheng.com/2015/06/10/荆棘谷的青山/</id>
    <published>2015-06-10T10:07:27.000Z</published>
    <updated>2015-10-26T08:15:15.000Z</updated>
    <content type="html"><![CDATA[<h2 id="正文">正文</h2><ul>
<li><p>在GitPages安家了，第一篇博文，又是新的一行＂Hello world＂，一次新的开始．</p>
</li>
<li><p>建博客的念头由来已久，但又久未实行．总在挑什么语言，选什么框架，却忘了博客应该做什么．仔细想想，抛开技术因素，内容才是博客价值所在．</p>
</li>
<li><p>博文会以技术为主，原创为主，偶尔也会写点风马牛不想及的东西，权且一乐。有时间也会逐步把有道云笔记的一直以来积累的东西，挑一挑转过来。希望能做到少而精，有些意义．就像荆棘谷里的那个知名任务，一片片零散的残章断卷后，讲述了一个美妙的故事．</p>
</li>
<li><p>一直以来也拜读了许多大咖的博客，细致度与原创性都令人佩服，不求超越，只求也像他们一样有一颗沉静的心，能仔细的把好东西沉淀下来。</p>
</li>
<li><p>读博路上人渐稀。正值毕业季，可惜这次我是一个孤独的旁观者，只好贪婪的呼吸点弥漫在空气之中的喜悦．前路漫漫，希望能从这篇博文开始，真正的做点事情.</p>
</li>
</ul>
<p> 　　 </p>
<h2 id="">　　</h2><h2 id="最后附一段Eminem的歌词，烂熟于心，很喜欢：">最后附一段Eminem的歌词，烂熟于心，很喜欢：</h2><blockquote>
<p>There’s batteries in my Walkman nothing isthe matter with me<br>即使生活充满悲剧，只要我Walkman还有电，困难算什么东西<br>Shit look on the bright side at least Iain’t walking<br>想想开心的事，至少我还有辆单车，是一个老司机<br>I bike ride through the neighborhood of myapartment<br>我嗨皮的驶过邻居家门口<br>Complex on a ten speed which I’ve acquiredparts that I<br>骑着这辆时速十码的组装车<br>Found in the garbage, a frame and put tireson it<br>从垃圾堆里找的部件，然后把轮胎装到架子上<br>Headphones on straight ahead and kids tryto start shit<br>耳机在耳边轰鸣，夹杂着熊孩子们的嘲笑<br>But if this all there is for me life offers<br>如果这是生活赐予我的全部<br>Why bother even tryna put up a fight, it’snonsense<br>那我为何还要与别人争斗？这毫无意义<br>But I think a lightbulb just lit up in my conscience<br>突然有些东西，它照亮了我的脑海<br>What about those rhymes I’ve been jottin’<br>是那些我随手记下的词句<br>They are kinda givin’ me confidence<br>它一直给予我自信和希望<br>Instead of tryna escape through my comics,<br>与其挣扎在这喜剧般的人生<br>Why don’t I just blast a little somethinglike Onyx<br>为何不像Onyx组合一样唱点什么？</p>
</blockquote>
<p>Robin<br>2015.6.10   夜</p>
]]></content>
    <summary type="html">
    <![CDATA[<h2 id="正文">正文</h2><ul>
<li><p>在GitPages安家了，第一篇博文，又是新的一行＂Hello world＂，一次新的开始．</p>
</li>
<li><p>建博客的念头由来已久，但又久未实行．总在挑什么语言，选什么框架，却忘了博客应该做什么．仔细]]>
    </summary>
    
      <category term="纪念" scheme="http://robinzheng.com/tags/%E7%BA%AA%E5%BF%B5/"/>
    
      <category term="生活" scheme="http://robinzheng.com/categories/%E7%94%9F%E6%B4%BB/"/>
    
  </entry>
  
</feed>